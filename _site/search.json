[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "quartoblog",
    "section": "",
    "text": "SVMs\n\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInequalities\n\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyModels Trees\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\ngraphviz\n\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nSex in a Biological Context\n\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nMath Biology Video Project\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nDerek Sollberger, Emily Weigel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhospital_data\n\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nJupyterHub Showcase\n\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncurly operator\n\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStardew Valley Crops\n\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCanvas_Roster\n\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeasons Greetings by Riinu Pius\n\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nAnyway Heres Wonderwall\n\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPatchwork\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nSettlement Survival Start\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nCoefficient of Variation\n\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nIntentional Walks\n\n\n\n\n\n\n\nbaseball\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nOblicubes\n\n\n\n\n\n\n\nfonts\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nTypewriter\n\n\n\n\n\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nBump\n\n\n\n\n\n\n\nsports\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noverviewR\n\n\n\n\n\n\n\nexploratory data analysis\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nquarto\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalifornia Weather\n\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Today I followed Albert Rapp’s guide to creating a Quarto blog.\nFor me, it was a matter of making\n\na GitHub repository, called it quartoblog, and cloned it to my computer\ndeleted the quartoblog folder on my computer\nstarted a new project in RStudio to select Quarto web site (note that Quarto is already bundled in newer versions of the RStudio IDE)\ngot Netlify and GitHub to play nicely with each other and share the quartoblog repository\nupdated my domain redirect to the Netlify site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/bump/bump.html",
    "href": "posts/bump/bump.html",
    "title": "Bump",
    "section": "",
    "text": "library(\"ggbump\")\nlibrary(\"Lahman\")\nlibrary(\"tidyverse\")\n\nFor today’s easy foray, let us seek out the wins and losses of teams in the Teams data frame (I tend to call my data frames df for typing ease).\n\ndf <- Teams\n\nThere are about 3000 observations and 48 variables. I will need some of the column names.\n\ncolnames(df)\n\n [1] \"yearID\"         \"lgID\"           \"teamID\"         \"franchID\"      \n [5] \"divID\"          \"Rank\"           \"G\"              \"Ghome\"         \n [9] \"W\"              \"L\"              \"DivWin\"         \"WCWin\"         \n[13] \"LgWin\"          \"WSWin\"          \"R\"              \"AB\"            \n[17] \"H\"              \"X2B\"            \"X3B\"            \"HR\"            \n[21] \"BB\"             \"SO\"             \"SB\"             \"CS\"            \n[25] \"HBP\"            \"SF\"             \"RA\"             \"ER\"            \n[29] \"ERA\"            \"CG\"             \"SHO\"            \"SV\"            \n[33] \"IPouts\"         \"HA\"             \"HRA\"            \"BBA\"           \n[37] \"SOA\"            \"E\"              \"DP\"             \"FP\"            \n[41] \"name\"           \"park\"           \"attendance\"     \"BPF\"           \n[45] \"PPF\"            \"teamIDBR\"       \"teamIDlahman45\" \"teamIDretro\"   \n\n\nTo make a quick exploration, let us filter for the past 10 seasons of baseball (2012 to 2021) and select the columns I will use later.\n\ndf <- Teams |>\n  filter(yearID >= 2012) |>\n  select(yearID, lgID, franchID, divID, Rank)\nhead(df)\n\n  yearID lgID franchID divID Rank\n1   2012   NL      ARI     W    3\n2   2012   NL      ATL     E    2\n3   2012   AL      BAL     E    2\n4   2012   AL      BOS     E    5\n5   2012   AL      CHW     C    2\n6   2012   NL      CHC     C    5\n\n\nTo be honest, I thought I was going to have to code up some function to rank team wins within the MLB divisions, but the Lahman database already has that!\n\ndf_left <- df |> filter(yearID == 2012 & lgID == \"NL\")\ndf_right <- df |> filter(yearID == 2021 & lgID == \"NL\")\n\n\ndf |>\n  filter(lgID == \"NL\") |>\n  ggplot(aes(x = yearID, y = -Rank, color = franchID)) +\n  geom_bump(size = 2) +\n  geom_point(aes(x = yearID, y = -Rank, color = franchID),\n             size = 5) +\n  geom_label(aes(x = yearID, y = -Rank, label = franchID), data = df_left) +\n  geom_label(aes(x = yearID, y = -Rank, label = franchID), data = df_right) +\n  facet_wrap(. ~ divID, ncol = 1) +\n  labs(title = \"National League Standings\",\n       subtitle = \"early draft of bump plot\",\n       caption = \"Derek Sollberger\") +\n  theme(legend.position = \"none\",\n        panel.background = element_blank())\n\nWarning in f(...): 'StatBump' needs at least two observations per group\n\n\n\n\n\n\n\n\nbump plot"
  },
  {
    "objectID": "posts/typewriter/typewriter.html",
    "href": "posts/typewriter/typewriter.html",
    "title": "Typewriter",
    "section": "",
    "text": "library(\"gt\")\n\n\ndf <- data.frame(\n  Period = c(\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"),\n  Course = c(\"Academic Decathlon\", \"Spanish 4 AP\", \"Calculus AP BC\", \"English 3 Honors\", \"Computer Applications\", \"Physics AP\", \"US History\")\n)\n\n\n# https://gt.rstudio.com/reference/cell_borders.html\ndf |>\n  gt() |>\n  tab_options(\n    # table.background.color = \"#E8EBE6\",\n    table.font.names = \"Courier New\"\n  ) |>\n  tab_style(\n    locations = cells_body(\n      columns = everything(),\n      rows = everything()\n    ),\n    style = list(\n      cell_borders(\n        sides = c(\"top\", \"bottom\"),\n        color = \"cyan\",\n        style = \"solid\"\n      ),\n      cell_fill(color = \"#E8EBE6\") #https://www.crispedge.com/color/e8ebe6/\n    )\n  ) |>\n  tab_style(\n    locations = cells_body(\n      columns = \"Course\",\n      rows = everything()\n    ),\n    style = list(\n      cell_borders(\n        sides = c(\"left\"),\n        color = \"red\",\n        style = \"solid\"\n      ),\n      cell_fill(color = \"#E8EBE6\")\n    )\n  )\n\n\n\n\n\n  \n  \n    \n      Period\n      Course\n    \n  \n  \n    0\nAcademic Decathlon\n    1\nSpanish 4 AP\n    2\nCalculus AP BC\n    3\nEnglish 3 Honors\n    4\nComputer Applications\n    5\nPhysics AP\n    6\nUS History\n  \n  \n  \n\n\n\n\n\n\n\nhigh school junior year schedule"
  },
  {
    "objectID": "posts/overviewR/overviewR.html",
    "href": "posts/overviewR/overviewR.html",
    "title": "overviewR",
    "section": "",
    "text": "Today I wanted to try out the overviewR package.\n\ncheat sheet: https://github.com/cosimameyer/overviewR/blob/master/man/figures/CheatSheet_overviewR.pdf\n\n\nlibrary(\"overviewR\")\nlibrary(\"palmerpenguins\")\n\nAt this moment, I misread what overviewR does (I thought it would summarize everything). Instead, I will just try out one tool for now.\n\npenguins_raw |>\n  overview_na()"
  },
  {
    "objectID": "posts/oblicubes/oblicubes.html",
    "href": "posts/oblicubes/oblicubes.html",
    "title": "Oblicubes",
    "section": "",
    "text": "https://github.com/trevorld/oblicubes\n\n\nlibrary(\"bittermelon\")\n\n\nAttaching package: 'bittermelon'\n\n\nThe following object is masked from 'package:base':\n\n    which\n\nlibrary(\"grid\")\nlibrary(\"oblicubes\")\n\n\n# example from\n# https://github.com/trevorld/oblicubes\nfont_file <- system.file(\"fonts/spleen/spleen-8x16.hex.gz\", package = \"bittermelon\")\nfont <- read_hex(font_file)\nbml <- as_bm_list(\"RSTATS\", font = font)\n# Add a shadow effect and border\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"grey20\", \"lightblue\", \"darkblue\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Bio 18\nbml <- as_bm_list(\"BIO 18\", font = font)\n\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Math 32\nbml <- as_bm_list(\"MATH32\", font = font)\n\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Spark\nbml <- as_bm_list(\"SPORTS\", font = font)\n\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\nand then I cropped an resized each image to 262 by 146 pixels\n\nhttps://unomaha.instructure.com/courses/33506/pages/create-a-canvas-dashboard-image-for-your-course"
  },
  {
    "objectID": "posts/oblicubes.html",
    "href": "posts/oblicubes.html",
    "title": "Oblicubes",
    "section": "",
    "text": "https://github.com/trevorld/oblicubes\n\n\nlibrary(\"bittermelon\")\n\n\nAttaching package: 'bittermelon'\n\n\nThe following object is masked from 'package:base':\n\n    which\n\nlibrary(\"grid\")\nlibrary(\"oblicubes\")\n\n\n# example from\n# https://github.com/trevorld/oblicubes\nfont_file <- system.file(\"fonts/spleen/spleen-8x16.hex.gz\", package = \"bittermelon\")\nfont <- read_hex(font_file)\nbml <- as_bm_list(\"RSTATS\", font = font)\n# Add a shadow effect and border\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"grey20\", \"lightblue\", \"darkblue\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Bio 18\nbml <- as_bm_list(\"BIO 18\", font = font)\n\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Math 32\nbml <- as_bm_list(\"MATH32\", font = font)\n\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Spark\nbml <- as_bm_list(\"SPORTS\", font = font)\n\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))"
  },
  {
    "objectID": "posts/intentional_walks/ibb.html",
    "href": "posts/intentional_walks/ibb.html",
    "title": "Intentional Walks",
    "section": "",
    "text": "Today’s exploration and visualization practive was inspired by this tweet:\n“Barry Bonds has over 200 more intentional walks than the Rays entire franchise.” — Cespedes Family BBQ, Dec. 2, 2013\n“Almost nine years later and Bonds still has 28 more intentional walks than the Rays franchise.” — Jim Passon, Aug. 25, 2022"
  },
  {
    "objectID": "posts/intentional_walks/ibb.html#gathering-the-data",
    "href": "posts/intentional_walks/ibb.html#gathering-the-data",
    "title": "Intentional Walks",
    "section": "Gathering the Data",
    "text": "Gathering the Data\nAt first, I thought that I could reasonably type in all of the data, but Bonds’ career was quite long. I copy-and-pasted the batting table from Baseball Reference and focused on the Year and IBB columns.\nThe franchise page for the Tampa Bay Rays did not have intentional walks quickly accessible, so I simply went through the year-by-year pages and gathered the IBB total (fortunately easy to find visually as the last column).\n\ndf_bonds <- readxl::read_xlsx(\"ibb.xlsx\", sheet = \"Bonds\")\ndf_rays  <- readxl::read_xlsx(\"ibb.xlsx\", sheet = \"Rays\")"
  },
  {
    "objectID": "posts/intentional_walks/ibb.html#data-wrangling",
    "href": "posts/intentional_walks/ibb.html#data-wrangling",
    "title": "Intentional Walks",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nWhile I could probably affect the horizontal axis later in a ggplot visualization, I prefer today to have the data in one data frame. Let me merge the data.\n\ndf <- df_bonds |>\n  full_join(df_rays, by = \"Year\") |>\n  rename(\"IBB_Bonds\" = \"IBB.x\",\n        \"IBB_Rays\" = \"IBB.y\")\n\nThe naturally missing values probably would not affect the calculations much, but for peace of mind, let us impute those values to be zeroes.\n\ndf$IBB_Bonds[is.na(df$IBB_Bonds)] <- 0\ndf$IBB_Rays[is.na(df$IBB_Rays)]   <- 0\n\nIn the spirit of the tweet, we will need to talk about cumulative totals.\n\ndf <- df |>\n  mutate(IBB_Bonds_total = cumsum(IBB_Bonds),\n         IBB_Rays_total  = cumsum(IBB_Rays))"
  },
  {
    "objectID": "posts/intentional_walks/ibb.html#data-visualization",
    "href": "posts/intentional_walks/ibb.html#data-visualization",
    "title": "Intentional Walks",
    "section": "Data Visualization",
    "text": "Data Visualization\nWhile line graphs (with areas filled) would probably be the most visually appealing, it might be better to treat seasons as discrete entries in a bar plot.\n\ndf |>\n  ggplot() +\n  geom_bar(aes(x = Year, y = IBB_Bonds_total),\n           stat = \"identity\")\n\n\n\n\nNow I am curious what the bars will look like in team colors.\n\nbaseplot <- df |>\n  ggplot() +\n  \n  # https://teamcolorcodes.com/san-francisco-giants-color-codes/\n  geom_bar(aes(x = Year, y = IBB_Bonds_total),\n           color = \"#27251F\", fill = \"#FD5A1E\",\n           stat = \"identity\") +\n  \n  # https://teamcolorcodes.com/tampa-bay-rays-color-codes/\n  geom_bar(aes(x = Year, y = IBB_Rays_total),\n           color = \"#8FBCE6\", fill = \"#092C5C\",\n           stat = \"identity\")\n\n# print\nbaseplot\n\n\n\n\nMoving toward aesthetic beauty, I will update some of the theme elements.\n\n# some ideas from\n# https://github.com/nikopech/TidyTuesday/blob/master/R/2022-08-09/2022_08_09_ferris_wheels.R\n\ncurrent_plot <- baseplot +\n  labs(title = \"\",\n       subtitle = \"\",\n       caption = \"Derek Sollberger | August 26, 2022\",\n       x = \"season\",\n       y = \"intentional walk cumulative total\") +\n  theme(legend.position = \"none\",\n        panel.background = element_blank(),\n        plot.background = element_rect(\n            fill = \"#FFFFFF\", \n            color = \"#27251F\"\n        ),\n        plot.title = element_markdown(face = \"italic\",\n                                      margin = margin(b = 5),\n                                      size = 14),\n        plot.title.position = \"plot\",\n        plot.subtitle = element_markdown(face = \"italic\",\n                                         margin = margin(b = 5),\n                                         size = 12),\n        plot.caption = element_markdown(margin = margin(t = 0), \n                                        size = 10),\n        plot.caption.position = \"plot\",\n        plot.margin = margin(50, 50, 50, 50))\n\n# print\ncurrent_plot\n\n\n\n\nNow, I am going to attempt to add arrows (and later: labels) to highlight certain areas of the graph. This is still very new to me, so fingers crossed.\n\ncurrent_plot +\n  # In 2004, Barry Bonds received 120 intentional walks\n  annotate(color = \"#FD5A1E\",\n           geom = \"curve\",\n           size = 0.5,\n           x = 1999, xend = 2004,\n           y = 800, yend = 604) +\n  geom_textbox(aes(x = 1999, y = 800,\n                   color = \"#000000\",\n                   label = \"In 2004, Barry Bonds received 120 intentional walks\"),\n               size = 2) +\n  \n  # Start of Barry Bonds' MLB career\n  annotate(color = \"#FD5A1E\",\n           geom = \"curve\",\n           size = 0.5,\n           x = 1988, xend = 1986,\n           y = 200, yend = 0) +\n  geom_textbox(aes(x = 1988, y = 200,\n                   color = \"#000000\",\n                   label = \"Start of Barry Bonds' MLB career\"),\n               size = 2)\n\n\n\n# print\n# current_plot\n\nAt the moment, I am having difficulty getting labels to naturally appear beyond the panel.\nInstead, I will focus on the title and caption.\n\n# some ideas from\n# https://github.com/nikopech/TidyTuesday/blob/master/R/2022-08-09/2022_08_09_ferris_wheels.R\n\ncurrent_plot <- baseplot +\n  labs(title = \"From 1986 to 2007, Barry Bonds accumulated 688 intentional walks\",\n       subtitle = \"From 1998 to present, the Tampa Bay Rays have accumulated 28 fewer walks\",\n       caption = \"Derek Sollberger | August 26, 2022\",\n       x = \"season\",\n       y = \"intentional walk cumulative total\") +\n  theme(legend.position = \"none\",\n        panel.background = element_blank(),\n        plot.background = element_rect(\n            fill = \"#FFFFFF\", \n            color = \"#27251F\"\n        ),\n        plot.title = element_text(color = \"#FD5A1E\", size = 14, hjust = 0.5),\n        plot.title.position = \"plot\",\n        plot.subtitle = element_text(color = \"#092C5C\", size = 12, hjust = 0.5),\n        plot.caption = element_markdown(margin = margin(t = 0), \n                                        size = 10),\n        plot.caption.position = \"plot\",\n        plot.margin = margin(20, 20, 20, 20))\n\n# print\ncurrent_plot"
  },
  {
    "objectID": "posts/coefficient_of_variation.html",
    "href": "posts/coefficient_of_variation.html",
    "title": "Coefficient of Variation",
    "section": "",
    "text": "For a future lecture in my Sports Analytics course, I want an example of a baseball statistic where the averages for two players are similar, but their variances in that same statistic are quite different. It is still early in the semester, so I am looking for an easy-to-understand statistic. Therefore, I will explore home runs per season."
  },
  {
    "objectID": "posts/coefficient_of_variation.html#a-walk-through-the-data",
    "href": "posts/coefficient_of_variation.html#a-walk-through-the-data",
    "title": "Coefficient of Variation",
    "section": "A Walk Through the Data",
    "text": "A Walk Through the Data\nThe Lahman database contains a lot of baseball statistics, and today I will focus on the Batting data frame.\n\nlibrary(\"Lahman\")\nlibrary(\"tidyverse\")\n\nFor interest, I will filter the observations to retain the players from the past 18 seasons (since my students are about 18 years old, haha) and player-seasons that included at least 100 at-bats.\n\ndf <- Batting |>\n  filter(yearID >= (2021 - 18)) |>\n  filter(AB >= 100)\n\nTo be illustrative, permit me to select mainly the seasons and home run columns.\n\ndf <- df |>\n  select(playerID, yearID, AB, HR)\n\nAt the moment, here is what our data looks like.\n\nhead(df)\n\n   playerID yearID  AB HR\n1 abreubo01   2003 577 20\n2 alfoned01   2003 514 13\n3 almoner01   2003 100  1\n4 alomaro01   2003 263  2\n5 alomaro01   2003 253  3\n6 alomasa02   2003 194  5\n\n\nSince I am concerned with season averages, I am going to group_by the player name. From there, let us then compute the averages and standard deviations for home runs.\n\ndf <- df |>\n  group_by(playerID) |>\n  mutate(xbar = mean(HR, na.rm = TRUE),\n         s    = sd(HR, na.rm = TRUE)) |>\n  ungroup()\nhead(df)\n\n# A tibble: 6 × 6\n  playerID  yearID    AB    HR  xbar      s\n  <chr>      <int> <int> <int> <dbl>  <dbl>\n1 abreubo01   2003   577    20 14.3   8.94 \n2 alfoned01   2003   514    13  8.67  5.86 \n3 almoner01   2003   100     1  1    NA    \n4 alomaro01   2003   263     2  2.67  0.577\n5 alomaro01   2003   253     3  2.67  0.577\n6 alomasa02   2003   194     5  2.33  2.52 \n\n\nFor various reasons, some players only appear once in this subset of data, so their variance is effectively zero (missing in the computation). To answer my original inquiry mathematically, I will now compute the coefficient of variation (and avoid a divide-by-zero error from those players who hit zero home runs).\n\\[CoV = \\frac{s}{\\bar{x}}\\]\n\ndf <- df |>\n  # filter(!is.na(s)) |>\n  filter(xbar > 0) |>\n  mutate(CoV = s/xbar) |>\n  arrange(desc(CoV))\n\nIn this metric, the top scores are\n\nhead(df)\n\n# A tibble: 6 × 7\n  playerID  yearID    AB    HR  xbar     s   CoV\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>\n1 gathrjo01   2005   203     0  0.2  0.447  2.24\n2 gathrjo01   2006   154     0  0.2  0.447  2.24\n3 gathrjo01   2006   229     1  0.2  0.447  2.24\n4 gathrjo01   2007   228     0  0.2  0.447  2.24\n5 gathrjo01   2008   279     0  0.2  0.447  2.24\n6 burriem01   2008   240     1  0.25 0.5    2   \n\n\nand the bottom scores are\n\ntail(df)\n\n# A tibble: 6 × 7\n  playerID  yearID    AB    HR  xbar     s   CoV\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>\n1 wadela01    2021   336    18    18    NA    NA\n2 wallsta01   2021   152     1     1    NA    NA\n3 walshja01   2021   530    29    29    NA    NA\n4 whiteel04   2021   198     6     6    NA    NA\n5 williju02   2021   119     4     4    NA    NA\n6 wisdopa01   2021   338    28    28    NA    NA"
  },
  {
    "objectID": "posts/coefficient_of_variation.html#heavy-hitters",
    "href": "posts/coefficient_of_variation.html#heavy-hitters",
    "title": "Coefficient of Variation",
    "section": "Heavy Hitters",
    "text": "Heavy Hitters\nSo far, these results seem to be fine mathematically, but they might be uninteresting to the casual baseball fan. To further prune down to recognizable players, let me further filter the data down to players with at least 5 of these 100+ at-bat seasons in the past 18 seasons—and had an average of over 10 home runs per season.\n\ndf <- df |>\n  group_by(playerID) |>\n  mutate(seasons = n()) |>\n  ungroup() |>\n  filter(seasons >= 5) |>\n  filter(xbar >= 5) |>\n  arrange(desc(CoV))\n\nNow, the top scores in using the coefficient of variation as my metric are\n\nhead(df)\n\n# A tibble: 6 × 8\n  playerID  yearID    AB    HR  xbar     s   CoV seasons\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>   <int>\n1 santada01   2014   405     7   7.5  10.3  1.38       6\n2 santada01   2015   261     0   7.5  10.3  1.38       6\n3 santada01   2016   233     2   7.5  10.3  1.38       6\n4 santada01   2017   143     3   7.5  10.3  1.38       6\n5 santada01   2019   474    28   7.5  10.3  1.38       6\n6 santada01   2021   116     5   7.5  10.3  1.38       6\n\n\nand the lowest \\(Cov\\) are\n\ntail(df)\n\n# A tibble: 6 × 8\n  playerID  yearID    AB    HR  xbar     s   CoV seasons\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>   <int>\n1 delgaca01   2003   570    42  34.5  6.32 0.183       6\n2 delgaca01   2004   458    32  34.5  6.32 0.183       6\n3 delgaca01   2005   521    33  34.5  6.32 0.183       6\n4 delgaca01   2006   524    38  34.5  6.32 0.183       6\n5 delgaca01   2007   538    24  34.5  6.32 0.183       6\n6 delgaca01   2008   598    38  34.5  6.32 0.183       6\n\n\nWell, I do recognize more of the player names, but I realize that the \\(Cov\\) alone does not completely solve my question since I still wanted similar averages between two players."
  },
  {
    "objectID": "posts/coefficient_of_variation.html#a-metric-on-top-of-a-metric",
    "href": "posts/coefficient_of_variation.html#a-metric-on-top-of-a-metric",
    "title": "Coefficient of Variation",
    "section": "A Metric on top of a Metric",
    "text": "A Metric on top of a Metric\nNow, hear me out. If there are two players \\(A\\) and \\(B\\), then what I am looking for is a rate of change of the form\n\\[Y = \\frac{\\text{CoV}_{A} - \\text{CoV}_{B}}{\\bar{x}_{A} - \\bar{x}_{B}}\\]"
  },
  {
    "objectID": "posts/cofficient_of_variation/coefficient_of_variation.html",
    "href": "posts/cofficient_of_variation/coefficient_of_variation.html",
    "title": "Coefficient of Variation",
    "section": "",
    "text": "For a future lecture in my Sports Analytics course, I want an example of a baseball statistic where the averages for two players are similar, but their variances in that same statistic are quite different. It is still early in the semester, so I am looking for an easy-to-understand statistic. Therefore, I will explore home runs per season."
  },
  {
    "objectID": "posts/cofficient_of_variation/coefficient_of_variation.html#a-walk-through-the-data",
    "href": "posts/cofficient_of_variation/coefficient_of_variation.html#a-walk-through-the-data",
    "title": "Coefficient of Variation",
    "section": "A Walk Through the Data",
    "text": "A Walk Through the Data\nThe Lahman database contains a lot of baseball statistics, and today I will focus on the Batting data frame.\n\nlibrary(\"Lahman\")\nlibrary(\"tidyverse\")\n\nFor interest, I will filter the observations to retain the players from the past 18 seasons (since my students are about 18 years old, haha) and player-seasons that included at least 100 at-bats.\n\ndf <- Batting |>\n  filter(yearID >= (2021 - 18)) |>\n  filter(AB >= 100)\n\nTo be illustrative, permit me to select mainly the seasons and home run columns.\n\ndf <- df |>\n  select(playerID, yearID, AB, HR)\n\nAt the moment, here is what our data looks like.\n\nhead(df)\n\n   playerID yearID  AB HR\n1 abreubo01   2003 577 20\n2 alfoned01   2003 514 13\n3 almoner01   2003 100  1\n4 alomaro01   2003 263  2\n5 alomaro01   2003 253  3\n6 alomasa02   2003 194  5\n\n\nSince I am concerned with season averages, I am going to group_by the player name. From there, let us then compute the averages and standard deviations for home runs.\n\ndf <- df |>\n  group_by(playerID) |>\n  mutate(xbar = mean(HR, na.rm = TRUE),\n         s    = sd(HR, na.rm = TRUE)) |>\n  ungroup()\nhead(df)\n\n# A tibble: 6 × 6\n  playerID  yearID    AB    HR  xbar      s\n  <chr>      <int> <int> <int> <dbl>  <dbl>\n1 abreubo01   2003   577    20 14.3   8.94 \n2 alfoned01   2003   514    13  8.67  5.86 \n3 almoner01   2003   100     1  1    NA    \n4 alomaro01   2003   263     2  2.67  0.577\n5 alomaro01   2003   253     3  2.67  0.577\n6 alomasa02   2003   194     5  2.33  2.52 \n\n\nFor various reasons, some players only appear once in this subset of data, so their variance is effectively zero (missing in the computation). To answer my original inquiry mathematically, I will now compute the coefficient of variation (and avoid a divide-by-zero error from those players who hit zero home runs).\n\\[CoV = \\frac{s}{\\bar{x}}\\]\n\ndf <- df |>\n  # filter(!is.na(s)) |>\n  filter(xbar > 0) |>\n  mutate(CoV = s/xbar) |>\n  arrange(desc(CoV))\n\nIn this metric, the top scores are\n\nhead(df)\n\n# A tibble: 6 × 7\n  playerID  yearID    AB    HR  xbar     s   CoV\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>\n1 gathrjo01   2005   203     0  0.2  0.447  2.24\n2 gathrjo01   2006   154     0  0.2  0.447  2.24\n3 gathrjo01   2006   229     1  0.2  0.447  2.24\n4 gathrjo01   2007   228     0  0.2  0.447  2.24\n5 gathrjo01   2008   279     0  0.2  0.447  2.24\n6 burriem01   2008   240     1  0.25 0.5    2   \n\n\nand the bottom scores are\n\ntail(df)\n\n# A tibble: 6 × 7\n  playerID  yearID    AB    HR  xbar     s   CoV\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>\n1 wadela01    2021   336    18    18    NA    NA\n2 wallsta01   2021   152     1     1    NA    NA\n3 walshja01   2021   530    29    29    NA    NA\n4 whiteel04   2021   198     6     6    NA    NA\n5 williju02   2021   119     4     4    NA    NA\n6 wisdopa01   2021   338    28    28    NA    NA"
  },
  {
    "objectID": "posts/cofficient_of_variation/coefficient_of_variation.html#heavy-hitters",
    "href": "posts/cofficient_of_variation/coefficient_of_variation.html#heavy-hitters",
    "title": "Coefficient of Variation",
    "section": "Heavy Hitters",
    "text": "Heavy Hitters\nSo far, these results seem to be fine mathematically, but they might be uninteresting to the casual baseball fan. To further prune down to recognizable players, let me further filter the data down to players with at least 5 of these 100+ at-bat seasons in the past 18 seasons—and had an average of over 10 home runs per season.\n\ndf <- df |>\n  group_by(playerID) |>\n  mutate(seasons = n()) |>\n  ungroup() |>\n  filter(seasons >= 5) |>\n  filter(xbar >= 15) |>\n  arrange(desc(CoV))\n\nNow, the top scores in using the coefficient of variation as my metric are\n\nhead(df)\n\n# A tibble: 6 × 8\n  playerID  yearID    AB    HR  xbar     s   CoV seasons\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>   <int>\n1 solerjo01   2015   366    10  16.3  14.2 0.869       7\n2 solerjo01   2016   227    12  16.3  14.2 0.869       7\n3 solerjo01   2018   223     9  16.3  14.2 0.869       7\n4 solerjo01   2019   589    48  16.3  14.2 0.869       7\n5 solerjo01   2020   149     8  16.3  14.2 0.869       7\n6 solerjo01   2021   308    13  16.3  14.2 0.869       7\n\n\nand the lowest \\(Cov\\) are\n\ntail(df)\n\n# A tibble: 6 × 8\n  playerID  yearID    AB    HR  xbar     s   CoV seasons\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>   <int>\n1 delgaca01   2003   570    42  34.5  6.32 0.183       6\n2 delgaca01   2004   458    32  34.5  6.32 0.183       6\n3 delgaca01   2005   521    33  34.5  6.32 0.183       6\n4 delgaca01   2006   524    38  34.5  6.32 0.183       6\n5 delgaca01   2007   538    24  34.5  6.32 0.183       6\n6 delgaca01   2008   598    38  34.5  6.32 0.183       6\n\n\nWell, I do recognize more of the player names, but I realize that the \\(Cov\\) alone does not completely solve my question since I still wanted similar averages between two players."
  },
  {
    "objectID": "posts/cofficient_of_variation/coefficient_of_variation.html#a-metric-on-top-of-a-metric",
    "href": "posts/cofficient_of_variation/coefficient_of_variation.html#a-metric-on-top-of-a-metric",
    "title": "Coefficient of Variation",
    "section": "A Metric on top of a Metric",
    "text": "A Metric on top of a Metric\nNow, hear me out. If there are two players \\(A\\) and \\(B\\), then what I am looking for is a rate of change of the form\n\\[Y = \\bigg|\\frac{s_{A} - s_{B}}{\\bar{x}_{A} - \\bar{x}_{B}}\\bigg|\\]\nFirst, let me simplify the data frame down to just the player names, home run averages, and their standard deviations.\n\ndf2 <- df |>\n  select(playerID, xbar, s) |>\n  distinct()\nhead(df2)\n\n# A tibble: 6 × 3\n  playerID   xbar     s\n  <chr>     <dbl> <dbl>\n1 solerjo01  16.3  14.2\n2 valenjo03  16.2  13.3\n3 muncyma01  20.5  16.6\n4 yelicch01  17.8  13.8\n5 moralke01  16.1  11.7\n6 carpema01  15.5  11.1\n\n\nPresently, I have about 221 observations, so a pair-wise calculations would be computed over about 4.8841^{4} pairs (hopefully manageable by a computer).\n\nN <- nrow(df2)\ndf3 <- data.frame(player1 = rep(NA, N^2),\n                  player2 = rep(NA, N^2),\n                  Y       = rep(NA, N^2))\n\nfor(i in 1:N){\n  for(j in 1:N){\n    row_number = j*(i-1) + j\n    df3$player1[row_number] <- df2$playerID[i]\n    df3$player2[row_number] <- df2$playerID[j]\n    \n    if(i == j){\n      this_Y_value <- 0\n    } else {\n      denominator <- df2$xbar[i] - df2$xbar[j]\n      if(denominator == 0){ denominator <- 0.1 }\n      \n      this_Y_value <- abs((df2$s[i] - df2$s[j]) / denominator)\n    }\n    \n    df3$Y[row_number] <- this_Y_value\n  }\n  \n  # if((row_number %% 10000) == 0){\n  #   print(paste(\"Currently computing row number\", row_number))\n  # }\n}\n\nIn this improvised metric, the top 10 scores were\n\ndf3 |>\n  arrange(desc(Y)) |>\n  top_n(10)\n\nSelecting by Y\n\n\n     player1   player2        Y\n1  mccutan01 ramirjo01 692.3109\n2  polloaj01 utleych01 348.8826\n3  mccutan01 ensbemo01 291.9631\n4  hosmeer01 quentca01 244.8742\n5  mccanbr01  belljo02 209.0920\n6  jacobmi02   bayja01 198.8301\n7   kentje01 giambja01 190.9222\n8  jonesga02 kinslia01 182.2589\n9  contrwi01  shawtr01 180.9609\n10 polloaj01 morrilo01 179.5927\n\n\nUsing the playerInfo() helper function in the Lahman package, we can verify the names of the players.\n\nplayerInfo(\"mccutan01\")\n\n       playerID nameFirst  nameLast\n11832 mccutan01    Andrew McCutchen\n\nplayerInfo(\"ramirjo01\")\n\n       playerID nameFirst nameLast\n14966 ramirjo01      Jose  Ramirez"
  },
  {
    "objectID": "posts/cofficient_of_variation/coefficient_of_variation.html#data-visualization",
    "href": "posts/cofficient_of_variation/coefficient_of_variation.html#data-visualization",
    "title": "Coefficient of Variation",
    "section": "Data Visualization",
    "text": "Data Visualization\nThis whole time, I was hoping for a neat boxplot.\n\ndf |>\n  filter(playerID == \"mccutan01\" | playerID == \"ramirjo01\") |>\n  ggplot(aes(x = playerID, y = HR)) +\n  geom_boxplot(color = c(\"#27251F\", \"#00385D\"),\n               fill = c(\"#FDB827\", \"#E50022\")) +\n  stat_summary(fun=mean, geom=\"point\", shape=20, size=14, color=\"blue\", fill=\"blue\") +\n  scale_x_discrete(labels = c(\"Andrew McCutchen\", \"Jose Ramirez\")) +\n  labs(title = \"Similar Means, Different Variances\",\n       subtitle = stringr::str_wrap(\"Andrew McCutchen and Jose Ramirez have averaged about 20.4 home runs per season, but in different ways (Qualifiers: after 2002 season, 100+ AB seasons, at least 5 100+ AB seasons, home run average over 15 HR/season)\"),\n       caption = \"Derek Sollberger, 2022-09-02\",\n       x = \"MLB Player\",\n       y = \"home runs in a season\") +\n  theme(axis.text.x = element_text(size = 15),\n        legend.position = \"none\",\n        panel.background = element_blank(),\n        plot.background = element_rect(\n            fill = \"#FFFFFF\", \n            color = \"#27251F\"\n        ),\n        plot.title = element_text(color = \"#E50022\", size = 20, hjust = 0.5),\n        plot.title.position = \"plot\",\n        plot.subtitle = element_text(color = \"blue\", size = 12, hjust = 0.0),\n        plot.caption = element_text(color = \"#092C5C\", size = 10, hjust = 1.0),\n        plot.caption.position = \"plot\",\n        plot.margin = margin(20, 20, 20, 20))"
  },
  {
    "objectID": "posts/Settlement_Survival/Settlement_Survival.html",
    "href": "posts/Settlement_Survival/Settlement_Survival.html",
    "title": "Settlement Survival Start",
    "section": "",
    "text": "library(\"DiagrammeR\")\n\nI am going to take some liberties and actually add more prerequisites in the beginning (otherwise, the graph simply starts with about 12 unlinked nodes).\n\nmy_plot <- DiagrammeR::mermaid(\"\n  graph TD\n  start[Select Location]\n  \n  house1[1 House]\n  farms[2 Standard Fields]\n  water[Big Well]\n  forest_hut[Forester's Hut]\n  gather_hut[Gatherer's Hut]\n  hunter_hut[Hunter's Hut]\n  chopping_house[Chopping House]\n  house7[7 Houses]\n  repair_shop[Repair Shop]\n  church[Church]\n  clinic[Clinic]\n  distillery[Distillery]\n  \n  start --> house1\n  start --> farms\n  start --> water\n  start --> gather_hut\n  \n  house1 --> house7\n  farms --> clinic\n  gather_hut --> forest_hut\n  gather_hut --> hunter_hut\n  water --> repair_shop\n  \n  forest_hut --> chopping_house\n  \n  house7 --> church\n  repair_shop --> distillery\n\")\n\n\n# print\nmy_plot"
  },
  {
    "objectID": "posts/patchwork/patchwork.html",
    "href": "posts/patchwork/patchwork.html",
    "title": "Patchwork",
    "section": "",
    "text": "library(\"patchwork\")\nlibrary(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.1\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ncorrelatedValues = function(x, r = 0.9){\n  r2 = r**2\n  ve = 1-r2\n  SD = sqrt(ve)\n  e  = rnorm(length(x), mean=0, sd=SD)\n  y  = r*x + e\n  return(y)\n}\n\nEarlier tonight, I was making these plots for a quick lecture about correlation, so let me just grab some copies.\n\nx <- rnorm(100, mean = 0, sd = 1)\ny <- correlatedValues(x, r = -0.9)\n\ncor_value <- cor(x,y, use = \"pairwise.complete.obs\")\n\ndf_for_graph <- data.frame(x,y)\np1 <- df_for_graph |>\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(title = \"Correlation Example\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", strongly and negatively correlated\"),\n       caption = \"Spark 01\")\n\n\nx <- rnorm(100, mean = 0, sd = 1)\ny <- correlatedValues(x, r = -0.5)\n\ncor_value <- cor(x,y, use = \"pairwise.complete.obs\")\n\ndf_for_graph <- data.frame(x,y)\np2 <- df_for_graph |>\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(title = \"Correlation Example\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", slightly and negatively correlated\"),\n       caption = \"Spark 01\")\n\n\nx <- rnorm(100, mean = 0, sd = 1)\ny <- correlatedValues(x, r = 0)\n\ncor_value <- cor(x,y, use = \"pairwise.complete.obs\")\n\ndf_for_graph <- data.frame(x,y)\np3 <- df_for_graph |>\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(title = \"Correlation Example\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", virtually uncorrelated\"),\n       caption = \"Spark 01\")\n\n\nx <- rnorm(100, mean = 0, sd = 1)\ny <- correlatedValues(x, r = 0.5)\n\ncor_value <- cor(x,y, use = \"pairwise.complete.obs\")\n\ndf_for_graph <- data.frame(x,y)\np4 <- df_for_graph |>\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(title = \"Correlation Example\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", slightly and positively correlated\"),\n       caption = \"Spark 01\")\n\n\nx <- rnorm(100, mean = 0, sd = 1)\ny <- correlatedValues(x, r = 0.9)\n\ncor_value <- cor(x,y, use = \"pairwise.complete.obs\")\n\ndf_for_graph <- data.frame(x,y)\np5 <- df_for_graph |>\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(title = \"Correlation Example\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", strongly and positively correlated\"),\n       caption = \"Spark 01\")\n\nNow for the patchwork\n\n#patchwork\n(p1 + p2 + p3) / (p4 + p5)\n\n\n\n\nNow for the annotation.\n\noverall_plot <- (p1 + p2 + p3) / (p4 + p5)\n\noverall_plot + plot_annotation(\n  title = \"Overall Title\",\n  subtitle = \"overall subtitle\",\n  caption = \"overall caption\"\n)\n\n\n\n\nGreat! Everything is working as planned."
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html",
    "href": "posts/wonderwall/wonderwall.html",
    "title": "Anyway Heres Wonderwall",
    "section": "",
    "text": "library(\"patchwork\")\nlibrary(\"tidyverse\")"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#frets",
    "href": "posts/wonderwall/wonderwall.html#frets",
    "title": "Anyway Heres Wonderwall",
    "section": "Frets",
    "text": "Frets\nI think I will start with the frets. (Disclaimer: I am very much just a beginner with playing guitar, and my descriptions may also be elementary.) I need to represent about 5 rows of space along with 6 columns of strings.\n\ndf <- data.frame(x = 1:6, y = 0:5) #generic data\n\ndf |>\n  ggplot(aes(x = x, y = y))\n\n\n\n\nGuitar chord diagrams are not necessarily scaled with equally spaced units in the horizontal and vertical directions, but that might make things easier for me in this recreational side project. Also, I will apply theme_linedraw temporarily to show what I am thinking.\n\ndf |>\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  theme_linedraw()\n\n\n\n\nThe \\(y\\) axis has the math-class default, but guitar notation goes in the other direction.\n\n# https://stackoverflow.com/questions/70193132/how-to-reverse-y-axis-values-ggplot2-r\ndf |>\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  scale_y_reverse() +\n  theme_linedraw()\n\n\n\n\nWhen I remove theme_linedraw (analogy: removing the training wheels from a bicycle), I will need to (at least)\n\nturn off the background color\nturn off the grid line colors\nturn off the border lines too\n\n\ndf |>\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  scale_y_reverse() +\n  theme(\n    panel.background = element_blank()\n  )\n\n\n\n\nActually, let’s go ahead and give the background a “wood” color.\n\n# https://redketchup.io/color-picker\ndf |>\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  scale_y_reverse() +\n  theme(\n    panel.background = element_rect(fill = \"#2D2C32\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nLet’s see if we can cutoff the plot at the top (“nut”) and right side of the plot.\n\ndf |>\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  scale_y_reverse() +\n  theme(\n    panel.background = element_rect(fill = \"#2D2C32\"),\n    panel.grid.minor = element_blank()\n  ) +\n  xlim(c(1,6)) +\n  ylim(c(0,5))\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nThat did not work (and produced a warning because I affected the \\(y\\) axis twice), so perhaps a better strategy is to make a colorful rectangle manually.\n\ndf |>\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  geom_rect(aes(xmin = 1, xmax = 6, ymin = 0, ymax = 5),\n            fill = \"#2D2C32\") +\n  scale_y_reverse() +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nThis whole time, I have been thinking about making line segments of certain colors for the frets, strings, and the nut. (Note: silver is not a base color in R??)\n\ndf_frets <- data.frame(x1 = 1, x2 = 6, y = 1:5)\ndf_strings <- data.frame(x = 1:6, y1 = 0, y2 = 5)\n\ndf |>\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  geom_rect(aes(xmin = 1, xmax = 6, ymin = 0, ymax = 5),\n            fill = \"#2D2C32\") +\n  geom_segment(aes(x = x1, y = y, xend = x2, yend = y),\n            color = \"gray50\", data = df_frets) +\n  geom_segment(aes(x = 1, y = 0, xend = 6, yend = 0),\n               color = \"tan\", size = 3) +\n  geom_segment(aes(x = x, y = y1, xend = x, yend = y2),\n            color = \"#C0C0C0\", data = df_strings, size = 2) +\n  scale_y_reverse() +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nThis looks great in my opinion. Let’s turn off the axis tick marks and labels, and save this as a base plot.\n\nfret_background <- df |>\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  geom_rect(aes(xmin = 1, xmax = 6, ymin = 0, ymax = 5),\n            fill = \"#2D2C32\") +\n  geom_segment(aes(x = x1, y = y, xend = x2, yend = y),\n            color = \"gray50\", data = df_frets) +\n  geom_segment(aes(x = 1, y = 0, xend = 6, yend = 0),\n               color = \"tan\", size = 3) +\n  geom_segment(aes(x = x, y = y1, xend = x, yend = y2),\n            color = \"#C0C0C0\", data = df_strings, size = 2) +\n  scale_y_reverse() +\n  theme(\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    panel.background = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n# print\nfret_background"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#first-chord-em",
    "href": "posts/wonderwall/wonderwall.html#first-chord-em",
    "title": "Anyway Heres Wonderwall",
    "section": "First Chord (Em)",
    "text": "First Chord (Em)\nAt first, I was thinking of using geom_label to indicate the finger positions, but I bet that using geom_point and then geom_text has more customization options.\n\ndf_em <- data.frame(x = c(2,3), y = c(2,2) - 0.5, label = c(\"1\", \"2\"))\nfret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_em, size = 15) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_em, size = 10)\n\n\n\n\nLet us go ahead and label this chord. Note that I want the dots to appear in between the frets (hence the “-0.5” in the \\(y\\) coordinates).\n\ndf_em <- data.frame(x = c(2,3), y = c(2,2) - 0.5, label = c(\"1\", \"2\"))\n\nchord_em <- fret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_em, size = 12) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_em, size = 8) +\n  labs(title = \"Em\") +\n  theme(plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\n\n# print\nchord_em"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#the-other-four-chords",
    "href": "posts/wonderwall/wonderwall.html#the-other-four-chords",
    "title": "Anyway Heres Wonderwall",
    "section": "The Other Four Chords",
    "text": "The Other Four Chords\n\ndf_g <- data.frame(x = c(2,1,6), y = c(2,3,3) - 0.5, label = c(\"1\", \"2\", \"4\"))\n\nchord_g <- fret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_g, size = 12) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_g, size = 8) +\n  labs(title = \"G\") +\n  theme(plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\n\n# print\nchord_g\n\n\n\n\nThis is something that I worried about. In guitar chord diagrams, there are indicators to tell the player if a string is played open (indicated with an “O”, that is that no fingers are placed on the strings while the string is strung) or if a string is closed (not strung at all, indicated with an “X”), but these indicators are placed above the nut—for our purposes: outside of the graph.\n\ndf_d <- data.frame(x = c(4,6,5), y = c(2,2,3) - 0.5, label = c(\"1\", \"2\", \"3\"))\n\nchord_d <- fret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_d, size = 12) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_d, size = 8) +\n  geom_text(aes(x = 1, y = -0.2, label = \"X\"), size = 5) +\n  geom_text(aes(x = 2, y = -0.2, label = \"X\"), size = 5) +\n  labs(title = \"D\") +\n  theme(plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\n\n# print\nchord_d\n\n\n\n\n\ndf_a7sus4 <- data.frame(x = c(3,5), y = c(2,3) - 0.5, label = c(\"1\", \"2\"))\n\nchord_a7sus4 <- fret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_a7sus4, size = 12) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_a7sus4, size = 8) +\n  geom_text(aes(x = 1, y = -0.2, label = \"X\"), size = 5) +\n  labs(title = \"A7sus4\") +\n  theme(plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\n\n# print\nchord_a7sus4\n\n\n\n\n\ndf_c <- data.frame(x = c(5,3,2), y = c(1,2,3) - 0.5, label = c(\"1\", \"2\", \"3\"))\n\nchord_c <- fret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_c, size = 12) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_c, size = 8) +\n  geom_text(aes(x = 1, y = -0.2, label = \"X\"), size = 5) +\n  labs(title = \"C\") +\n  theme(plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\n\n# print\nchord_c"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#first-draft",
    "href": "posts/wonderwall/wonderwall.html#first-draft",
    "title": "Anyway Heres Wonderwall",
    "section": "First Draft",
    "text": "First Draft\nWith 5 chords, I am imagining a layout of plots with about 2 rows.\n\n(chord_em + chord_g + chord_d) / (chord_a7sus4 + chord_c)"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#a-caption-tile",
    "href": "posts/wonderwall/wonderwall.html#a-caption-tile",
    "title": "Anyway Heres Wonderwall",
    "section": "A Caption Tile",
    "text": "A Caption Tile\nYes, I figured that I would want to squeeze in a “square” of text somewhere to even out the image.\n\n# https://statisticsglobe.com/plot-only-text-in-r\ncaption_plot <- ggplot() +\n  annotate(\"text\",\n           x = 1, y = 1,\n           size = 5,\n           label = \"Wonderwall by Oasis\\n \\nsimplified tab\\nvia Ultimate Guitar\\ncapo: 2nd fret\\nkey: F#m\") +\n  coord_equal() +\n  theme(\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    panel.background = element_rect(fill = \"#2D2C32\"),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n# print\ncaption_plot\n\n\n\n\nNow to see if this “caption plot” fits in nicely.\n\n(chord_em + chord_g + chord_d) / (chord_a7sus4 + caption_plot +chord_c)\n\n\n\n\nAt the time of this writing, it is near midnight, and my motivation for refinement is decreasing rapidly, lol.\n\nmain_plot <- (chord_em + chord_g + chord_d) / (chord_a7sus4 + caption_plot +chord_c)"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#strumming-pattern",
    "href": "posts/wonderwall/wonderwall.html#strumming-pattern",
    "title": "Anyway Heres Wonderwall",
    "section": "Strumming Pattern",
    "text": "Strumming Pattern\nI, and I assume other beginners, do not natually pick up a strumming pattern by ear. Fortunately, the tab did include that. But how do I add the pattern to this diagram?\n\n# https://community.rstudio.com/t/up-and-down-arrows-inside-r/76287\n\n# first, I will type into words, and then do a search-and-replace\n# down, pause, down, pause, down, pause, down, up, down, up, down, pause, down, pause, down, up, down, up, down, pause, down, pause, down, up, pause, up, pause, up, down, pause, down, pause\nstrumming_pattern <- intToUtf8(c(8595, 8196, 8595, 8196, 8595, 8196, 8595, 8593, 8595, 8593, 8595, 8196, 8595, 8196, 8595, 8593, 8595, 8593, 8595, 8196, 8595, 8196, 8595, 8593, 8196, 8593, 8196, 8593, 8595, 8196, 8595, 8196))"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#annotationg-and-echoing-the-meme",
    "href": "posts/wonderwall/wonderwall.html#annotationg-and-echoing-the-meme",
    "title": "Anyway Heres Wonderwall",
    "section": "Annotationg and Echoing the Meme",
    "text": "Annotationg and Echoing the Meme\nWith patchwork, we can furthermore add a title, subtitle, and/or caption to the overall plot.\n\nfinal_plot <- main_plot +\n  plot_annotation(title = \"Anyway, Here's Wonderwall\",\n                  caption = paste(\"strumming pattern:\", strumming_pattern, \"\\nGraph by @DerekSollberger\"),\n                  theme = theme(plot.title = element_text(\n                    size = 25, hjust = 0.5\n                  ),\n                  plot.caption = element_text(\n                    size = 20, hjust = 0.5\n                  )))\n\n# print\nfinal_plot\n\n\n\n\n\nggsave(\"wonderwall.png\", plot = final_plot, device = \"png\",\n       width = 8, height = 6, units = \"in\")\n\n\n\n\nmeme chart"
  },
  {
    "objectID": "posts/Canvas_Roster/Canvas_Roster.html",
    "href": "posts/Canvas_Roster/Canvas_Roster.html",
    "title": "Canvas_Roster",
    "section": "",
    "text": "With the goal of learning my students’ names faster, I wrote an R script that will convert a roster (“People” in a Canvas course) to a gt table for later offline use.\n\nknitr::opts_chunk$set(eval = FALSE)\n\n\nSetup\nIn your Canvas course, go to People, right-click on the roster, and click “Save Page As”\n\nbe sure to save the “complete” web page (this creates a folder with all of the students’ pictures)\nplace the also downloaded HTML file into the folder\nplace this code script into the folder\n\n\n# change this file name as needed\nHTML_file_name <- \"Course Roster S23-BIO 018 01.htm\"\n\nlibrary(\"gt\")\nlibrary(\"gtExtras\")\nlibrary(\"rvest\")\nlibrary(\"tidyverse\")\n\n\n\nRoster table\nhttps://www.r-bloggers.com/2020/04/scrape-html-table-using-rvest/\n\ncontent <- read_html(HTML_file_name)\ntables <- content |> html_table(fill = TRUE)\nroster_table <- tables[[1]]\n\nroster_df <- roster_table |>\n  select(Name, Section) |>\n  mutate(discussion_tag = str_extract(Section, \"Discussion-\\\\d\\\\dD\")) |>\n  separate(discussion_tag, sep = \"-\", into = c(\"disregard\", \"discussion_section\")) |>\n  select(Name, discussion_section) |>\n  \n  # remove instructor of record\n  filter(!is.na(discussion_section))\n\n\n\nImages\nhttps://community.rstudio.com/t/scraping-images-from-the-web/133239\n\nimage_urls <- content |> html_elements(\"img\")\nnum_images <- length(image_urls)\n\nimage_df_raw <- data.frame(html_raw = rep(NA, num_images))\nfor(img in 1:num_images){\n  image_df_raw$html_raw[img] <- as.character(image_urls[[img]])\n}\n\nimage_df <- image_df_raw |>\n  \n  # skip first two images?\n  slice(3:n()) |>\n  \n  # roundabout because some people's names have more than 2 words\n  separate(col = \"html_raw\", sep = \"alt=\", \n           into = c(\"part1\", \"part2\")) |>\n  \n  separate(col = \"part1\", sep = \" \", into = c(\"tag1\", \"src\")) |>\n  separate(col = \"src\", sep = \"/\", into = c(\"stem\", \"profile_picture\")) |>\n  separate(col = \"part2\", sep = \" aria\", into = c(\"student_name\", \"tag2\")) |>\n  \n  select(profile_picture, student_name)\n\n# remove quotation marks\nimage_df$profile_picture <- str_replace_all(image_df$profile_picture, \"\\\"\", \"\")\nimage_df$student_name <- str_replace_all(image_df$student_name, \"\\\"\", \"\")\n\n\n\nMerge\n\nroster_df <- roster_df |>\n  left_join(image_df, by = c(\"Name\" = \"student_name\")) |>\n  select(profile_picture, Name, discussion_section)\n\n\n\ngt\nhttps://jthomasmock.github.io/gtExtras/reference/gt_img_rows.html\n\nroster_gt <- roster_df |>\n  gt() |>\n  gt_img_rows(columns = profile_picture, img_source = \"local\")\n\n\ngtsave(roster_gt, \"roster.png\")"
  },
  {
    "objectID": "posts/California_weather/California_weather.html",
    "href": "posts/California_weather/California_weather.html",
    "title": "California Weather",
    "section": "",
    "text": "library(\"tidyverse\")\n\nI want to create and visualize a simple data set for my Data Science courses (that I teach in California).\n\nData Source\n\nUniversity of California\nAgriculture and Natural Resources\nStatewide Integrated Pest Management Program\nhttps://ipm.ucanr.edu/WEATHER/wxactstnames.html\n\n\n\nFixed-Width Files\nToday I learned how to read fixed-width files in the Tidyverse. From there, I simply need to give the columns easy-to-use names.\n\nSF_df <- readr::read_fwf(\"SF_2021.txt\")\n\nRows: 365 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\n\nchr  (3): X1, X4, X7\ndbl  (3): X3, X5, X6\ntime (1): X2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(SF_df) <- c(\"date\", \"time\", \"precipitation\",\n                     \"check1\", \"high\", \"low\", \"check2\")"
  },
  {
    "objectID": "posts/Christmas_plots/Christmas_plots.html",
    "href": "posts/Christmas_plots/Christmas_plots.html",
    "title": "Seasons Greetings by Riinu Pius",
    "section": "",
    "text": "a place to collect nerdy holiday plots\n\nlibrary(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n# https://gist.github.com/riinuots/52c0089bd5c41a44389e95cc8cb3943d\none = tibble(x    = c(2, 4, 3, 2.75, 2.75, 3.25, 3.25),\n             y    = c(1, 1, 6, 0,    1,    1,    0),\n            group = c(rep(\"#005a32\", 3), rep(\"#543005\", 4)))\n              \nggplot(one, aes(x, y, fill = group)) +\n  geom_polygon()\n\n\n\n\n\ntwo = tribble(~x,   ~y,\n              2.7,  2,\n              3,    3,\n              3.4,  1.6,\n              3.5,  2.5,\n              3.1,  4,\n              2.95, 5,\n              2.7,  3.7,\n              2.4,  2.45,\n              2.35, 1.7,\n              3.1,  2.1,\n              3.2,  3.1,\n              2.75, 3,\n              2.9, 1.4,\n              2.9, 4.4) %>% \n  mutate(group = \"gold\")\n\nggplot(one, aes(x, y, fill = group)) +\n  geom_polygon() +\n  geom_point(data = two, shape = 21, size = 5)"
  },
  {
    "objectID": "posts/greenhouse/greenhouse.html",
    "href": "posts/greenhouse/greenhouse.html",
    "title": "Stardew Valley Crops",
    "section": "",
    "text": "Here is some quick code to help me plan a crop layout in Stardew Valley.\n\nlibrary(\"figpatch\") #https://bradyajohnston.github.io/figpatch/\nlibrary(\"patchwork\")\n\n\n# load images\n# https://stardewvalleywiki.com/Crops\nancient_fruit <- figpatch::fig(\"Ancient_Fruit.png\")\nblueberry <- figpatch::fig(\"Blueberry.png\")\ncactus_fruit <- figpatch::fig(\"Cactus_Fruit.png\")\ncoffee_bean <- figpatch::fig(\"Coffee_Bean.png\")\ncorn <- figpatch::fig(\"Corn.png\")\ncranberries <- figpatch::fig(\"Cranberries.png\")\ngrape <- figpatch::fig(\"Grape.png\")\ngreen_bean <- figpatch::fig(\"Green_Bean.png\")\nhops <- figpatch::fig(\"Hops.png\")\nhot_pepper <- figpatch::fig(\"Hot_Pepper.png\")\npineapple <- figpatch::fig(\"Pineapple.png\")\nstrawberry <- figpatch::fig(\"Strawberry.png\")\ntea_leaves <- figpatch::fig(\"Tea_Leaves.png\")\ntomato <- figpatch::fig(\"Tomato.png\")\n\n\ngreenhouse_plot <- patchwork::wrap_plots(\n  \n  # row 1\n  grape, hops, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper,\n  \n  # row 2\n  grape, hops, tomato, tomato, tomato, tomato, tomato, tomato, tomato, tomato, tomato, tomato, \n  \n  # row 3\n  grape, tea_leaves, cranberries, cranberries, cranberries, cranberries, cranberries, cranberries, cranberries, cranberries, cranberries, cranberries, \n  \n  # row 4\n  grape, tea_leaves, strawberry, strawberry, strawberry, strawberry, strawberry, strawberry, strawberry, strawberry, strawberry, strawberry, \n  \n  # row 5\n  grape, tea_leaves, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, \n  \n  # row 6\n  green_bean, tea_leaves, corn, corn, corn, corn, corn, corn, corn, corn, corn, corn, \n  \n  # row 7\n  green_bean, tea_leaves, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, \n  \n  # row 8\n  green_bean, tea_leaves, blueberry, blueberry, blueberry, blueberry, blueberry, blueberry, blueberry, blueberry, blueberry, blueberry, \n  \n  # row 9\n  green_bean, hops, pineapple, pineapple, pineapple, pineapple, pineapple, pineapple, pineapple, pineapple, pineapple, pineapple, \n  \n  # row 10\n  green_bean, hops, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, \n  \n  ncol = 12\n)\n\n\ngreenhouse_plot +\n  patchwork::plot_annotation(title = \"Stardew Valley\",\n                  subtitle = \"crops that continue to produce\")"
  },
  {
    "objectID": "posts/curly_operator/curly_operator.html",
    "href": "posts/curly_operator/curly_operator.html",
    "title": "curly operator",
    "section": "",
    "text": "The “curly operator” was added in rlang a few years ago, and I have yet to really use it much. It came in handy during a data analyst consulting gig I had during the summer of 2021, and I should use it more.\n\nlibrary(\"palmerpenguins\")\nlibrary(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nFor instance, can I make a helper function to reduce typing out the same few lines of code that I use often?\n\nsummary_stats <- function(data_frame, grouping_variable, numerical_variable){\n  data_frame |>\n    filter(!is.na({{grouping_variable}})) |>\n    group_by({{grouping_variable}}) |>\n    summarize(min = min({{numerical_variable}}, na.rm = TRUE),\n            xbar = mean({{numerical_variable}}, na.rm = TRUE),\n            med = median({{numerical_variable}}, na.rm = TRUE),\n            s = sd({{numerical_variable}}, na.rm = TRUE),\n            max = max({{numerical_variable}}, na.rm = TRUE))\n}\n\n\nsummary_stats(penguins, species, bill_length_mm)\n\n# A tibble: 3 × 6\n  species     min  xbar   med     s   max\n  <fct>     <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Adelie     32.1  38.8  38.8  2.66  46  \n2 Chinstrap  40.9  48.8  49.6  3.34  58  \n3 Gentoo     40.9  47.5  47.3  3.08  59.6\n\n\n\nsummary_stats(penguins, island, body_mass_g)\n\n# A tibble: 3 × 6\n  island      min  xbar   med     s   max\n  <fct>     <int> <dbl> <dbl> <dbl> <int>\n1 Biscoe     2850 4716. 4775   783.  6300\n2 Dream      2700 3713. 3688.  417.  4800\n3 Torgersen  2900 3706. 3700   445.  4700"
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html",
    "title": "JupyterHub Showcase",
    "section": "",
    "text": "library(\"palmerpenguins\")\nlibrary(\"tidyverse\")"
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html#login",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html#login",
    "title": "JupyterHub Showcase",
    "section": "Login",
    "text": "Login\nAt UC Merced, our 2i2c server is located at\n\nhttps://ucmerced.2i2c.cloud\n\nEach user sees a shared and a shared-readwrite folder\n\n\n\nshared and shared-readwrite"
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html#file-structure",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html#file-structure",
    "title": "JupyterHub Showcase",
    "section": "File Structure",
    "text": "File Structure\nFor this demonstration, I have made the following directories in the root directory.\n\nExampleCourse_instructor\nExampleCourse_student\n\nalong with an ExampleCourse inside the shared-readwrite directory.\n\n\n\nfile_structure"
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html#making-an-assignment",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html#making-an-assignment",
    "title": "JupyterHub Showcase",
    "section": "Making an Assignment",
    "text": "Making an Assignment\nThis semester, I taught classes with the R programming language, and here I will continue to use R. I start my work inside of the ExampleCourse-instructor directory. We will create an assignment inside of a Jupyter notebook, but with an R kernel. In other words, click on the “R” button under “Notebook”\nThis script has been named HW1.ipynb.\nThe 2i2c server comes with the Otter Grader tools built by data science faculty at UC Berkeley. Instructors can highly customize the functionality of these files with the initialization cell (a raw code block).\n\n\n\ninitialization\n\n\nMy colleagues and I have discussed ways to ease beginning students into the skills of installing code packages. For the sake of visual brevity, here I will assume that the code packages have been installed already.\n\n# code packages\nlibrary(\"palmerpenguins\")\nlibrary(\"tidyverse\")\n\n\nstr(penguins)\n\n\nPrompts\nInstructions for the student can be made with normal typing augmented by markdown as this notebook environment is built for literate programming.\n\n\n\ninstructions are typed in markdown\n\n\n\n# HW1\n\nIn this assignment, we will build a custom function to compute sample statistics.  Pay attention to the usage of the curly braces `{{...}}`, and we will use the functions on the `palmerpenguins` data set\n\n\n1. Write a custom function called `summary_stats` that takes 3 inputs\n\n    * data_frame\n    * grouping_variable\n    * numerical_variable\n\nand outputs the `summarize` command on the following sample statistics: minimum, mean, median, standard deviation, and maximum.  Please follow the given stencil.\n\n\n\nWriting a Problem\nInside of a code cell, an instructor can type in the intended answer between # BEGIN SOLUTION and # END SOLUTION comments. From there, what the student will see falls between the # BEGIN PROMPT and # END PROMPT comments.\n\n\n\nan example coding task (from the instructors point of view)\n\n\n\n1. Write a custom function called `summary_stats` that takes 3 inputs\n\n    * data_frame\n    * grouping_variable\n    * numerical_variable\n\nand outputs the `summarize` command on the following sample statistics: minimum, mean, median, standard deviation, and maximum.  Please follow the given stencil.\n\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats <- function(data_frame, grouping_variable, numerical_variable){\n  data_frame |>\n    filter(!is.na({{grouping_variable}})) |>\n    group_by({{grouping_variable}}) |>\n    summarize(min = min({{numerical_variable}}, na.rm = TRUE),\n            xbar = mean({{numerical_variable}}, na.rm = TRUE),\n            med = median({{numerical_variable}}, na.rm = TRUE),\n            s = sd({{numerical_variable}}, na.rm = TRUE),\n            max = max({{numerical_variable}}, na.rm = TRUE))\n}\n# END SOLUTION\n. = \" # BEGIN PROMPT\nsummary_stats <- function(data_frame, grouping_variable, numerical_variable){\n  data_frame |>\n    filter(!is.na({{grouping_variable}})) |>\n    group_by({{grouping_variable}}) |>\n    summarize(min = min({{numerical_variable}}, na.rm = TRUE),\n            xbar = _____,\n            med = _____,\n            s = _____,\n            max = _____)\n}\n\" # END PROMPT\n\n\n2. Use your `summary_stats` function with the `penguins` data frame, grouped by the `species` categorical variable, on the `bill_length_mm` numerical variable.\n\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats(penguins, species, bill_length_mm)\n# END SOLUTION\n. = \" # BEGIN PROMPT\nsummary_stats(penguins, _____, _____)\n\" # END PROMPT\n\n\n3. Use your `summary_stats` function with the `penguins` data frame, grouped by the `island` categorical variable, on the `body_mass_g` numerical variable.\n\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats(penguins, island, body_mass_g)\n# END SOLUTION\n. = \" # BEGIN PROMPT\n\n\" # END PROMPT\n\n\n\nTest That\nAdvanced R programmers, especially those that make code packages, use the testthat package to create unit tests to verify that functions are working as intended. The Otter Grader framework continues this idea for making assignments in R.\n\n# If you want to check your code right now, uncomment the following line of code and run it\n# testthat::expect_equal(summary_stats(penguins, island, body_mass_g)$s[1], 782.8557, tol = 0.01)"
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html#assigning-the-assignment",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html#assigning-the-assignment",
    "title": "JupyterHub Showcase",
    "section": "Assigning the Assignment",
    "text": "Assigning the Assignment\nNow we will show the power of Otter Assign! Inside JupyterHub, open a terminal connection\n\nFile –> New –> Terminal\n\nUse the cd Unix command to navigate to the instructor files.\ncd ExampleCourse_instructor/\nSince our assignment is called HW1.ipynb, we will assign that notebook into the shared-readwrite directory. Tip: it is also a good idea to create a new directory for each homework assignment (to later manage solution files and student submissions).\notter assign HW1.ipynb ../shared-readwrite/ExampleCourse/HW1\n\n\n\nOtter Assign\n\n\nVerify that the HW1 directory was created within shared-readwrite/ExampleCourse."
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html#student-view",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html#student-view",
    "title": "JupyterHub Showcase",
    "section": "Student View",
    "text": "Student View\nWe also find our HW1 directory inside the shared directory. The shared directory is in a read-only state, so students will not be able to edit and save their work there.\nThe easiest route is for a student to download the HW1.ipynb file and then upload it into their ExampleCourse_student directory. Advanced users can use Unix commands for this copy.\ncp shared/ExampleCourse/HW1/student/HW1.ipynb ExampleCourse_student\nNotice how the student receives only the partial prompts and is ready for some homework!\n\n\n\nstudent view"
  },
  {
    "objectID": "posts/Dartmouth_Atlas_data/Dartmouth_Atlas_data.html",
    "href": "posts/Dartmouth_Atlas_data/Dartmouth_Atlas_data.html",
    "title": "hospital_data",
    "section": "",
    "text": "https://data.dartmouthatlas.org/\n\nlibrary(\"sf\")\n\nWarning: package 'sf' was built under R version 4.2.3\n\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(\"tidyverse\")\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\n\nmap_data <- sf::read_sf(\"HRR_Bdry__AK_HI_unmodified/hrr-shapefile/Hrr98Bdry_AK_HI_unmodified.shp\")\n\nmap_data |>\n  ggplot() +\n  geom_sf()"
  },
  {
    "objectID": "posts/JupyterHub_showcase/HW1.html",
    "href": "posts/JupyterHub_showcase/HW1.html",
    "title": "The Median Data Scientist",
    "section": "",
    "text": "BEGIN ASSIGNMENT init_cell: false export_cell: true\n\n# code packages\nlibrary(\"palmerpenguins\")\nlibrary(\"tidyverse\")\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\n\nHW1\nIn this assignment, we will build a custom function to compute sample statistics. Pay attention to the usage of the curly braces {{...}}, and we will use the functions on the palmerpenguins data set\n\nWrite a custom function called summary_stats that takes 3 inputs\n\ndata_frame\ngrouping_variable\nnumerical_variable\n\n\nand outputs the summarize command on the following sample statistics: minimum, mean, median, standard deviation, and maximum. Please follow the given stencil.\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats <- function(data_frame, grouping_variable, numerical_variable){\n  data_frame |>\n    filter(!is.na({{grouping_variable}})) |>\n    group_by({{grouping_variable}}) |>\n    summarize(min = min({{numerical_variable}}, na.rm = TRUE),\n            xbar = mean({{numerical_variable}}, na.rm = TRUE),\n            med = median({{numerical_variable}}, na.rm = TRUE),\n            s = sd({{numerical_variable}}, na.rm = TRUE),\n            max = max({{numerical_variable}}, na.rm = TRUE))\n}\n# END SOLUTION\n. = \" # BEGIN PROMPT\nsummary_stats <- function(data_frame, grouping_variable, numerical_variable){\n  data_frame |>\n    filter(!is.na({{grouping_variable}})) |>\n    group_by({{grouping_variable}}) |>\n    summarize(min = min({{numerical_variable}}, na.rm = TRUE),\n            xbar = _____,\n            med = _____,\n            s = _____,\n            max = _____)\n}\n\" # END PROMPT\n\n\nUse your summary_stats function with the penguins data frame, grouped by the species categorical variable, on the bill_length_mm numerical variable.\n\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats(penguins, species, bill_length_mm)\n# END SOLUTION\n. = \" # BEGIN PROMPT\nsummary_stats(penguins, _____, _____)\n\" # END PROMPT\n\n\n\nA tibble: 3 × 6\n\n    speciesminxbarmedsmax\n    <fct><dbl><dbl><dbl><dbl><dbl>\n\n\n    Adelie   32.138.7913938.802.66340546.0\n    Chinstrap40.948.8338249.553.33925658.0\n    Gentoo   40.947.5048847.303.08185759.6\n\n\n\n\n\nUse your summary_stats function with the penguins data frame, grouped by the island categorical variable, on the body_mass_g numerical variable.\n\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats(penguins, island, body_mass_g)\n# END SOLUTION\n. = \" # BEGIN PROMPT\n\n\" # END PROMPT\n\n\n\nA tibble: 3 × 6\n\n    islandminxbarmedsmax\n    <fct><int><dbl><dbl><dbl><int>\n\n\n    Biscoe   28504716.0184775.0782.85576300\n    Dream    27003712.9033687.5416.64414800\n    Torgersen29003706.3733700.0445.10794700\n\n\n\n\n\n# If you want to check your code right now, uncomment the following line of code and run it\n# testthat::expect_equal(summary_stats(penguins, island, body_mass_g)$s[1], 782.8557, tol = 0.01)\n\n\n\nSubmission\nOnce you are done with the tasks above,\n\nGo to “File”\nClick “Download as”\nDownload as “Notebook (.ipynb)\n\nThat will download a copy of this notebook onto your computer (probably into your Downloads folder). Please upload the .ipynb file back into our CatCourses site."
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#math-biology-video-project-1",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#math-biology-video-project-1",
    "title": "Math Biology Video Project",
    "section": "Math Biology Video Project",
    "text": "Math Biology Video Project\n\n\n\nEcology course (prereq: Intro Bio)\nFall 2018 semester\n91 students (85 students in study)\n24 videos\n\nabout 10 minutes per video\n\n\n\n\n\n\nimage credit: Shutterstock"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#learner-profile",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#learner-profile",
    "title": "Math Biology Video Project",
    "section": "Learner Profile",
    "text": "Learner Profile\n\n\n\n\n\nmath background\n\n\n\n\nmostly sophomores\nprior experience in flipped classrooms\nmostly White and Asian students\n77 percent female"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#literature",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#literature",
    "title": "Math Biology Video Project",
    "section": "Literature",
    "text": "Literature\n\nValuesValues 2EmotionsEmotions 2\n\n\n\n\n\nAndrews 2017\n\n\n\n\n\n\n\nMath Biology Values Instrument\n\n\n\n\n\n\n\nWachsmuth 2017\n\n\n\n\n\n\n\nMath Emotions Instrument"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#fall-2018-semester",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#fall-2018-semester",
    "title": "Math Biology Video Project",
    "section": "Fall 2018 Semester",
    "text": "Fall 2018 Semester\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur first glance at the survey data showed no significant results between the pre and post surveys."
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#discretize",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#discretize",
    "title": "Math Biology Video Project",
    "section": "Discretize",
    "text": "Discretize\nWe grouped students into “unchanged”, “increase”, or “decrease” groups based on their pre- and post-semester survey results for the MBVI queries—where “unchanged” was a difference of -1, 0, or 1 on the 7-point Likert scales"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#math-content",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#math-content",
    "title": "Math Biology Video Project",
    "section": "Math Content",
    "text": "Math Content\n\n\n\n\nOne VideoAll Videos"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#math-emotion",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#math-emotion",
    "title": "Math Biology Video Project",
    "section": "Math Emotion",
    "text": "Math Emotion"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#mbvi",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#mbvi",
    "title": "Math Biology Video Project",
    "section": "MBVI",
    "text": "MBVI\nMath Biology Values Instrument"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#video-stats",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#video-stats",
    "title": "Math Biology Video Project",
    "section": "Video Stats",
    "text": "Video Stats\n\nPlaysVisitsPersistence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo some students stop watching videos across term?"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#multiple-views",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#multiple-views",
    "title": "Math Biology Video Project",
    "section": "Multiple Views",
    "text": "Multiple Views\n\nQueryRPos CoeffNeg Coeff\n\n\nWhat influences students to view students multiple times?\n\n\n\n\n\nCall:\nglm(formula = multiple_plays ~ intrigue_cat + fun_cat + appeals_cat + \n    interesting_cat + valuable_cat + important_cat + essential_cat + \n    useful_cat + work_harder_cat + worry_cat + intimidate_cat + \n    easy_hard + complicated_simple + confusing_clear + comfortable_uncomfortable + \n    satisfying_frustrating + challenging_not_challenging + pleasant_unpleasant + \n    chaotic_organized + GPA + pct_math + video_length + video_number, \n    family = \"binomial\", data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3229  -0.9405  -0.4840   1.0090   2.2857  \n\nCoefficients: (1 not defined because of singularities)\n                              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                 -2.764e+00  1.859e+00  -1.487 0.137076    \nintrigue_catincrease         1.150e-01  1.302e+00   0.088 0.929576    \nintrigue_catdecrease         1.370e+00  1.173e+00   1.168 0.242842    \nfun_catincrease             -8.922e-01  5.231e-01  -1.705 0.088116 .  \nfun_catdecrease              2.311e+00  7.322e-01   3.156 0.001598 ** \nappeals_catincrease          2.869e-01  6.281e-01   0.457 0.647773    \nappeals_catdecrease         -1.698e+00  6.447e-01  -2.634 0.008434 ** \ninteresting_catincrease      1.707e+00  6.576e-01   2.595 0.009462 ** \ninteresting_catdecrease     -2.212e+00  4.147e-01  -5.334 9.62e-08 ***\nvaluable_catincrease        -1.199e+00  7.577e-01  -1.582 0.113597    \nvaluable_catdecrease        -1.618e+01  5.354e+02  -0.030 0.975895    \nimportant_catincrease       -8.767e-01  3.964e-01  -2.212 0.026967 *  \nimportant_catdecrease       -1.594e+01  5.354e+02  -0.030 0.976256    \nessential_catincrease       -3.513e-01  4.322e-01  -0.813 0.416374    \nessential_catdecrease        1.932e+01  5.354e+02   0.036 0.971210    \nuseful_catincrease           4.869e-01  4.568e-01   1.066 0.286416    \nuseful_catdecrease                  NA         NA      NA       NA    \nwork_harder_catincrease      2.774e-01  2.685e-01   1.033 0.301604    \nwork_harder_catdecrease     -2.725e-01  4.216e-01  -0.647 0.517945    \nworry_catincrease            5.338e-01  3.500e-01   1.525 0.127149    \nworry_catdecrease            9.584e-01  3.564e-01   2.689 0.007159 ** \nintimidate_catincrease      -1.419e+00  4.625e-01  -3.068 0.002155 ** \nintimidate_catdecrease      -1.469e+00  3.275e-01  -4.485 7.28e-06 ***\neasy_hard3                   1.374e+00  7.488e-01   1.834 0.066603 .  \neasy_hard4                   2.940e-01  7.214e-01   0.408 0.683598    \neasy_hard5                   1.593e+00  7.100e-01   2.243 0.024890 *  \neasy_hard6                  -3.706e-01  8.303e-01  -0.446 0.655323    \neasy_hard7                  -1.694e-01  1.037e+00  -0.163 0.870239    \ncomplicated_simple          -6.506e-01  1.773e-01  -3.671 0.000242 ***\nconfusing_clear              2.877e-01  1.620e-01   1.776 0.075785 .  \ncomfortable_uncomfortable   -3.927e-01  1.298e-01  -3.026 0.002476 ** \nsatisfying_frustrating       2.317e-01  1.159e-01   1.999 0.045608 *  \nchallenging_not_challenging -1.322e-01  1.641e-01  -0.805 0.420689    \npleasant_unpleasant          7.729e-01  1.475e-01   5.239 1.62e-07 ***\nchaotic_organized            3.058e-01  9.865e-02   3.100 0.001935 ** \nGPA                          2.159e-01  4.056e-01   0.532 0.594511    \npct_math                    -3.338e-03  3.305e-03  -1.010 0.312373    \nvideo_length                 1.076e-04  1.861e-04   0.578 0.563013    \nvideo_number                -4.326e-02  1.217e-02  -3.555 0.000378 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1198.6  on 867  degrees of freedom\nResidual deviance: 1000.0  on 830  degrees of freedom\n  (1089 observations deleted due to missingness)\nAIC: 1076\n\nNumber of Fisher Scoring iterations: 12\n\n\n\n\nViewing multiple times is encouraged by\n\nIt is fun (decrease) to use math to understand biology\nUsing math to understand biology is interesting (increase)\nI worry (decrease) about getting worse grades in a biology course that incorporates math than one that does not\npleasant versus unpleasant\nchaotic versus organized\n\n\n\nViewing multiple times is discouraged by\n\nUsing math to understand biology appeals (decrease) to me\nUsing math to understand biology is interesting (decrease)\nIt is important (increase) for me to be able to do math for my career in the life sciences\nTaking a biology course that incorporates math intimidates (both) me\ncomplicated versus simple\ncomfortable versus uncomfortable\nsatisfying versus frustrating\ntime in semester (video number)"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#viewing-time",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#viewing-time",
    "title": "Math Biology Video Project",
    "section": "Viewing Time",
    "text": "Viewing Time\n\nQueryRPos CoeffNeg Coeff\n\n\nWhat influences students to spend more time watching the videos?\n\n\n\n\n\nCall:\nglm(formula = as.numeric(avg_view_time_when_played) ~ intrigue_cat + \n    fun_cat + appeals_cat + interesting_cat + valuable_cat + \n    important_cat + essential_cat + useful_cat + work_harder_cat + \n    worry_cat + intimidate_cat + easy_hard + complicated_simple + \n    confusing_clear + comfortable_uncomfortable + satisfying_frustrating + \n    challenging_not_challenging + pleasant_unpleasant + chaotic_organized + \n    GPA + pct_math + video_length + video_number, family = \"gaussian\", \n    data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-853.86  -128.02   -24.41   108.91  1139.41  \n\nCoefficients: (1 not defined because of singularities)\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  180.5561   176.8864   1.021 0.307672    \nintrigue_catincrease         153.1457   116.2658   1.317 0.188134    \nintrigue_catdecrease         328.5588   104.4253   3.146 0.001712 ** \nfun_catincrease                2.3447    48.3834   0.048 0.961360    \nfun_catdecrease              -40.4265    74.1309  -0.545 0.585667    \nappeals_catincrease          -22.9577    56.2257  -0.408 0.683149    \nappeals_catdecrease           28.4891    66.2874   0.430 0.667466    \ninteresting_catincrease       85.8513    60.3164   1.423 0.155011    \ninteresting_catdecrease       -6.5632    41.9026  -0.157 0.875575    \nvaluable_catincrease        -153.6194    72.3949  -2.122 0.034136 *  \nvaluable_catdecrease        -872.1688   246.4276  -3.539 0.000424 ***\nimportant_catincrease        -16.3917    39.5645  -0.414 0.678758    \nimportant_catdecrease       -842.0904   255.4479  -3.297 0.001020 ** \nessential_catincrease         78.0939    41.2858   1.892 0.058900 .  \nessential_catdecrease        826.7246   268.3537   3.081 0.002133 ** \nuseful_catincrease           -28.3670    41.7338  -0.680 0.496876    \nuseful_catdecrease                 NA         NA      NA       NA    \nwork_harder_catincrease      -14.9730    27.3546  -0.547 0.584274    \nwork_harder_catdecrease       36.4671    38.6517   0.943 0.345711    \nworry_catincrease             13.4219    37.2188   0.361 0.718474    \nworry_catdecrease             -2.8790    35.4262  -0.081 0.935249    \nintimidate_catincrease        -2.7321    45.7014  -0.060 0.952344    \nintimidate_catdecrease        33.5371    32.8474   1.021 0.307552    \neasy_hard3                   -34.3803    74.1131  -0.464 0.642848    \neasy_hard4                   -25.4733    71.6178  -0.356 0.722167    \neasy_hard5                   -26.0246    69.5317  -0.374 0.708289    \neasy_hard6                  -100.2542    82.3422  -1.218 0.223748    \neasy_hard7                   221.9490    96.7221   2.295 0.021999 *  \ncomplicated_simple           -21.7275    17.1182  -1.269 0.204705    \nconfusing_clear               31.3572    16.0778   1.950 0.051472 .  \ncomfortable_uncomfortable     28.3085    13.1634   2.151 0.031801 *  \nsatisfying_frustrating       -20.8471    12.0115  -1.736 0.083008 .  \nchallenging_not_challenging  -17.2412    16.4447  -1.048 0.294744    \npleasant_unpleasant           -0.3234    14.5043  -0.022 0.982217    \nchaotic_organized            -19.1039     9.7751  -1.954 0.050996 .  \nGPA                           61.4313    39.0669   1.572 0.116225    \npct_math                       0.1931     0.3323   0.581 0.561199    \nvideo_length                   0.2106     0.0188  11.204  < 2e-16 ***\nvideo_number                   3.1522     1.2089   2.607 0.009288 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 51216.36)\n\n    Null deviance: 59330647  on 867  degrees of freedom\nResidual deviance: 42509579  on 830  degrees of freedom\n  (1089 observations deleted due to missingness)\nAIC: 11915\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\nViewing time is encouraged by\n\nUsing math to understand biology intrigues (decrease) me\ncomfortable versus uncomfortable\ntime in semester (video number)\n\n\n\nViewing time is discouraged by\n\nMath is valuable (both) for me for my life science career\nIt is important (decrease) for me to be able to do math for my career in the life sciences"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#take-home-messages",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#take-home-messages",
    "title": "Math Biology Video Project",
    "section": "Take Home Messages",
    "text": "Take Home Messages\nStudents who viewed videos multiple times and/or for longer duration were\n\nmore pessimistic about math emotions\nworried less about their course grade\nnot affected by the proportion of math content"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#thank-you",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#thank-you",
    "title": "Math Biology Video Project",
    "section": "Thank You",
    "text": "Thank You\n\n\nDerek Sollberger\n\nData Analyst\nUC Merced\ndsollberger@ucmerced.edu\n\n\nDr Emily Weigel\n\nCourse Instructor\nGeorgia Tech\nemily.weigel@biosci.gatech.edu"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html",
    "title": "Math Biology Video Project",
    "section": "",
    "text": "Derek Sollberger\n\nData Analyst\nUC Merced\n\n\nDr Emily Weigel\n\nCourse Instructor\nGeorgia Tech\n\n\n\n\n\n\n\n\nEcology course (prereq: Intro Bio)\nFall 2018 semester\n91 students (85 students in study)\n24 videos\n\nabout 10 minutes per video\n\n\n\n\n\n\nimage credit: Shutterstock\n\n\n\n\n\n\n\n\n\n\n\n\nmath background\n\n\n\n\nmostly sophomores\nprior experience in flipped classrooms\nmostly White and Asian students\n77 percent female\n\n\n\n\n\n\n\nValuesValues 2EmotionsEmotions 2\n\n\n\n\n\nAndrews 2017\n\n\n\n\n\n\n\nMath Biology Values Instrument\n\n\n\n\n\n\n\nWachsmuth 2017\n\n\n\n\n\n\n\nMath Emotions Instrument\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur first glance at the survey data showed no significant results between the pre and post surveys.\n\n\n\nWe grouped students into “unchanged”, “increase”, or “decrease” groups based on their pre- and post-semester survey results for the MBVI queries—where “unchanged” was a difference of -1, 0, or 1 on the 7-point Likert scales\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne VideoAll Videos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath Biology Values Instrument\n\n\n\n\n\n\n\n\n\n\n\n\nPlaysVisitsPersistence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo some students stop watching videos across term?"
  },
  {
    "objectID": "posts/sex_bio_context/sex_in_a_biological_context.html",
    "href": "posts/sex_bio_context/sex_in_a_biological_context.html",
    "title": "Sex in a Biological Context",
    "section": "",
    "text": "Goal\n\nEncourage students to use established research materials to explore sex in a biological context\n\nObjectives\n\nComment on an interview with Dr Joan Roughgarden\nSummarize a passage from a book by Dr Joan Roughgarden\nInterpret a couple pieces of data visualizaton from sex studies\n\nContext\n\ndata science course\nmostly biology majors, mostly sophomores\nhomework assignment\n\n\n\n\n\nTaskWord CloudSample Responses\n\n\n\n\nDr Joan Roughgarden was one of the most important researchers in the field of sexual selection theory, and she has written several works on the matters including the book Evolution’s Rainbow. Dr Roughgarden spoke at a WiSE conference (Women in Science and Engineering) a couple of years ago. For this resource , you may focus on the middle of the video from the “Evolution’s Rainbow” segment (at the 9:25 mark) to the “Career track” segment (at the 15:50 mark).\n\nhttps://www.youtube.com/watch?v=hDbsQu0XhPo\nsource: https://www.wisecology.net/speakers/joan-roughgarden/\n\n\n\n\n\nDr Joan Roughgarden\n\n\n\n\n\n\n\n\n\nresponses to the video interview\n\n\n\n\n\n“There is a lot of diversity in sex and gender in not only humans, but animals as well. With a variety of sex and gender in living beings that is identified by Dr. Roughgarden, it may mean that the sexual selection theory is outdated as we expand our knowledge on this topic. The theory needs to be rewritten for this modern era.”\n\n“This excerpt from Dr. Joan Roughgarden discusses her work on her book Evolution’s Rainbow and her personal experience as a (trans) woman in science. She discusses her work on gender and sexuality across all animal kingdoms, argues that sexual selection should be scrapped, and thinks it will collapse altogether. As a woman in science, she has seen that men’s careers in science are linear and are privileged with a sense of authority.”\n“The career track segment of Dr. Joan Roughgarden’s speech at the Women in Science and Engineering conference focused on her experiences as a transgender woman in the field of biology. She shared the challenges she faced while navigating her gender identity in a male-dominated field and the discrimination and bias she encountered. However, she also emphasized the importance of being true to oneself and finding a supportive community. Roughgarden’s message was one of resilience and perseverance in the face of adversity.”\n\n\n\n\n\n\n\n\nTaskWord CloudSample Responses\n\n\n\n\nSkim through chapter 2 “Sex versus Gender” of Evolution’s Rainbow. What is your impression of the writing? Or, what information did you get from this chapter?\n\n\n\n\nEvolution’s Rainbow\n\n\n\n\n\n\n\n\n\nresponses to the book excerpt\n\n\n\n\n\n“While reading the small passage, from Evolution’s Rainbow Chapter 2, it states that male and female are just biological categories. It starts by saying that people use the words, gender and sex wrong stating that the word’s mean male and female. In which the word’s male and female does not relate with have two different meanings biological criteria and in the social criteria.”\n“Looking through the text and readings we can see that Dr. Roughgarden discusses in terms about”male” and “female” instead of “man” and “woman”. As she done so she also discusses about when it comes to humans male and female don’t coincide 100 percent. She also discusses about the social categories rests in society and not in science.”\n\n“This excerpt approaches males and females from the sex, gender, and social perspective. It states that male and female as a \"Sex\" does not make sense since sex refers to the mixing of genes or reproduction, but some reproduction is asexual reproduction without two partners. Gender’s definition is embracing the biological definition but the other attributes of gender –masculinity and femininity– are not defined biologically. The author is writing up and tearing down these words and stripping them of meaning to reveal their inconsistencies.”\n\n\n\n\n\n\n\n\nTask (3)Word Cloud (3)Sample Responses (3)Task (4)Word Cloud (4)Sample Responses (4)\n\n\n\n\nWhat is your impression of the data visualization below? What information can you gather from the figure(s)?\n\nManey DL, Merritt JR, Prichard MR, Horton BM, Yi SV. Inside the supergene of the bird with four sexes. Horm Behav. 2020 Nov;126:104850. doi: 10.1016/j.yhbeh.2020.104850. Epub 2020 Sep 19. PMID: 32937166; PMCID: PMC7725849.\n\n\n\n\nData Viz 1\n\n\n\n\n\n\n\n\n\nresponses to the first graph\n\n\n\n\n\n“One morph performs a few of the aforementioned actions more frequently than the other. For instance, both sexes of WS birds sing more frequently in response to simulated territorial invasions than TS birds do. TS females seldom ever sing, despite the fact that TS males frequently sing loudly. While TS males are more likely to stay in their own territories, WS males are more prone to participate in territorial invasions. TS birds provide nestlings more frequently than their WS counterparts, and males reproduce this tendency more frequently than females. In general, WS birds appear to devote more time to mate-hunting and intrasexual rivalry, whereas TS birds adopt a more parental life-history approach.”\n\n“that data infomration that has been provided above has allowed me to follow what is being studied fairly easy since they would use the the color of the birds that they have to be distinguished and is being used in the boxplots in which helps us know which one is for which. with that in mind the boxplot allows us to understand the ranges that they have within the species while also allowing us to see how long the songs that each of the birds would sing within the ranges of ten minutes which shows us how different they are. while within the other boxplot then they would show the trips per hour in which helps us see how different they are compared the sexes from both of the species since the females TS have the highest trips per hour compared to the others. another thing they would use is the means, medians, and the quartiles so that they could be compared between the species and their sexes to know the differences.”\n“I can see that the tan-striped bird makes more trips but they have shorter songs sung. Compared to the white bird he made fewer trips but sang for longer times than the tan-striped bird. The used a box graph to show the difference.”\n\n\n\n\n\nWhat is your impression of the data visualization below? What information can you gather from the figure(s)?\n\nManey DL, Merritt JR, Prichard MR, Horton BM, Yi SV. Inside the supergene of the bird with four sexes. Horm Behav. 2020 Nov;126:104850. doi: 10.1016/j.yhbeh.2020.104850. Epub 2020 Sep 19. PMID: 32937166; PMCID: PMC7725849.\n\n\n\n\nData Viz 2\n\n\n\n\n\n\n\n\n\nresponses to the second graph\n\n\n\n\n\n“Prolactin has been associated with provisioning behaviors in both male and female songbirds. Females had a higher normalized VIP expression. Song rate is much higher in males.”\n\n“My impression of the data visualization is that is clear to follow, the colors are coordinated and it has just the necessary information. From the figures it can be gathered that WS have a higher VIP expression compared to that of the TS, also that TS females have a shorter range than all the other birds. Then from figures C and D we can observe whether there is correlation between the song rate and the VIP expression. For males there is no correlation but for females there is a slight positive correlation.”\n“The males of different markings have more differences to each other than the male and female of similar markings when looking at songs produced. Adding hormones into the mix helped condense the songs between birds of similar markings, making the plot a lot less scattered.”\n\n\n\n\n\n\n\n\nTaskSample Responses\n\n\n\n\nIf you wanted to discuss the topic of sex (and maybe gender) in a biological context, what other information would you seek out?\n\n\n\n\nresponses to the last prompt\n\n\n\n\n\n\n\n“If I would want to broaden my understanding on this topic, which is something that I will definitely be looking for, is probably toward podcasts and other articles can help me better understand the difference between sex and gender from there experiences.”\n“What are the biological factors of sex? Does mental health come into play?”\n“I would seek out more information on the genetic compositions of organisms and specifically compare genetic maps of female and male organisms. I would also look at asexual organisms and study how reproduction happens with an organism that is neither female nor male or an organism that is both. We could also study the chemicals released on organisms based on gender like how testosterone is released in male organisms and estrogen is released in female organisms.”"
  },
  {
    "objectID": "posts/graphviz/graphviz.html",
    "href": "posts/graphviz/graphviz.html",
    "title": "graphviz",
    "section": "",
    "text": "For years, I have been searching for the ability to make color-coded flowcharts, and I have found “graphviz online!”. We can use apps such as this one.\nThe code is very intuitive and easy to use. Furthermore, there are many attribute options for nodes and edges.\ndigraph G {\n\n\"Egg\" -> \"Fried\\nEgg\"\n\"Potato\" -> \"Hashbrowns\"\n\"Oil\" -> \"Hashbrowns\"\n\"Egg\" -> \"Pancakes\"\n\"Wheat\\nFlour\" -> \"Pancakes\"\n\n\"Fried\\nEgg\" -> \"Complete\\nBreakfast\"\n\"Hashbrowns\" -> \"Complete\\nBreakfast\"\n\"Milk\" -> \"Complete\\nBreakfast\"\n\"Pancakes\" -> \"Complete\\nBreakfast\"\n\n\n\"Egg\" [shape = circle, style = filled, fillcolor = \"#59C9F1\"]\n\"Potato\" [shape = circle, style = filled, fillcolor = \"#59C9F1\"]\n\"Oil\" [shape = square, style = filled, fillcolor = \"#DDA059\"]\n\"Wheat\\nFlour\" [shape = circle, style = filled, fillcolor = \"#59C9F1\"]\n\n\"Fried\\nEgg\" [shape = square, style = filled, fillcolor = \"#DDA059\"]\n\"Milk\" [shape = circle, style = filled, fillcolor = \"#59C9F1\"]\n\"Hashbrowns\" [shape = square, style = filled, fillcolor = \"#DDA059\"]\n\"Pancakes\" [shape = square, style = filled, fillcolor = \"#DDA059\"]\n\"Complete\\nBreakfast\" [shape = hexagon, style = filled, fillcolor = \" #FFD921\"]\n}\n\n\n\nStardew Valley complete breakfast"
  },
  {
    "objectID": "posts/SpatialDataNotes/04_spherical-geometries.html",
    "href": "posts/SpatialDataNotes/04_spherical-geometries.html",
    "title": "The Median Data Scientist",
    "section": "",
    "text": "Learning objectives:\n\nConsider geometries on a sphere\n\n\n\n\nTriangle on a Globe\n\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"maps\")\nlibrary(\"rnaturalearth\")\nlibrary(\"rnaturalearthdata\")\nlibrary(\"s2\")\nlibrary(\"sf\")\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] sf_1.0-12               s2_1.1.3                rnaturalearthdata_0.1.0\n[4] rnaturalearth_0.3.2     maps_3.4.1              ggplot2_3.4.2          \n[7] dplyr_1.1.2            \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10        pillar_1.9.0       compiler_4.2.2     class_7.3-20      \n [5] tools_4.2.2        digest_0.6.31      lattice_0.20-45    jsonlite_1.8.4    \n [9] evaluate_0.21      lifecycle_1.0.3    tibble_3.2.1       gtable_0.3.3      \n[13] pkgconfig_2.0.3    rlang_1.1.0        cli_3.6.0          DBI_1.1.3         \n[17] rstudioapi_0.14    xfun_0.39          fastmap_1.1.1      e1071_1.7-13      \n[21] withr_2.5.0        httr_1.4.5         knitr_1.42         generics_0.1.3    \n[25] vctrs_0.6.1        htmlwidgets_1.6.2  classInt_0.4-9     grid_4.2.2        \n[29] tidyselect_1.2.0   glue_1.6.2         R6_2.5.1           fansi_1.0.4       \n[33] rmarkdown_2.21     sp_1.6-0           magrittr_2.0.3     units_0.8-2       \n[37] scales_1.2.1       htmltools_0.5.4    colorspace_2.1-0   KernSmooth_2.23-20\n[41] utf8_1.2.3         proxy_0.4-27       wk_0.7.3           munsell_0.5.0     \n\n\n\n\n\n\nHow does the GeoJSON format (Butler et al. 2016) define “straight” lines between ellipsoidal coordinates (Section 3.1.1)? Using this definition of straight, how does LINESTRING(0 85,180 85) look like in an Arctic polar projection? How could this geometry be modified to have it cross the North Pole?\n\n\n\nthis_linestring <- st_linestring(matrix(c(0, 85, 180, 85), ncol = 2),\n                                 dim = \"XY\")\nclass(this_linestring)\n\n[1] \"XY\"         \"LINESTRING\" \"sfg\"       \n\n\n\n\n\n\nthis_linestring |>\n  ggplot() +\n  geom_sf(color = \"red\", linewidth = 3) +\n  labs(title = \"This Linestring\",\n       subtitle = \"Where is it?\",\n       caption = \"Spatial Data Science book club\") +\n  theme_minimal()\n\n\n\n\n\n# https://stackoverflow.com/questions/58919102/map-arctic-subarctic-regions-in-ggplot2-with-lambert-conformal-conic-projection\nworld <- ne_countries(scale = \"medium\", returnclass = \"sf\")\nworld_cropped <- world |>\n  st_make_valid() |>\n  st_crop(xmin = -180.0, xmax = 180.0, ymin = 45.0, ymax = 90.0)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# st_sfc(this_linestring) <- st_crs(world_cropped)\n\n# this_linestring_sf <- st_sf(this_linestring, \n#                             st_sfc(this_linestring, crs = \"EPSG:3995\"))\n\nggplot(data = world_cropped) + \n  geom_sf() + \n  # geom_sf(data = this_linestring, color = 'red') +\n  coord_sf(crs = \n             \"+proj=lcc +lat_1=50 +lat_2=70 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0\")\n\n\n\n\n\n\n\nFor a typical polygon on \\(S^2\\), how can you find out ring direction?\n\nA convention here is to define the inside as the left (or right) side of the polygon boundary when traversing its points in sequence. Reversal of the node order then switches inside and outside.\nAdditional resource: ESRI: Polygon page\n\n\n\n\n\nAntarctica_map <- map(fill = TRUE, plot = FALSE) |>\n  st_as_sf() |>\n  filter(ID == \"Antarctica\")\n\nAntarctica_map |> \n  ggplot() + \n  geom_sf() +\n  labs(title = \"Antarctica\", \n       subtitle = \"Think of the latitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nst_bbox(Antarctica_map)\n\n      xmin       ymin       xmax       ymax \n-180.00000  -85.19218  179.62032  -60.52090 \n\n\nwhich clearly does not contain the region (ymin being -90 and xmax 180).\n\nFiji_map <- map(fill = TRUE, plot = FALSE) |>\n  st_as_sf() |>\n  filter(ID == \"Fiji\")\n\nFiji_map |> \n  ggplot() + \n  geom_sf() +\n  labs(title = \"Fiji\", \n       subtitle = \"Think of the longitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nst_bbox(Fiji_map)\n\n      xmin       ymin       xmax       ymax \n-179.86734  -21.70586  180.17769  -12.47695 \n\n\nseems to span most of the Earth\n\n\n\ns2_bounds_cap(Antarctica_map)\n\n  lng lat   angle\n1   0 -90 29.4791\n\n\n\ns2_bounds_rect(Antarctica_map)\n\n  lng_lo lat_lo lng_hi   lat_hi\n1   -180    -90    180 -60.5209\n\n\n\ns2_bounds_rect(Fiji_map)\n\n    lng_lo    lat_lo    lng_hi    lat_hi\n1 174.5872 -21.70586 -178.2511 -12.47695\n\n\n\n\n\n\nMaps of Antarctica should probably display the South Pole. Do the following maps display the South Pole?\n\n\n\n# maps package\nm <- st_as_sf(map(fill=TRUE, plot=FALSE))\nAntarctica_map_A <- m[m$ID == \"Antarctica\", ]\nst_geometry(Antarctica_map_A) |>\n  ggplot() + \n  geom_sf() +\n  labs(title = \"Antarctica\", \n       subtitle = \"Think of the latitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nsf::st_is_valid(Antarctica_map_A)\n\n[1] TRUE\n\n\n\n# Natural Earth package\nne <- ne_countries(returnclass = \"sf\")\nAntarctica_map_B <- ne[ne$region_un == \"Antarctica\", \"region_un\"]\nst_geometry(Antarctica_map_B) |>\n  ggplot() + \n  geom_sf() +\n  labs(title = \"Antarctica\", \n       subtitle = \"Think of the latitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nsf::st_is_valid(Antarctica_map_B)\n\n[1] TRUE\n\n\n\n\n\n\nAntarctica_map_C <- st_geometry(Antarctica_map_A) |>\n  st_transform(3031)\nAntarctica_map_C |> \n  ggplot() + \n  geom_sf() +\n  labs(title = \"Antarctica\", \n       subtitle = \"Think of the latitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nsf::st_is_valid(Antarctica_map_C)\n\n[1] TRUE\n\n\n\nAntarctica_map_D <- st_geometry(Antarctica_map_B) |>\n  st_transform(3031)\nAntarctica_map_D |> \n  ggplot() + \n  geom_sf() +\n  labs(title = \"Antarctica\", \n       subtitle = \"Think of the latitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nsf::st_is_valid(Antarctica_map_D)\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\n\n\nMeeting chat log\n\nLOG"
  },
  {
    "objectID": "posts/TidyModels_trees/Tidymodels_Trees.html",
    "href": "posts/TidyModels_trees/Tidymodels_Trees.html",
    "title": "TidyModels Trees",
    "section": "",
    "text": "Years ago, I would use the caret package to perform a random forest search and plot an example of a decision tree. Can we do that now in the TidyModels module?\nHere I am adapting code from Stack Overflow\n\nlibrary(\"palmerpenguins\")\nlibrary(\"rpart\")\nlibrary(\"rpart.plot\")\nlibrary(\"tidymodels\")\n\n\ndf <- penguins |>\n  mutate(species = factor(species))\n\ndata_split <- initial_split(df)\ndf_train <- training(data_split)\ndf_test <- testing(data_split)\n\n\ndf_recipe <- recipe(species ~ ., data = df) %>%\n  step_normalize(all_numeric())\n\n\n#building model\ntree <- decision_tree() %>%\n   set_engine(\"rpart\") %>%\n   set_mode(\"classification\")\n\n\n#workflow\ntree_wf <- workflow() %>%\n  add_recipe(df_recipe) %>%\n  add_model(tree) %>%\n  fit(df_train) #results are found here \n\n\ntree_fit <- tree_wf |>\n  extract_fit_parsnip()\nrpart.plot(tree_fit$fit, roundint = FALSE)\n\n\n\n\nSo far,\n\nresponse variable has to be a factor type—and hence should be categorical—in a classification setting\nI need to learn what all of those numbers mean!\n\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] yardstick_1.2.0      workflowsets_1.0.1   workflows_1.1.3     \n [4] tune_1.1.1           tidyr_1.3.0          tibble_3.2.1        \n [7] rsample_1.1.1        recipes_1.0.6        purrr_1.0.1         \n[10] parsnip_1.1.0        modeldata_1.1.0      infer_1.0.4         \n[13] ggplot2_3.4.2        dplyr_1.1.2          dials_1.2.0         \n[16] scales_1.2.1         broom_1.0.4          tidymodels_1.1.0    \n[19] rpart.plot_3.1.1     rpart_4.1.19         palmerpenguins_0.1.1\n\nloaded via a namespace (and not attached):\n [1] jsonlite_1.8.4      splines_4.2.2       foreach_1.5.2      \n [4] prodlim_2023.03.31  GPfit_1.0-8         yaml_2.3.7         \n [7] globals_0.16.2      ipred_0.9-14        pillar_1.9.0       \n[10] backports_1.4.1     lattice_0.20-45     glue_1.6.2         \n[13] digest_0.6.31       hardhat_1.3.0       colorspace_2.1-0   \n[16] htmltools_0.5.4     Matrix_1.5-3        timeDate_4022.108  \n[19] pkgconfig_2.0.3     lhs_1.1.6           DiceDesign_1.9     \n[22] listenv_0.9.0       gower_1.0.1         lava_1.7.2.1       \n[25] timechange_0.2.0    generics_0.1.3      ellipsis_0.3.2     \n[28] withr_2.5.0         furrr_0.3.1         nnet_7.3-18        \n[31] cli_3.6.1           survival_3.4-0      magrittr_2.0.3     \n[34] evaluate_0.21       future_1.32.0       fansi_1.0.4        \n[37] parallelly_1.35.0   MASS_7.3-58.1       class_7.3-20       \n[40] tools_4.2.2         data.table_1.14.8   lifecycle_1.0.3    \n[43] munsell_0.5.0       compiler_4.2.2      rlang_1.1.0        \n[46] grid_4.2.2          iterators_1.0.14    rstudioapi_0.14    \n[49] htmlwidgets_1.6.2   rmarkdown_2.21      gtable_0.3.3       \n[52] codetools_0.2-18    R6_2.5.1            lubridate_1.9.2    \n[55] knitr_1.42          fastmap_1.1.1       future.apply_1.10.0\n[58] utf8_1.2.3          parallel_4.2.2      Rcpp_1.0.10        \n[61] vctrs_0.6.1         tidyselect_1.2.0    xfun_0.39"
  },
  {
    "objectID": "posts/ISLR/ch9.html",
    "href": "posts/ISLR/ch9.html",
    "title": "SVMs",
    "section": "",
    "text": "Learning objectives:\n\nImplement a binary classification model using a maximal margin classifier.\nImplement a binary classification model using a support vector classifier.\nImplement a binary classification model using a support vector machine (SVM).\nGeneralize SVM models to multi-class cases.\n\nSupport vector machine (SVM), an approach for classification developed in 1990. SVM is a generalizaion of classifiers methods, in particular:\n\nmaximal margin classifier (it requires that the classes be separable by a linear boundary).\nsupport vector classifier\nsupport vector machine: binary classification setting with two classes\n\n\n\n\n\nlibrary(\"caTools\")\nlibrary(\"dplyr\")\nlibrary(\"e1071\")\nlibrary(\"ggplot2\")\nlibrary(\"ISLR\")\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ISLR_1.4       ggplot2_3.4.2  e1071_1.7-13   dplyr_1.1.2    caTools_1.18.2\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       cli_3.6.1         knitr_1.43        rlang_1.1.1      \n [5] xfun_0.39         generics_0.1.3    jsonlite_1.8.4    glue_1.6.2       \n [9] colorspace_2.1-0  htmltools_0.5.5   scales_1.2.1      fansi_1.0.4      \n[13] rmarkdown_2.22    grid_4.3.0        munsell_0.5.0     evaluate_0.21    \n[17] tibble_3.2.1      bitops_1.0-7      fastmap_1.1.1     yaml_2.3.7       \n[21] lifecycle_1.0.3   compiler_4.3.0    htmlwidgets_1.6.2 pkgconfig_2.0.3  \n[25] rstudioapi_0.14   digest_0.6.31     R6_2.5.1          class_7.3-21     \n[29] tidyselect_1.2.0  utf8_1.2.3        pillar_1.9.0      magrittr_2.0.3   \n[33] withr_2.5.0       gtable_0.3.3      proxy_0.4-27      tools_4.3.0      \n\n\n\n\n\n\n\nimage credit: Deep AI\n\n\n\nA hyperplane is a \\(p-1\\)-dimensional flat subspace of a \\(p\\)-dimensional space. For example, in a 2-dimensional space, a hyperplane is a flat one-dimensional space: a line.\n(standard form) Definition of 2D hyperplane in 3D space: \\[\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3}= 0\\]\n(inner products) Any \\(X\\) s.t. \\(X = (X_{1}, X_{2})^T\\) for which the equation above is satisfied is a point on the hyperplane.\n\nAdditional resource: Deep AI\n\n\n\n\nConsider a matrix X of dimensions \\(n*p\\), and a \\(y_{i} \\in \\{-1, 1\\}\\). We have a new observation, \\(x^*\\), which is a vector \\(x^* = (x^*_{1}...x^*_{p})^T\\) which we wish to classify to one of two groups.\nWe will use a separating hyperplane to classify the observation.\n\n\n\nWe can label the blue observations as \\(y_{i} = 1\\) and the pink observations as \\(y_{i} = -1\\).\nThus, a separating hyperplane has the property s.t. \\(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip} > 0\\) if \\(y_{i} =1\\) and \\(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip} < 0\\) if \\(y_{i} = -1\\).\nIn other words, a separating hyperplane has the property s.t. \\(y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) > 0\\) for all \\(i = 1...n\\).\nConsider also the magnitude of \\(f(x^*)\\). If it is far from zero, we are confident in its classification, whereas if it is close to 0, then \\(x^*\\) is located near the hyperplane, and we are less confident about its classification.\n\n\n\n\n\n\nGenerally, if data can be perfectly separated using a hyperplane, an infinite amount of such hyperplanes exist.\nAn intuitive choice is the maximal margin hyperplane, which is the hyperplane that is farthest from the training data.\nWe compute the perpendicular distance from each training observation to the hyperplane. The smallest of these distances is known as the margin.\nThe maximal margin hyperplane is the hyperplane for which the margin is maximized. We can classify a test observation based on which side of the maximal margin hyperplane it lies on, and this is known as the maximal margin classifier.\nThe maximal margin classifier classifies \\(x^*\\) based on the sign of \\(f(x^*) = \\beta_{0} + \\beta_{1}x^*_{1} + ... + \\beta_{p}x^*_{p}\\).\n\n\n\nNote the 3 training observations that lie on the margin and are equidistant from the hyperplane. These are the support vectors (vectors in \\(p\\)-dimensional space; in this case \\(p=2\\)).\nThey support the hyperplane because if their location was changed, the hyperplane would change.\nThe maximal margin hyperplane depends on these observations, but not the others (unless the other observations were moved at or within the margin).\n\n\n\n\n\nConsider constructing an MMC based on the training observations \\(x_{1}...x_{n} \\in \\mathbb{R}^p\\). This is the solution to the optimization problem:\n\n\\[\\text{max}_{\\beta_{0}...\\beta_{p}, M} \\space M\\] \\[\\text{subject to } \\sum_{j=1}^{p}\\beta_{j}^2 = 1\\] \\[y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) \\geq M \\quad \\forall i = 1...n\\]\n\n\\(M\\) is the margin, and the \\(\\beta\\) coeffients are chosen to maximize \\(M\\).\nThe constraint (3rd equation) ensures that each observation will be correctly classified, as long as M is positive.\n\n\n\nThe 2nd and 3rd equations ensure that each data point is on the correct side of the hyperplane and at least M-distance away from the hyperplane.\nThe perpendicular distance to the hyperplane is given by \\(y_{i}(\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} ... + \\beta_{p}x_{ip})\\).\n\n\nBut what if our data is not separable by a linear hyperplane?\n\n\n\nIndividual data points greatly affect formation of the maximal margin classifier\n\n\n\n\n\n\nWe can’t always use a hyperplane to separate two classes.\nEven if such a classifier does exist, it’s not always desirable, due to overfitting or too much sensitivity to individual observations.\nThus, it might be worthwhile to consider a classifier/hyperplane that misclassifies a few observations in order to improve classification of the remaining data points.\nThe support vector classifier, a.k.a the soft margin classifier, allows some training data to be on the wrong side of the margin or even the hyperplane.\n\n\n\n\n\nThe SVC classifies a test observation based on which side of the hyperplane it lies.\n\n\\[\\text{max}_{\\beta_{0}...\\beta_{p}, \\epsilon_{1}...\\epsilon_{n}, M} \\space M\\] \\[\\text{subject to } \\sum_{j=1}^{p}\\beta_{j}^2 = 1\\] \\[y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) \\geq M(1 - \\epsilon_{i})\\] \\[\\epsilon_{i} \\geq 0, \\quad \\sum_{i=1}^{n}\\epsilon_{i} \\leq C\\]\n\n\\(C\\) is a nonnegative tuning parameter, typically chosen through cross-validation, and can be thought of as the budget for margin violation by the observations.\nThe \\(\\epsilon_{i}\\) are slack variables that allow individual observations to be on the wrong side of the margin or hyperplane. The \\(\\epsilon_{i}\\) indicates where the \\(i^{\\text{th}}\\) observation is located with regards to the margin and hyperplane.\n\nIf \\(\\epsilon_{i} = 0\\), the observation is on the correct side of the margin.\nIf \\(\\epsilon_{i} > 0\\), the observation is on the wrong side of margin\nIf \\(\\epsilon_{i} > 1\\), the observation is on the wrong side of the hyperplane.\n\nSince \\(C\\) constrains the sum of the \\(\\epsilon_{i}\\), it determines the number and magnitude of violations to the margin. If \\(C=0\\), there is no margin for violation, thus all the \\(\\epsilon_{1},...,\\epsilon_{n} = 0\\).\nNote that if \\(C>0\\), no more than \\(C\\) observations can be on wrong side of hyperplane, since in these cases \\(\\epsilon_{i} > 1\\).\n\n\n\n\n\n\nA property of the classifier is that only data points which lie on or violate the margin will affect the hyperplane. These data points are known as support vectors.\n\\(C\\) controls the bias-variance tradeoff of the classifier.\n\nWhen \\(C\\) is large: high bias, low variance\nWhen \\(C\\) is small: low bias, high variance\n\nThe property of the SVC solely being dependent on certain observations in classification differs from other classification methods such as LDA (depends on mean of all observations in each class, as well as each class’s covariance matrix using all observations).\nHowever, logistic regression is more similar to SVC in that it has low sensitivity to observations far from the decision boundary.\n\n\n\n\n\nMany decision boundaries are not linear.\nWe could fit an SVC to the data using \\(2p\\) features (in the case of \\(p\\) features and using a quadratic form).\n\n\\[X_{1}, X_{1}^{2}, \\quad X_{2}, X_{2}^{2}, \\quad\\cdots, \\quad X_{p}, X_{p}^{2}\\]\n\\[\\text{max}_{\\beta_{0},\\beta_{11},\\beta_{12},\\dots,\\beta_{p1},\\beta_{p2} \\epsilon_{1},\\dots,\\epsilon_{n}, M} \\space M\\] \\[\\text{subject to }  y_{i}\\left(\\beta_{0} + \\sum_{j=1}^{p} \\beta_{ji}x_{ji} + \\sum_{j=1}^{p} \\beta_{ji}x_{ji}^{2}\\right) \\geq M(1 - \\epsilon_{i})\\]\n\\[\\epsilon_{i} \\geq 0, \\quad \\sum_{i=1}^{n}\\epsilon_{i} \\leq C, \\quad \\sum_{j=1}^{p}\\sum_{k=1}^{2} \\beta_{jk}^{2} = 1\\]\n\nNote that in the enlarged feature space (here, with the quadratic terms), the decision boundary is linear. But in the original feature space, it is quadratic \\(q(x) = 0\\) (in this example), and generally the solutions are not linear.\nOne could also include interaction terms, higher degree polynomials, etc., and thus the feature space could enlarge quickly and entail unmanageable computations.\n\n\n\n\n\nThe SVM is an extension of the SVC which results from using kernels to enlarge the feature space. A kernel is a function that quantifies the similarity of two data points.\nEssentially, we want to enlarge the feature space to make use of a nonlinear decision boundary, while avoiding getting bogged down in unmanageable calculations.\nThe solution to the SVC problem in the SVM context involves only the inner products (AKA dot products) of the observations.\n\n\\[\\langle x_{i}  \\; , x_{i'} \\; \\rangle = \\sum_{j=1}^{p}x_{ij}x_{i'j}\\]\nIn the context of SVM, the linear support vector classifier is as follows:\n\\[f(x) = \\beta_{0} + \\sum_{i=1}^{n}\\alpha_{i}\\langle \\; x, x_i\\; \\rangle\\]\n\nTo estimate the \\(n\\) \\(\\alpha_{i}\\) coefficients and \\(\\beta_{0}\\), we only need the \\(\\binom{n}{2}\\) inner products between all pairs of training observations.\nNote that in the equation above, in order to compute \\(f(x)\\) for the new point \\(x\\), we need the inner product between the new point and all the training observations. However, \\(\\alpha_{i} = 0\\) for all points that are not on or within the margin (i.e., points that are not support vectors). So we can rewrite the equation as follows, where \\(S\\) is the set of support point indices:\n\n\\[f(x) = \\beta_{0} + \\sum_{i \\in S}\\alpha_{i}\\langle \\; x, x_{i} \\; \\rangle\\]\n\nReplace every inner product with \\(K(x_{i}, x_{i'})\\), where \\(K\\) is a kernel function.\n\\(K(x_{i}, x_{i'}) = \\sum_{j=1}^{p}x_{ij}x_{i'j}\\) is the SVC and is known as a linear kernel since it is linear in the features.\nOne could also have kernel functions of the following form, where \\(d\\) is a positive integer:\n\n\\[K(x_{i}, x_{i'}) = \\left(1 + \\sum_{j=1}^{p}x_{ij}x_{i'j}\\right)^d\\]\n\nThis will lead to a much more flexible decision boundary, and is basically fitting an SVC in a higher-dimensional space involving polynomials of degree \\(d\\), instead of the original feature space.\nWhen an SVC is combined with a nonlinear kernel as above, the result is a support vector machine.\n\n\\[f(x) =  \\beta_{0} + \\sum_{i \\in S}\\alpha_{i}K(x, x_{i})\\]\n\n\n\n\n\n\nimage credit: Manin Bocss\n\n\n\nThere are other options besides polynomial kernel functions, and a popular one is a radial kernel.\n\n\\[K(x, x_{i}) = \\text{exp}\\left(-\\gamma\\sum_{j=1}^p(x_{ij} - x_{i'j})^2\\right), \\quad \\gamma > 0\\]\n\nFor a given test observations \\(x^*\\), if it is far from \\(x_{i}\\), then \\(K(x^*, x_{i})\\) will be small given the negative \\(\\gamma\\) and large \\(\\sum_{j=1}^p(x^*_{j} - x_{ij})^2)\\).\nThus, \\(x_{i}\\) will play little role in \\(f(x^*)\\).\nThe predicted class for \\(x^*\\) is based on the sign of \\(f(x^*)\\), so training observations far from a given test point play little part in determining the label for a test observation.\nThe radial kernel therefore exhibits local behavior with respect to other observations.\n\n\n\n\n\n\n\nimage credit: Manin Bocss\n\n\n\nThe advantage of using a kernel rather than simply enlarging feature space is computational, since it is only necessary to compute \\(\\binom{n}{2}\\) kernel functions.\nFor radial kernels, the feature space is implicit and infinite dimensional, so we could not do the computations in such a space anyways.\n\n\n\n\n\nThe concept of separating hyperplanes does not extend naturally to more than two classes, but there are some ways around this.\nA one-versus-one approach constructs \\(K \\choose 2\\) SVMs, where \\(K\\) is the number of classes. An observation is classified to each of the \\(K \\choose 2\\) classes, and the number of times it appears in each class is counted.\nThe \\(k^\\text{th}\\) class might be coded as +1 versus the \\((k')^\\text{th}\\) class is coded as -1.\nThe data point is classified to the class for which it was most often assigned in the pairwise classifications.\nAnother option is one-versus-all classification. This can be useful when there are a lot of classes.\n\\(K\\) SVMs are fitted, and one of the K classes to the remaining \\(K-1\\) classes.\n\\(\\beta_{0k}...\\beta_{pk}\\) denotes the parameters that results from constructing an SVM comparing the \\(k\\)th class (coded as +1) to the other classes (-1).\nAssign test observation \\(x^*\\) to the class \\(k\\) for which \\(\\beta_{0k} + ... + \\beta_{pk}x^*_{p}\\) is largest.\n\n\n\n\n\n\nlibrary(\"tidymodels\")\nlibrary(\"kernlab\") # We'll use the plot method from this.\n\n\nset.seed(1)\nsim_data <- matrix(\n  rnorm (20 * 2), \n  ncol = 2,\n  dimnames = list(NULL, c(\"x1\", \"x2\"))\n) %>% \n  as_tibble() %>% \n  mutate(\n    y = factor(c(rep(-1, 10), rep(1, 10)))\n  ) %>%\n  mutate(\n    x1 = ifelse(y == 1, x1 + 1, x1),\n    x2 = ifelse(y == 1, x2 + 1, x2)\n  )\n\nsim_data %>% \n  ggplot() +\n  aes(x1, x2, color = y) +\n  geom_point() +\n  labs(title = \"Trying to make a hyperplane classifier\",\n       subtitle = \"simulated data\",\n       caption = \"R4DS book club\") +\n  theme_minimal()\n\n\n# generated this using their process then saved it to use here.\ntest_data <- readRDS(\"data/09-testdat.rds\") %>% \n  rename(x1 = x.1, x2 = x.2)\n\ntest_data %>% \n  ggplot() +\n  aes(x1, x2, color = y) +\n  geom_point() +\n  labs(title = \"Trying to make a hyperplane classifier\",\n       subtitle = \"simulated data\",\n       caption = \"R4DS book club\") +\n  theme_minimal()\n\nWe create a spec for a model, which we’ll update throughout this lab with different costs.\n\nsvm_linear_spec <- svm_poly(degree = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\", scaled = FALSE)\n\nThen we do a couple fits with manual cost.\n\nsvm_linear_fit_10 <- svm_linear_spec %>% \n  set_args(cost = 10) %>%\n  fit(y ~ ., data = sim_data)\nsvm_linear_fit_10\n\nsvm_linear_fit_10 %>%\n  extract_fit_engine() %>%\n  plot()\n\nsvm_linear_fit_01 <- svm_linear_spec %>% \n  set_args(cost = 0.1) %>%\n  fit(y ~ ., data = sim_data)\nsvm_linear_fit_01\nsvm_linear_fit_01 %>%\n  extract_fit_engine() %>%\n  plot()\n\nsvm_linear_fit_001 <- svm_linear_spec %>% \n  set_args(cost = 0.01) %>%\n  fit(y ~ ., data = sim_data)\nsvm_linear_fit_001\nsvm_linear_fit_001 %>%\n  extract_fit_engine() %>%\n  plot()\n\n\n\nLet’s find the best cost.\n\nsvm_linear_wf <- workflow() %>%\n  add_model(\n    svm_linear_spec %>% set_args(cost = tune())\n  ) %>%\n  add_formula(y ~ .)\n\nset.seed(1234)\nsim_data_fold <- vfold_cv(sim_data, strata = y)\n\nparam_grid <- grid_regular(cost(), levels = 10)\n\n# Our grid isn't identical to the book, but it's close enough.\nparam_grid\n\n\ntune_res <- tune_grid(\n  svm_linear_wf, \n  resamples = sim_data_fold, \n  grid = param_grid\n)\n\n# We ran this locally and then saved it so everyone doesn't need to wait for\n# this to process each time they build the book.\n\n# saveRDS(tune_res, \"data/09-tune_res.rds\")\n\n\nautoplot(tune_res)\n\nTune can pull out the best result for us.\n\nbest_cost <- select_best(tune_res, metric = \"accuracy\")\n\nsvm_linear_final <- finalize_workflow(svm_linear_wf, best_cost)\n\nsvm_linear_fit <- svm_linear_final %>% fit(sim_data)\n\nsvm_linear_fit %>% \n  augment(new_data = test_data) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n\\[\\text{accuracy} = \\frac{9 + 8}{9 + 1 + 2 + 8} = 0.85\\]\n\nsvm_linear_fit_001 %>% \n  augment(new_data = test_data) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n\\[\\text{accuracy} = \\frac{11 + 3}{11 + 6 + 0 + 3} = 0.70\\]\n\n\n\n\nsim_data_sep <- sim_data %>% \n  mutate(\n    x1 = ifelse(y == 1, x1 + 0.5, x1),\n    x2 = ifelse(y == 1, x2 + 0.5, x2)\n  )\n\nsim_data_sep %>% \n  ggplot() +\n  aes(x1, x2, color = y) +\n  geom_point()\n\nsvm_fit_sep_1e5 <- svm_linear_spec %>% \n  set_args(cost = 1e5) %>%\n  fit(y ~ ., data = sim_data_sep)\n\nsvm_fit_sep_1e5\nsvm_fit_sep_1e5 %>%\n  extract_fit_engine() %>%\n  plot()\n\nsvm_fit_sep_1 <- svm_linear_spec %>% \n  set_args(cost = 1) %>%\n  fit(y ~ ., data = sim_data_sep)\n\nsvm_fit_sep_1\nsvm_fit_sep_1 %>%\n  extract_fit_engine() %>%\n  plot()\n\ntest_data_sep <- test_data %>% \n  mutate(\n    x1 = ifelse(y == 1, x1 + 0.5, x1),\n    x2 = ifelse(y == 1, x2 + 0.5, x2)\n  )\n\nsvm_fit_sep_1e5 %>% \n  augment(new_data = test_data_sep) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n\\[\\text{accuracy} = \\frac{9 + 8}{8 + 1 + 2 + 8} = 0.85\\]\n\nsvm_fit_sep_1 %>% \n  augment(new_data = test_data_sep) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n\\[\\text{accuracy} = \\frac{9 + 9}{9 + 0 + 2 + 9} = 0.90\\]\n\n\n\n\n\n\n\nggplot setup\n\n\nN <- 50 #resolution\nx <- seq(-10, 10, length.out = N)\ny <- seq(-10, 10, length.out = N)\n\ndf <- expand.grid(x,y)\ncolnames(df) <- c(\"xval\", \"yval\")\n\neuclidean_distance <- function(x1, y1, x2, y2){\n  # computes the Euclidean distance between (x1, y1) and (x2, y2)\n  sqrt( (x2 - x1)^2 + (y2 - y1)^2 )\n}\n\naccuracy_calculation <- function(confusion_matrix){\n  # computes the accuracy revealed by a 2x2 confusion matrix\n  Q <- confusion_matrix\n  (Q[1,1] + Q[2,2]) / (Q[1,1] + Q[1,2] + Q[2,1] + Q[2,2])\n}\n\n\n\n\nThis problem involves hyperplanes in two dimensions.\n\n\n\n\nblue: \\(1 + 3X_{1} - X_{2} > 0\\)\nred: \\(1 + 3X_{1} - X_{2} < 0\\)\n\n\n\ncode\n\n\ndf1 <- df |>\n  # math function\n  mutate(shade = ifelse(yval > 3*xval + 1, \"blue\", \"red\"))\n\ndf1 |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # main line\n  geom_segment(aes(x = -(11/3), y = -10, xend = 3, yend = 10),\n               color = \"black\", linewidth = 3) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = 3x + 1\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nblue: \\(-2 + X_{1} + 2X_{2} > 0\\)\nred: \\(-2 + X_{1} + 2X_{2} < 0\\)\n\n\n\ncode\n\n\ndf1b <- df |>\n  # math function\n  mutate(shade = ifelse(yval > 1 - 0.5*xval, \"blue\", \"red\"))\n\ndf1b |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # main line\n  geom_segment(aes(x = -10, y = 6, xend = 10, yend = -4),\n               color = \"black\", linewidth = 3) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = 1 - 0.5x\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWe now investigate a non-linear decision boundary.\n\nblue: \\((1 + X_{1})^{2} + (2 - X_{2})^{2} > 4\\)\nred: \\((1 + X_{1})^{2} + (2 - X_{2})^{2} < 4\\)\n\n\n\ncode\n\n\ndf2 <- df |>\n  # math function\n  mutate(shade = ifelse(euclidean_distance(xval, yval, -1, 2) > 4, \n                        \"blue\", \"red\"))\n\ndf2 |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"(x+1)^2 + (y-2)^2 = 4\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo what class is the observation (0, 0) classified? (−1, 1)? (2, 2)? (3, 8)?\n\n\nifelse(euclidean_distance(0, 0, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(-1, 1, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(2, 2, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(3, 8, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"blue\"\n\n\n\nWhile the decision boundary\n\n\\[(1 + X_{1})^{2} + (2 - X_{2})^{2} = 4\\]\nis not linear in \\(X_{1}\\) and \\(X_{2}\\), it is linear in terms of \\(X_{1}\\), \\(X_{1}^{2}\\), \\(X_{2}\\), \\(X_{2}^{2}\\)\n\\[\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}^{2} + \\beta_{4}X_{2}^{2} = 0\\]\nwith \\(\\beta_{0} = 1\\), \\(\\beta_{1} = 2\\), \\(\\beta_{2} = -4\\), \\(\\beta_{3} = 1\\), and \\(\\beta_{4} = 1\\).\n\n\n\n\nobs <- 1:7\nxvals <- c(3,2,4,1,2,4,4)\nyvals <- c(4,2,4,4,1,3,1)\nclass_label <- c(\"Red\", \"Red\", \"Red\", \"Red\", \"Blue\", \"Blue\", \"Blue\")\ndf3 <- data.frame(obs, xvals, yvals, class_label)\ndf3\n\n  obs xvals yvals class_label\n1   1     3     4         Red\n2   2     2     2         Red\n3   3     4     4         Red\n4   4     1     4         Red\n5   5     2     1        Blue\n6   6     4     3        Blue\n7   7     4     1        Blue\n\n\n\nWe are given \\(n = 7\\) observations in \\(p = 2\\) dimensions. For each observation, there is an associated class label.\n\n\n\ncode\n\n\ndf3 |>\n  ggplot() +\n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n  coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Where to Draw the Separating Hyperplane?\",\n       # subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\nSketch the optimal separating hyperplane, and provide the equation for this hyperplane\n\n\n\ncode\n\n\ndf3 |>\n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  # margins\n  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n    coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\n\n\n\nblue: \\(0.5 - X_{1} + X_{2} < 0\\)\nred: \\(0.5 - X_{1} + X_{2} > 0\\)\n\n\nmaximal margin in indicated by the dashed lines, with margin\n\n\\[M = \\frac{0.5}{\\sqrt{2}} \\approx 0.3536\\] (e) Indicate the support vectors for the maximal margin classifier.\n\n\ncode\n\n\ndf3e <- df3 |>\n  mutate(supp_vec = ifelse(obs %in% c(2,3,5,6), \n                           \"support vector\", \"other data\"))\n  \ndf3e$supp_vec <- factor(df3e$supp_vec,\n                        levels = c(\"support vector\", \"other data\"))\n  \ndf3e |>  \n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  # margins\n  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = supp_vec),\n             size = 5) +\n    coord_equal() +\n  scale_color_manual(values = c(\"purple\", \"gray50\")) +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\nArgue that a slight movement of the seventh observation would not affect the maximal margin hyperplane.\nSketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane.\n\n\n\ncode\n\n\ndf3 |>\n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0, y = 1/4, xend = 5, yend = 16/4),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n    coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Separating Hyperplane (not optimal)\",\n       subtitle = \"y = 0.25(3x+1)\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\nDraw an additional observation on the plot so that the two classes are no longer separable by a hyperplane.\n\n\n\ncode\n\n\nnew_dot <- data.frame(obs = 8, xvals = 0, yvals = 5, class_label = \"blue\")\ndf3h <- rbind(df3, new_dot)\ndf3h |>\n  ggplot() +\n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n  coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Where to Draw the Separating Hyperplane?\",\n       subtitle = \"new data at (0,5)\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\n\n\n\nMostly transcibed from OnMee’s solutions.\n\n\nGenerate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data. Which technique performs best on the test data? Make plots and report training and test error rates in order to back up your assertions.\n\n# Generating a dataset with visible non-linear separation.\nset.seed(3)\nx=matrix(rnorm(100*2), ncol=2)\ny=c(rep(-1,70), rep(1,30))\nx[1:30,]=x[1:30,]+3.3\nx[31:70,]=x[31:70,]-3\ndat=data.frame(x=x, y=as.factor(y))\n# Training and test sets.\nsample.data = sample.split(dat$x.1, SplitRatio = 0.7)\ntrain.set = subset(dat, sample.data==T)\ntest.set = subset(dat, sample.data==F)\nplot(x,col=(2-y), xlab='X1', ylab='X2', main='Dataset with non-linear separation')\n\n\n\n\n\n\n\n# Best model.\nset.seed(3)\ntune.out=tune(svm,\n              y ~ .,\n              data = train.set,\n              kernel='linear',\n              ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))\nbestmod=tune.out$best.model\nplot(bestmod, dat)\n\n\n\n\n\n# Predictions on training set.\nypred=predict(bestmod, train.set)\ntable_4_train = table(predict=ypred, truth=train.set$y)\nprint(table_4_train)\n\n       truth\npredict -1  1\n     -1 50 20\n     1   0  0\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_train), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 71.43 percent\"\n\n\n\n#Predictions on the test set.\nypred=predict(bestmod, test.set)\ntable_4_test = table(predict=ypred, truth=test.set$y)\nprint(table_4_test)\n\n       truth\npredict -1  1\n     -1 20 10\n     1   0  0\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_test), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 66.67 percent\"\n\n\n\n\n\n\n# Best model using cross validaitonon a set of values for cost and gamma.\nset.seed(3)\ntune.out=tune(svm, y~., data=train.set, kernel='radial',\nranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))\nbestmod = tune.out$best.model\nplot(bestmod, train.set)\n\n\n\n\n\n# Predictions on training set.\nypred=predict(bestmod, train.set)\ntable_4_train = table(predict=ypred, truth=train.set$y)\nprint(table_4_train)\n\n       truth\npredict -1  1\n     -1 50  0\n     1   0 20\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_train), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 100 percent\"\n\n\n\n#Predictions on the test set.\nypred=predict(bestmod, test.set)\ntable_4_test = table(predict=ypred, truth=test.set$y)\nprint(table_4_test)\n\n       truth\npredict -1  1\n     -1 20  0\n     1   0 10\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_test), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 100 percent\"\n\n\n\n\n\n\nWe have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary.We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features.\n\nGenerate a data set with n = 500 and p = 2, such that the observations belong to two classes with a quadratic decision boundary between them.\n\n\nx1 <- runif (500) - 0.5\nx2 <- runif (500) - 0.5\ny <- 1 * (x1^2 - x2^2 > 0)\ndf <- data.frame(x1=x1, x2=x2, y=as.factor(y))\n\n\nPlot the observations, colored according to their class labels. Your plot should display X1 on the x-axis, and X2 on the yaxis.\n\n\nplot(x1,x2,col = (2 - y))\n\n\n\n\n\nFit a logistic regression model to the data, using X1 and X2 as predictors.\n\n\nglm.fit = glm(y~x1+x2, data=df, family = 'binomial')\n# Predictions\nglm.probs = predict(glm.fit, newdata=df, type = 'response')\nglm.preds = rep(0,500)\nglm.preds[glm.probs>0.50] = 1\ntable_5_c <- table(preds=glm.preds, truth=df$y)\nprint(table_5_c)\n\n     truth\npreds   0   1\n    0  24  21\n    1 214 241\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_c), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 53 percent\"\n\n\n\nApply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be linear.\n\n\n# Plot using predicted class labels for observations.\nplot(x1,x2,col=2-glm.preds)\n\n\n\n\n\nNow fit a logistic regression model to the data using non-linear functions of X1 and X2 as predictors\n\n\nglm.fit = glm(y~I(x1^2)+I(x2^2), data = df, family = 'binomial')\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nglm.probs = predict(glm.fit, newdata = df, type = 'response')\nglm.preds = rep(0,500)\nglm.preds[glm.probs>0.5] = 1\ntable_5_e <- table(preds=glm.preds, truth=df$y)\nprint(table_5_e)\n\n     truth\npreds   0   1\n    0 238   0\n    1   0 262\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_e), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 100 percent\"\n\n\n\nApply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be obviously non-linear.\n\n\nplot(x1,x2,col=2-glm.preds)\n\n\n\n\n\nFit a support vector classifier to the data with X1 and X2 as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.\n\n\n#Best model\ntune.out=tune(svm,y~.,data = df,kernel='linear',\nranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))\nbestmod=tune.out$best.model\n\n#Predictions\nypred=predict(bestmod, newdata=df, type='response')\n\ntable_5_g <- table(predict=ypred, truth=df$y)\nprint(table_5_g)\n\n       truth\npredict   0   1\n      0   0   0\n      1 238 262\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_g), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 52.4 percent\"\n\nplot(x1,x2,col=ypred)\n\n\n\n\n\nFit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.\n\n\ntune.out=tune(svm, y~., data=df, kernel='radial',\nranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))\nbestmod=tune.out$best.mode\n\n#Predictions\nypred=predict(bestmod, newdata=df, type='response')\n\ntable_5_h <- table(predict=ypred, truth=df$y)\nprint(table_5_h)\n\n       truth\npredict   0   1\n      0 235   0\n      1   3 262\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_h), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 99.4 percent\"\n\nplot(x1,x2,col=ypred)\n\n\n\n\n\n\n\nAt the end of Section 9.6.1, it is claimed that in the case of data that is just barely linearly separable, a support vector classifier with a small value of cost that misclassifies a couple of training observations may perform better on test data than one with a huge value of cost that does not misclassify any training observations. You will now investigate this claim.\n\nGenerate two-class data with p = 2 in such a way that the classes are just barely linearly separable.\n\n\nset.seed(111)\nx1 = runif(1000,-5,5)\nx2 = runif(1000,-5,5)\nx = cbind(x1,x2)\ny = rep(NA,1000)\n# Classify points above abline(1.5,1) as 1 and below abline(-1.5,1) as -1, and the rest as 0.\n\n#Removing points classed as 0 will created a more widely separated dataset.\n# Actual decision boundary is a line where y=x, which is abline(0,1).\nfor (i in 1:1000)\nif (x[i,2]-x[i,1] > 1.5) y[i]=1 else if (x[i,2]-x[i,1] < -1.5) y[i]=-1 else y[i]=0\n\n# Combine x an y and remove all rows with y=0.\nx = cbind(x,y)\nx = x[x[,3]!=0,]\n\nplot(x[,1],x[,2],col=2-x[,3], xlab=\"X1\", ylab=\"X2\",xlim = c(-5,5), ylim = c(-5,5))\nabline(0,1, col=\"red\")\nabline(1.5,1)\nabline(-1.5,1)\nabline(h=0,v=0)\n\n\n\n\n\n#Generate random points to be used as noise along line y=1.5x(+-0.1).\nx.noise = matrix(NA,100,3)\nx.noise[,1] = runif(100,-5,5)\n\n#Y coordinate values for first 50 points\nx.noise[1:50,2] = (1.5*x.noise[1:50,1])-0.1 \nx.noise[51:100,2] = (1.5*x.noise[51:100,1])+0.1\nx.noise[,3] = c(rep(-1,50), rep(1,50)) # class values for all noise observations\nplot(x[,1],x[,2],col=2-x[,3], xlab='X1', ylab='X2',\nylim = c(-5,5),xlim = c(-5,5),\nmain=\"Dataset that is linearly separable and with added noise\")\npar(new = TRUE)\nplot(x.noise[,1],x.noise[,2],col=2-x.noise[,3], axes=F,\nxlab=\"\", ylab=\"\", ylim = c(-5,5), xlim = c(-5,5))\n#Noise\nabline(0,1.5,col=\"blue\")\n#Actual decision boundary\nabline(0,1,col=\"red\")\n\n\n\n\n\nCompute the cross-validation error rates for support vector classifiers with a range of cost values. How many training errors are misclassified for each value of cost considered, and how does this relate to the cross-validation errors obtained?\n\n\nx = rbind(x,x.noise)\ntrain.dat = data.frame(x1=x[,1],x2=x[,2], y=as.factor(x[,3]))\n\n#Linear SVM models with various values of cost.\ntune.out=tune(svm,y~.,data=train.dat,kernel='linear',\nranges=list(cost=c(0.001,0.01,0.1,1,5,10,100,1000)))\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n  100\n\n- best performance: 0 \n\n- Detailed performance results:\n   cost      error dispersion\n1 1e-03 0.05351161 0.02002264\n2 1e-02 0.05352630 0.01834727\n3 1e-01 0.05352630 0.01834727\n4 1e+00 0.05352630 0.01834727\n5 5e+00 0.04745813 0.02110627\n6 1e+01 0.04138995 0.02317355\n7 1e+02 0.00000000 0.00000000\n8 1e+03 0.00000000 0.00000000\n\n\n\nGenerate an appropriate test data set, and compute the test errors corresponding to each of the values of cost considered. Which value of cost leads to the fewest test errors, and how does this compare to the values of cost that yield the fewest training errors and the fewest cross-validation errors?\n\n\n# New test set\nset.seed(1221)\ntest.x1 = runif(1000,-5,5)\ntest.x2 = runif(1000,-5,5)\ntest.y = rep(NA,1000)\n\n# Actual decision boundary of the train set is y=x,\n# so points above line are classed as 1 and points below -1\nfor (i in 1:1000){\n  if (test.x1[i]-test.x2[i] < 0) test.y[i]=1 else if (test.x1[i]-test.x2[i] > 0) test.y[i]=-1\n}\n\n# Test dataframe\ntest.dat = data.frame(x1=test.x1,x2=test.x2,y=as.factor(test.y))\n\nplot(test.dat$x1,test.dat$x2,col=2-test.y, xlab=\"X1\", ylab=\"X2\")\n\n\n\n\n\n# Performance of model with cost of 0.1 on test set.\nsvmfit = svm(y~., data = train.dat, kernel = 'linear', cost = 0.1)\nypred = predict(svmfit, newdata = test.dat, type = 'response')\ntable_6_1 <- table(predict=ypred, truth=test.dat$y)\nprint(table_6_1)\n\n       truth\npredict  -1   1\n     -1 483  10\n     1    4 503\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_6_1), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 98.6 percent\"\n\n\n\n# Performance of model with cost of 10 on test set.\nsvmfit = svm(y~., data = train.dat, kernel = 'linear', cost = 10)\nypred = predict(svmfit, newdata = test.dat, type = 'response')\ntable_6_2 <- table(predict=ypred, truth=test.dat$y)\nprint(table_6_2)\n\n       truth\npredict  -1   1\n     -1 446  48\n     1   41 465\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_6_2), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 91.1 percent\"\n\n\n\n\n\nIn this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set.\n\nstr(Auto)\n\n'data.frame':   392 obs. of  9 variables:\n $ mpg         : num  18 15 18 16 17 15 14 14 14 15 ...\n $ cylinders   : num  8 8 8 8 8 8 8 8 8 8 ...\n $ displacement: num  307 350 318 304 302 429 454 440 455 390 ...\n $ horsepower  : num  130 165 150 150 140 198 220 215 225 190 ...\n $ weight      : num  3504 3693 3436 3433 3449 ...\n $ acceleration: num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n $ year        : num  70 70 70 70 70 70 70 70 70 70 ...\n $ origin      : num  1 1 1 1 1 1 1 1 1 1 ...\n $ name        : Factor w/ 304 levels \"amc ambassador brougham\",..: 49 36 231 14 161 141 54 223 241 2 ...\n\n\n\nCreate a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.\n\n\nset.seed(222)\nauto.length = length(Auto$mpg)\nmpg.median = median(Auto$mpg)\nmpg01 = rep(NA,auto.length)\n# Class 1 if car's mpg is above median and 0 if below. Results stored in mpg01 variable.\nfor (i in 1:auto.length) if (Auto$mpg[i] > mpg.median) mpg01[i]=1 else mpg01[i]=0\n# Dataframe\nauto.df = Auto\nauto.df$mpg01 = as.factor(mpg01)\n\n\nFit a support vector classifier to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter.\n\n\n# Using a linear SVM to predict mpg01.\nlinear.tune=tune(svm,mpg01~.,data=auto.df,kernel='linear',\nranges=list(cost=c(0.001,0.01,0.1,1,5,10,100,1000)))\nsummary(linear.tune)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n    1\n\n- best performance: 0.01275641 \n\n- Detailed performance results:\n   cost      error dispersion\n1 1e-03 0.09955128 0.04760888\n2 1e-02 0.07666667 0.04375200\n3 1e-01 0.04596154 0.02359743\n4 1e+00 0.01275641 0.01808165\n5 5e+00 0.01532051 0.01318724\n6 1e+01 0.01788462 0.01234314\n7 1e+02 0.03057692 0.01606420\n8 1e+03 0.03057692 0.01606420\n\nlinear.tune$best.parameters\n\n  cost\n4    1\n\nlinear.tune$best.performance\n\n[1] 0.01275641\n\n\n\nNow repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of gamma and degree and cost.\n\n\n# Using a radial SVM to predict mpg01.\nradial.tune=tune(svm,mpg01~.,data=auto.df,kernel='radial',\nranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))\n# summary(radial.tune)\n\nradial.tune$best.parameters\n\n  cost gamma\n3   10   0.5\n\nradial.tune$best.performance\n\n[1] 0.04820513\n\n\n\n# Using a polynomial SVM to predict mpg01.\npolynomial.tune=tune(svm,mpg01~.,data=auto.df,kernel='polynomial',\nranges=list(cost=c(0.1,1,10,100,1000), degree=c(1,2,3,4,5)))\n# summary(polynomial.tune)\n\npolynomial.tune$best.parameters\n\n  cost degree\n5 1000      1\n\npolynomial.tune$best.performance\n\n[1] 0.01282051\n\n\n\n\n\nThis problem involves the OJ data set which is part of the ISLR2 package.\n\nCreate a training set containing a random sample of 800 observations, and a test set containing the remaining observations.\n\n\nset.seed(131)\n# Training and test sets.\nsample.data = sample.split(OJ$Purchase, SplitRatio = 800/length(OJ$Purchase))\ntrain.set = subset(OJ, sample.data==T)\ntest.set = subset(OJ, sample.data==F)\n\n\nFit a support vector classifier to the training data using cost = 0.01, with Purchase as the response and the other variables as predictors.\n\n\nsvmfit = svm(Purchase~., data = train.set, kernel = \"linear\", cost=0.01)\nsummary(svmfit)\n\n\nCall:\nsvm(formula = Purchase ~ ., data = train.set, kernel = \"linear\", \n    cost = 0.01)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  0.01 \n\nNumber of Support Vectors:  438\n\n ( 220 218 )\n\n\nNumber of Classes:  2 \n\nLevels: \n CH MM\n\n\n\nWhat are the training and test error rates?\n\n\n# Predictions on training set\nsvm.pred = predict(svmfit, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n\n       truth\npredict  CH  MM\n     CH 435  76\n     MM  53 236\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n\n[1] \"Error rate: 16.12 percent\"\n\n\n\n# Predictions on test set\nsvm.pred = predict(svmfit, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n\n       truth\npredict  CH  MM\n     CH 151  36\n     MM  14  69\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n\n[1] \"Error rate: 18.52 percent\"\n\n\n\nUse the tune() function to select an optimal cost.\n\n\n# Using cross validation to select optimal cost\nset.seed(131)\ntune.out = tune(svm, Purchase~., data = train.set, kernel = \"linear\",\nranges=list(cost=c(0.01,0.1,0.5,1,10)))\n\n\nCompute the training and test error rates using this new value for cost.\n\n\n# Predictions on training set\nsvm.pred = predict(tune.out$best.mod, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n\n       truth\npredict  CH  MM\n     CH 432  72\n     MM  56 240\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n\n[1] \"Error rate: 16 percent\"\n\n\n\n# Predictions on test set\nsvm.pred = predict(tune.out$best.mod, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n\n       truth\npredict  CH  MM\n     CH 151  34\n     MM  14  71\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n\n[1] \"Error rate: 17.78 percent\"\n\n\n\nRepeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma.\n\n\nset.seed(131)\ntune.out = tune(svm, Purchase~., data = train.set, kernel = \"radial\",\nranges=list(cost=c(0.01,0.1,0.5,1,10)))\n\n\n# Predictions on training set\nsvm.pred = predict(tune.out$best.mod, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n\n       truth\npredict  CH  MM\n     CH 450  81\n     MM  38 231\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n\n[1] \"Error rate: 14.88 percent\"\n\n\n\n# Predictions on test set\nsvm.pred = predict(tune.out$best.mod, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n\n       truth\npredict  CH  MM\n     CH 150  37\n     MM  15  68\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n\n[1] \"Error rate: 19.26 percent\"\n\n\n\nRepeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set degree = 2.\n\n\nset.seed(131)\ntune.out = tune(svm, Purchase~., data = train.set, kernel = \"polynomial\",\nranges=list(cost=c(0.01,0.1,0.5,1,10)), degree=2)\n\n\n# Predictions on training set\nsvm.pred = predict(tune.out$best.mod, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n\n       truth\npredict  CH  MM\n     CH 455  76\n     MM  33 236\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n\n[1] \"Error rate: 13.62 percent\"\n\n\n\n# Predictions on test set\nsvm.pred = predict(tune.out$best.mod, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n\n       truth\npredict  CH  MM\n     CH 148  41\n     MM  17  64\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n\n[1] \"Error rate: 21.48 percent\""
  },
  {
    "objectID": "posts/inequalities/inequalities.html",
    "href": "posts/inequalities/inequalities.html",
    "title": "Inequalities",
    "section": "",
    "text": "There are probably several ways to graph mathematical inequalities in R. Here, I will simply try a brute force method of literally plotting many dots.\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\n\nHere, I will simply try a brute force method of literally plotting many dots.\n\nN <- 100 #resolution\nx <- seq(-10, 10, length.out = N)\ny <- seq(-10, 10, length.out = N)\n\ndf <- expand.grid(x,y)\ncolnames(df) <- c(\"xval\", \"yval\")\n\nToday’s tasks come from chapter 9 of the popular ISLR textbook\n\nConceptual Task 1a\n\ndf1 <- df |>\n  # math function\n  mutate(shade = ifelse(yval > 3*xval + 1, \"blue\", \"red\"))\n\n\ndf1 |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # main line\n  geom_segment(aes(x = -(11/3), y = -10, xend = 3, yend = 10),\n               color = \"black\", linewidth = 3) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = 3x + 1\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\nConceptual Task 1b\n\ndf1b <- df |>\n  # math function\n  mutate(shade = ifelse(yval > 1 - 0.5*xval, \"blue\", \"red\"))\n\n\ndf1b |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # main line\n  geom_segment(aes(x = -10, y = 6, xend = 10, yend = -4),\n               color = \"black\", linewidth = 3) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = 1 - 0.5x\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\nConceptual Task 2\n\neuclidean_distance <- function(x1, y1, x2, y2){\n  # computes the Euclidean distance between (x1, y1) and (x2, y2)\n  sqrt( (x2 - x1)^2 + (y2 - y1)^2 )\n}\n\n\ndf2 <- df |>\n  # math function\n  mutate(shade = ifelse(euclidean_distance(xval, yval, -1, 2) > 4, \n                        \"blue\", \"red\"))\n\n\ndf2 |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"(x+1)^2 + (y-2)^2 = 4\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nifelse(euclidean_distance(0, 0, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(-1, 1, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(2, 2, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(3, 8, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"blue\"\n\n\n\n\nConceptual Task 3\n\n\n\n\nobs <- 1:7\nxvals <- c(3,2,4,1,2,4,4)\nyvals <- c(4,2,4,4,1,3,1)\nclass_label <- c(\"Red\", \"Red\", \"Red\", \"Red\", \"Blue\", \"Blue\", \"Blue\")\ndf3 <- data.frame(obs, xvals, yvals, class_label)\n\n\ndf3 |>\n  ggplot() +\n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n  coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Where to Draw the Separating Hyperplane?\",\n       # subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\ndf3 |>\n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  # margins\n  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n    coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\ndf3e <- df3 |>\n  mutate(supp_vec = ifelse(obs %in% c(2,3,5,6), \n                           \"support vector\", \"other data\"))\n  \ndf3e$supp_vec <- factor(df3e$supp_vec,\n                        levels = c(\"support vector\", \"other data\"))\n  \ndf3e |>  \n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  # margins\n  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = supp_vec),\n             size = 5) +\n    coord_equal() +\n  scale_color_manual(values = c(\"purple\", \"gray50\")) +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\ndf3 |>\n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0, y = 1/4, xend = 5, yend = 16/4),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n    coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Separating Hyperplane (not optimal)\",\n       subtitle = \"y = 0.25(3x+1)\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\nnew_dot <- data.frame(obs = 8, xvals = 0, yvals = 5, class_label = \"blue\")\ndf3h <- rbind(df3, new_dot)\ndf3h |>\n  ggplot() +\n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n  coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Where to Draw the Separating Hyperplane?\",\n       subtitle = \"new data at (0,5)\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.2 dplyr_1.1.2  \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       cli_3.6.1         knitr_1.43        rlang_1.1.1      \n [5] xfun_0.39         generics_0.1.3    jsonlite_1.8.4    labeling_0.4.2   \n [9] glue_1.6.2        colorspace_2.1-0  htmltools_0.5.5   scales_1.2.1     \n[13] fansi_1.0.4       rmarkdown_2.22    grid_4.3.0        munsell_0.5.0    \n[17] evaluate_0.21     tibble_3.2.1      fastmap_1.1.1     yaml_2.3.7       \n[21] lifecycle_1.0.3   compiler_4.3.0    htmlwidgets_1.6.2 pkgconfig_2.0.3  \n[25] rstudioapi_0.14   farver_2.1.1      digest_0.6.31     R6_2.5.1         \n[29] tidyselect_1.2.0  utf8_1.2.3        pillar_1.9.0      magrittr_2.0.3   \n[33] withr_2.5.0       tools_4.3.0       gtable_0.3.3"
  }
]