---
title: "Linear Regression Demonstration"
author: "Derek Sollberger"
date: "2023-08-23"
format:
  html:
    theme: cerulean
    toc: true

---

\newcommand{\ds}{\displaystyle}


# Warm-Up

## On Predictions

Predict how much moxillation takes place at 70 traxolline.

![](traxolline.png)

# Setting

## Tidyverse

```{r}
#| message: false
#| warning: false
library("moderndive") #for get_regression_table()
library("tidyverse")

df_raw <- readr::read_csv("air_quality_demo_data.csv")

# brand colors
# orange on white: #e77500
# orange on black: #f58025
```


## Data

This data set can be seen at [aqicn.org](https://aqicn.org/station/@402871/#/z/12) and was accessed through the [PurpleAir](https://www2.purpleair.com/) API

```{r}
head(df_raw)
```

* about 10 weeks of weather data, at 6-hour intervals ($n = 291$ observations)
* query: Why do we record different types of particulate matter measurements?

## Wrangling

```{r}
#| message: false
#| warning: false
df <- df_raw |>
  separate(time_stamp, into = c("date", "time"), 
           remove = FALSE, sep = " ") |>
  mutate(time = ifelse(is.na(time), "00:00:00", time),
         day_part = case_when(
           time == "06:00:00" ~ "morning",
           time == "12:00:00" ~ "noon",
           time == "18:00:00" ~ "evening",
           .default = "midnight"
         )) |>
  rename(pm1_0 = pm1.0_atm, pm2_5 = pm2.5_atm) |>
  select(time, day_part, humidity, temperature, pm2_5)
```

* `separate` the time stamp into `date` and `time` columns
* described parts of the day: morning, noon, evening, midnight
* renamed particulate matter columns for ease

```{r}
head(df)
```

# Linear Regression

::::: {.panel-tabset}

## Setup

* response variable ($Y$): particulate matter (2.5 microns)
* predictor variable ($X_{1}$): temperature	

$$Y = a + b_{1}X_{1}$$

* context: levels above 35 micrograms per cubic meter are considered unhealthy (according to the [Indoor Air Hygiene Institute](https://www.indoorairhygiene.org/pm2-5-explained/))

## Graph

```{r}
#| echo: false
#| eval: true
df |>
  ggplot(aes(x = temperature, y = pm2_5)) +
  geom_point() +
  geom_hline(yintercept = 35, color = "#e77500",
             linewidth = 3, linetype = 2) +
  labs(title = "Princeton Air Quality",
       subtitle = "Summer 2023",
       caption = "Source: PurpleAir") +
  theme_minimal()
```

## Code

```{r}
#| echo: true
#| eval: false
df |>
  ggplot(aes(x = temperature, y = pm2_5)) +
  geom_point() +
  geom_hline(yintercept = 35, color = "#e77500",
             linewidth = 3, linetype = 2) +
  labs(title = "Princeton Air Quality",
       subtitle = "Summer 2023",
       caption = "Source: PurpleAir") +
  theme_minimal()
```

:::::

## Where Fit?

![Where do we draw the line?](many_splits.gif)

## Residuals

:::: {.columns}

::: {.column width="70%"}
Goal: Given a bivariate data set $\{x_{i}, y_{i}\}_{i=1}^{n}$, form a **linear regression model**

$$\hat{y} = a + bx$$

that ``best fits'' the data.  Note that such a line will not go through all of the data (except in linear, deterministic situations), so

- denote $y_{i}$ for true outcomes
- denote $\hat{y}_{i}$ for estimates (or predictions)
- then $y_{i} - \hat{y}_{i}$ is the $i^{\text{th}}$ *residual*
:::

::: {.column width="30%"}
![image credit: www.jmp.com](residuals.png)
:::

::::

## Method of Least Squares

Like our derivation of formulas for variance and standard deviation, scientists decided to square the residuals (focus on size of residuals, avoid positive versus negative signs).  Let the *total error* be

$$E(a,b) = \ds\sum_{i = 1}^{n} (y_{i} - \hat{y}_{i})^{2} = \ds\sum_{i = 1}^{n} (y_{i} - a - bx_{i})^{2} $$

- The ``best-fit line'' minimizes the error.
- To minimize the error, start by setting the partial derivatives equal to zero:

$$\ds\frac{\partial E}{\partial a} = 0, \quad \ds\frac{\partial E}{\partial b} = 0$$
Thankfully, the function $E(a,b)$ is an elliptical paraboloid, so there is a global minimum at the critical point, and that minimum is found where

$$a = \ds\frac{ (\sum y_{i})(\sum x_{i}^{2}) - (\sum x_{i})(\sum x_{i}y_{i}) }{ n\sum x_{i}^{2} - (\sum x_{i})^{2} }, \quad b = \ds\frac{ n\sum x_{i}y_{i} - (\sum x_{i})(\sum y_{i}) }{ n\sum x_{i}^{2} - (\sum x_{i})^{2} }$$

::: {.callout-note collapse="true"}
## Linear Regression Model (Another View) 

If sample means $\bar{x}$ and $\bar{y}$, sample standard deviations $s_{x}$ and $s_{y}$, and correlation coefficient $r$ were previously computed, then the best-fit linear regression line $\hat{y} = mx + b$ is computed with

$$m = \ds\frac{ rs_{y} }{ s_{x} }, \quad b = \bar{y} - m\bar{x}$$

- If correlation $r > 0$, then the slope of the regression line is also positive
- If correlation $r < 0$, then the slope of the regression line is also negative
:::

::: {.callout-warning collapse="true"}
## Outliers
In a scatterplot, an **outlier** is a point lying far away from the other data points.  Paired sample data may include one or more *influential points*, which are points that strongly affect the graph of the regression line.
:::

## ggplot

::::: {.panel-tabset}

## geom_smooth

The `geom_smooth` layer is a quick way to draw the linear regression graph in `ggplot`.

## Graph

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| code-line-numbers: "4"
df |>
  ggplot(aes(x = temperature, y = pm2_5)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#e77500", 
              linewidth = 3, se = FALSE) +
  labs(title = "Princeton Air Quality",
       subtitle = "Summer 2023",
       caption = "Source: PurpleAir") +
  theme_minimal()
```

## Code

```{r}
#| echo: true
#| eval: false
#| message: false
#| warning: false
#| source-line-numbers: 4
df |>
  ggplot(aes(x = temperature, y = pm2_5)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#e77500", 
              linewidth = 3, se = FALSE) +
  labs(title = "Princeton Air Quality",
       subtitle = "Summer 2023",
       caption = "Source: PurpleAir") +
  theme_minimal()
```

:::::

## Model

```{r}
model1 <- lm(pm2_5 ~ temperature, data = df)
```

```{r}
model1
```

* Interpretation: for every one-degree increase in temperature, the PM2.5 level increases by 0.3805


## Prediction

::::: {.panel-tabset}

## Example

Predict the PM2.5 level for a 78-degree day.

## predict

```{r}
predict(model1,
        newdata = data.frame(temperature = 78))
```

## Activity

Think about what is meant by linear regression.  Draw a large area for a graph ($x$- and $y$-axes, first quadrant only, you do not have to label the axes).  Come up with a scatterplot situation where performing linear regression and a subsequent prediction would be a *bad idea*.

:::::

# Multivariate Linear Regression

::::: {.panel-tabset}

## Setup

* response variable ($Y$): particulate matter (2.5 microns)
* predictor variables 

    - $X_{1}$: temperature
    - $X_{2}$: humidity

$$Y = a + b_{1}X_{1} + b_{2}X_{2}$$

## Model

```{r}
model2 <- lm(pm2_5 ~ temperature + humidity,
             data = df)
```

## Activity

Describe the regression coefficients for the two predictor variables (hint: rates of change).

```{r}
moderndive::get_regression_table(model2)
```

## Example

Predict the PM2.5 level for a 78-degree day where the humidity is 50 percent.

## predict

```{r}
predict(model2,
        newdata = data.frame(temperature = 78,
                             humidity = 50))
```

:::::

# Categorical Variables

::::: {.panel-tabset}

## Setup

* response variable ($Y$): particulate matter (2.5 microns)
* predictor variables 

    - $X_{1}$: temperature

$$Y = a + b_{1}X_{1} + b_{3}X_{3} + b_{4}X_{4} + b_{5}X_{5}$$

$$X_{3} = \begin{cases} 1 & \text{morning} \\ 0 & \text{otherwise}
\end{cases}$$

$$X_{4} = \begin{cases} 1 & \text{afternoon} \\ 0 & \text{otherwise}
\end{cases}$$

$$X_{5} = \begin{cases} 1 & \text{evening} \\ 0 & \text{otherwise}
\end{cases}$$

## Model

```{r}
model3 <- lm(pm2_5 ~ temperature + day_part,
             data = df)
```

```{r}
moderndive::get_regression_table(model3)
```
## Activity

* Predict the PM2.5 level for a 60-degree morning.
* Predict the PM2.5 level for a 75-degree evening.

$$Y = -42.844 + 0.680X_{1} + 5.056X_{3} + 8.477X_{4} + 4.186X_{5}$$

$$X_{3} = \begin{cases} 1 & \text{morning} \\ 0 & \text{otherwise}
\end{cases}$$

$$X_{4} = \begin{cases} 1 & \text{afternoon} \\ 0 & \text{otherwise}
\end{cases}$$

$$X_{5} = \begin{cases} 1 & \text{evening} \\ 0 & \text{otherwise}
\end{cases}$$

## Graph

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| code-line-numbers: "4"
df |>
  group_by(day_part) |>
  ggplot(aes(x = temperature, y = pm2_5,
             color = day_part, group = day_part)) +
  geom_point() +
  geom_smooth(aes(color = day_part), method = "lm",
              linewidth = 3, se = FALSE) +
  labs(title = "Princeton Air Quality",
       subtitle = "Summer 2023",
       caption = "Source: PurpleAir") +
  theme_minimal() +
  ylim(0,50)
```

## code

```{r}
#| echo: true
#| eval: false
#| message: false
#| warning: false
#| code-line-numbers: "5"
df |>
  group_by(day_part) |>
  ggplot(aes(x = temperature, y = pm2_5,
             color = day_part, group = day_part)) +
  geom_point() +
  geom_smooth(aes(color = day_part), method = "lm",
              linewidth = 3, se = FALSE) +
  labs(title = "Princeton Air Quality",
       subtitle = "Summer 2023",
       caption = "Source: PurpleAir") +
  theme_minimal() +
  ylim(0,50)
```

## predict

```{r}
predict(model3,
        newdata = data.frame(temperature = 60,
                             day_part = "morning"))
```
```{r}
predict(model3,
        newdata = data.frame(temperature = 75,
                             day_part = "evening"))
```

:::::




# Looking Ahead