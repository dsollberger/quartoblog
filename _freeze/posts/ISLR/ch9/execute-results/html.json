{
  "hash": "b3e58d2034e8b40e1a321ff0c277c7bb",
  "result": {
    "markdown": "---\ntitle: \"SVMs\"\nauthor: \"Derek Sollberger\"\nformat: html\ndate: \"2023-06-03\"\n---\n\n\n# Support Vector Machines\n\n**Learning objectives:**\n\n- Implement a binary classification model using a **maximal margin classifier**.\n- Implement a binary classification model using a **support vector classifier**.\n- Implement a binary classification model using a **support vector machine** (SVM).\n- Generalize SVM models to **multi-class** cases.\n\n**Support vector machine (SVM)**, an approach for classification developed in 1990. \nSVM is a generalizaion of classifiers methods, in particular:\n\n- **maximal margin classifier** (it requires that the classes be separable by a linear boundary).\n- **support vector classifier**\n- **support vector machine**: binary classification setting with two classes\n\n\n::: {.cell layout-align=\"center\" fig.dim='100%'}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"caTools\")\nlibrary(\"dplyr\")\nlibrary(\"e1071\")\nlibrary(\"ggplot2\")\nlibrary(\"ISLR\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ISLR_1.4       ggplot2_3.4.2  e1071_1.7-13   dplyr_1.1.2    caTools_1.18.2\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       cli_3.6.1         knitr_1.43        rlang_1.1.1      \n [5] xfun_0.39         generics_0.1.3    jsonlite_1.8.4    glue_1.6.2       \n [9] colorspace_2.1-0  htmltools_0.5.5   scales_1.2.1      fansi_1.0.4      \n[13] rmarkdown_2.22    grid_4.3.0        munsell_0.5.0     evaluate_0.21    \n[17] tibble_3.2.1      bitops_1.0-7      fastmap_1.1.1     yaml_2.3.7       \n[21] lifecycle_1.0.3   compiler_4.3.0    htmlwidgets_1.6.2 pkgconfig_2.0.3  \n[25] rstudioapi_0.14   digest_0.6.31     R6_2.5.1          class_7.3-21     \n[29] tidyselect_1.2.0  utf8_1.2.3        pillar_1.9.0      magrittr_2.0.3   \n[33] withr_2.5.0       gtable_0.3.3      proxy_0.4-27      tools_4.3.0      \n```\n:::\n:::\n\n\n\n## Hyperplane\n\n![image credit: Deep AI](images/hyperplane.png)\n\n- A *hyperplane* is a $p-1$-dimensional flat subspace of a $p$-dimensional space. For example, in a 2-dimensional space, a hyperplane is a flat one-dimensional space: a line. \n- (standard form) Definition of 2D hyperplane in 3D space:\n$$\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3}= 0$$\n- (inner products) Any $X$ s.t. $X = (X_{1}, X_{2})^T$ for which the equation above is satisfied is a point on the hyperplane.\n\nAdditional resource: [Deep AI](https://deepai.org/machine-learning-glossary-and-terms/hyperplane)\n\n\n## Separating Hyperplane\n\n- Consider a matrix **X** of dimensions $n*p$, and a $y_{i} \\in \\{-1, 1\\}$. We have a new observation, $x^*$, which is a vector $x^* = (x^*_{1}...x^*_{p})^T$ which we wish to classify to one of two groups.\n- We will use a *separating hyperplane* to classify the observation.\n\n![](images/fig9_2.png)\n \n- We can label the blue observations as $y_{i} = 1$ and the pink observations as $y_{i} = -1$. \n- Thus, a separating hyperplane has the property s.t. $\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip} > 0$ if $y_{i} =1$ and $\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip} < 0$ if $y_{i} = -1$.\n- In other words, a separating hyperplane has the property s.t. $y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) > 0$ for all $i = 1...n$.\n- Consider also the *magnitude* of $f(x^*)$. If it is far from zero, we are confident in its classification, whereas if it is close to 0, then $x^*$ is located near the hyperplane, and we are less confident about its classification.\n\n## Maximal Margin Classifier\n\n![](images/fig9_3.png)\n\n- Generally, if data can be perfectly separated using a hyperplane, an infinite amount of such hyperplanes exist.\n- An intuitive choice is the *maximal margin hyperplane*, which is the hyperplane that is farthest from the training data.\n- We compute the perpendicular distance from each training observation to the hyperplane. The smallest of these distances is known as the *margin*.\n- The *maximal margin hyperplane* is the hyperplane for which the *margin* is maximized. We can classify a test observation based on which side of the maximal margin hyperplane it lies on, and this is known as the *maximal margin classifier*.\n- The maximal margin classifier classifies $x^*$ based on the sign of $f(x^*) = \\beta_{0} + \\beta_{1}x^*_{1} + ... + \\beta_{p}x^*_{p}$.\n\n![](images/fig9_3.png)\n\n- Note the 3 training observations that lie on the margin and are equidistant from the hyperplane. These are the **support vectors** (vectors in $p$-dimensional space; in this case $p=2$).\n- They support the hyperplane because if their location was changed, the hyperplane would change. \n- The maximal margin hyperplane depends on these observations, *but not the others* (unless the other observations were moved at or within the margin).\n\n## Mathematics of the MMC\n\n- Consider constructing an MMC based on the training observations $x_{1}...x_{n} \\in \\mathbb{R}^p$. This is the solution to the optimization problem:\n\n$$\\text{max}_{\\beta_{0}...\\beta_{p}, M} \\space M$$\n$$\\text{subject to } \\sum_{j=1}^{p}\\beta_{j}^2 = 1$$\n$$y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) \\geq M \\quad \\forall i = 1...n$$\n\n- $M$ is the *margin*, and the $\\beta$ coeffients are chosen to maximize $M$. \n- The constraint (3rd equation) ensures that each observation will be correctly classified, as long as M is positive. \n\n![](images/fig9_3.png)\n\n- The 2nd and 3rd equations ensure that each data point is on the correct side of the hyperplane and at least M-distance away from the hyperplane.\n- The perpendicular distance to the hyperplane is given by $y_{i}(\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} ... + \\beta_{p}x_{ip})$.\n\n> But what if our data is not separable by a linear hyperplane?\n\n![](images/fig9_4.png)\n\n> Individual data points greatly affect formation of the maximal margin classifier\n\n![](images/fig9_5.png)\n\n\n## Support Vector Classifiers\n\n- We can't always use a hyperplane to separate two classes. \n- Even if such a classifier does exist, it's not always desirable, due to overfitting or too much sensitivity to individual observations.\n- Thus, it might be worthwhile to consider a classifier/hyperplane that misclassifies a few observations in order to improve classification of the remaining data points. \n- The *support vector classifier*, a.k.a the *soft margin classifier*, allows some training data to be on the wrong side of the margin or even the hyperplane. \n\n\n## Mathematics of the SVC\n- The SVC classifies a test observation based on which side of the hyperplane it lies. \n\n$$\\text{max}_{\\beta_{0}...\\beta_{p}, \\epsilon_{1}...\\epsilon_{n}, M} \\space M$$\n$$\\text{subject to } \\sum_{j=1}^{p}\\beta_{j}^2 = 1$$\n$$y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) \\geq M(1 - \\epsilon_{i})$$\n$$\\epsilon_{i} \\geq 0, \\quad \\sum_{i=1}^{n}\\epsilon_{i} \\leq C$$\n\n- $C$ is a nonnegative tuning parameter, typically chosen through cross-validation, and can be thought of as the budget for margin violation by the observations. \n- The $\\epsilon_{i}$ are *slack variables* that allow individual observations to be on the wrong side of the margin or hyperplane. The $\\epsilon_{i}$ indicates where the $i^{\\text{th}}$ observation is located with regards to the margin and hyperplane. \n\n    - If $\\epsilon_{i} = 0$, the observation is on the correct side of the margin.\n    - If $\\epsilon_{i} > 0$, the observation is on the wrong side of margin\n    - If $\\epsilon_{i} > 1$, the observation is on the wrong side of the hyperplane. \n    \n- Since $C$ constrains the sum of the $\\epsilon_{i}$, it determines the number and magnitude of violations to the margin. If $C=0$, there is no margin for violation, thus all the $\\epsilon_{1},...,\\epsilon_{n} = 0$. \n- Note that if $C>0$, no more than $C$ observations can be on wrong side of hyperplane, since in these cases $\\epsilon_{i} > 1$. \n\n\n## Tuning Parameter\n\n![](images/fig9_7.png)\n\n- A property of the classifier is that only data points which lie on or violate the margin will affect the hyperplane. These data points are known as *support vectors*.\n- $C$ controls the bias-variance tradeoff of the classifier. \n\n    - When $C$ is large: high bias, low variance\n    - When $C$ is small: low bias, high variance\n\n- The property of the SVC solely being dependent on certain observations in classification differs from other classification methods such as LDA (depends on mean of all observations in each class, as well as each class's covariance matrix using all observations). \n- However, logistic regression is more similar to SVC in that it has low sensitivity to observations far from the decision boundary.\n\n\n## Nonlinear Classification\n\n- Many decision boundaries are not linear.\n- We could fit an SVC to the data using $2p$ features (in the case of $p$ features and using a quadratic form).\n\n$$X_{1}, X_{1}^{2}, \\quad X_{2}, X_{2}^{2}, \\quad\\cdots, \\quad X_{p}, X_{p}^{2}$$\n\n\n$$\\text{max}_{\\beta_{0},\\beta_{11},\\beta_{12},\\dots,\\beta_{p1},\\beta_{p2} \\epsilon_{1},\\dots,\\epsilon_{n}, M} \\space M$$\n$$\\text{subject to }  y_{i}\\left(\\beta_{0} + \\sum_{j=1}^{p} \\beta_{ji}x_{ji} + \\sum_{j=1}^{p} \\beta_{ji}x_{ji}^{2}\\right) \\geq M(1 - \\epsilon_{i})$$\n\n$$\\epsilon_{i} \\geq 0, \\quad \\sum_{i=1}^{n}\\epsilon_{i} \\leq C, \\quad \\sum_{j=1}^{p}\\sum_{k=1}^{2} \\beta_{jk}^{2} = 1$$\n\n- Note that in the enlarged feature space (here, with the quadratic terms), the decision boundary is linear. But in the original feature space, it is quadratic $q(x) = 0$ (in this example), and generally the solutions are not linear.\n- One could also include interaction terms, higher degree polynomials, etc., and thus the feature space could enlarge quickly and entail unmanageable computations.\n\n\n## Support Vector Machines\n\n- The SVM is an extension of the SVC which results from using *kernels* to enlarge the feature space. A kernel is a function that quantifies the similarity of two data points.\n- Essentially, we want to enlarge the feature space to make use of a nonlinear decision boundary, while avoiding getting bogged down in unmanageable calculations.\n- The solution to the SVC problem in the SVM context involves only the *inner products* (AKA dot products) of the observations.\n\n$$\\langle x_{i}  \\; , x_{i'} \\; \\rangle = \\sum_{j=1}^{p}x_{ij}x_{i'j}$$\n\nIn the context of SVM, the linear support vector classifier is as follows:\n\n$$f(x) = \\beta_{0} + \\sum_{i=1}^{n}\\alpha_{i}\\langle \\; x, x_i\\; \\rangle$$\n\n- To estimate the $n$ $\\alpha_{i}$ coefficients and $\\beta_{0}$, we only need the $\\binom{n}{2}$ inner products between all pairs of training observations.\n- Note that in the equation above, in order to compute $f(x)$ for the new point $x$, we need the inner product between the new point and all the training observations. However, $\\alpha_{i} = 0$ for all points that are not on or within the margin (i.e., points that are not support vectors). So we can rewrite the equation as follows, where $S$ is the set of support point indices:\n\n$$f(x) = \\beta_{0} + \\sum_{i \\in S}\\alpha_{i}\\langle \\; x, x_{i} \\; \\rangle$$\n\n- Replace every inner product with $K(x_{i}, x_{i'})$, where $K$ is a kernel function. \n- $K(x_{i}, x_{i'}) = \\sum_{j=1}^{p}x_{ij}x_{i'j}$ is the SVC and is known as a linear kernel since it is linear in the features. \n- One could also have kernel functions of the following form, where $d$ is a positive integer:\n\n$$K(x_{i}, x_{i'}) = \\left(1 + \\sum_{j=1}^{p}x_{ij}x_{i'j}\\right)^d$$\n\n- This will lead to a much more flexible decision boundary, and is basically fitting an SVC in a higher-dimensional space involving polynomials of degree $d$, instead of the original feature space. \n- When an SVC is combined with a nonlinear kernel as above, the result is a *support vector machine*.\n\n$$f(x) =  \\beta_{0} + \\sum_{i \\in S}\\alpha_{i}K(x, x_{i})$$\n\n\n## Radial Kernels\n\n![image credit: Manin Bocss](images/gaussian.png)\n\n- There are other options besides polynomial kernel functions, and a popular one is a *radial kernel*.\n\n$$K(x, x_{i}) = \\text{exp}\\left(-\\gamma\\sum_{j=1}^p(x_{ij} - x_{i'j})^2\\right), \\quad \\gamma > 0$$\n\n- For a given test observations $x^*$, if it is far from $x_{i}$, then $K(x^*, x_{i})$ will be small given the negative $\\gamma$ and large $\\sum_{j=1}^p(x^*_{j} - x_{ij})^2)$. \n- Thus, $x_{i}$ will play little role in $f(x^*)$.\n- The predicted class for $x^*$ is based on the sign of $f(x^*)$, so training observations far from a given test point play little part in determining the label for a test observation.\n- The radial kernel therefore exhibits local behavior with respect to other observations.\n\n## SVM with Radial Kernels\n\n![image credit: Manin Bocss](images/fig9_9.png)\n\n- The advantage of using a kernel rather than simply enlarging feature space is computational, since it is only necessary to compute $\\binom{n}{2}$ kernel functions. \n- For radial kernels, the feature space is implicit and infinite dimensional, so we could not do the computations in such a space anyways.\n\n\n## More than Two Classes\n\n- The concept of separating hyperplanes does not extend naturally to more than two classes, but there are some ways around this.\n- A *one-versus-one* approach constructs $K \\choose 2$ SVMs, where $K$ is the number of classes. An observation is classified to each of the $K \\choose 2$ classes, and the number of times it appears in each class is counted.\n- The $k^\\text{th}$ class might be coded as +1 versus the $(k')^\\text{th}$ class is coded as -1. \n- The data point is classified to the class for which it was most often assigned in the pairwise classifications.\n- Another option is *one-versus-all* classification. This can be useful when there are a lot of classes.\n- $K$ SVMs are fitted, and one of the K classes to the remaining $K-1$ classes. \n- $\\beta_{0k}...\\beta_{pk}$ denotes the parameters that results from constructing an SVM comparing the $k$th class (coded as +1) to the other classes (-1).\n- Assign test observation $x^*$ to the class $k$ for which $\\beta_{0k} + ... + \\beta_{pk}x^*_{p}$ is largest.\n\n## Lab: Support Vector Classifier\n\n<!--\n\n[obsolete link, but could still be a good resource if found]\n\nThis is a hybrid of [Emil's tidymodels labs](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/support-vector-machines.html) and the original lab. I make sure our data matches theirs so we can compare more directly.\n\nEditorial: the lab 9 notes are terse and should probably be revised for clarity and interest\n\n-->\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidymodels\")\nlibrary(\"kernlab\") # We'll use the plot method from this.\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nsim_data <- matrix(\n  rnorm (20 * 2), \n  ncol = 2,\n  dimnames = list(NULL, c(\"x1\", \"x2\"))\n) %>% \n  as_tibble() %>% \n  mutate(\n    y = factor(c(rep(-1, 10), rep(1, 10)))\n  ) %>%\n  mutate(\n    x1 = ifelse(y == 1, x1 + 1, x1),\n    x2 = ifelse(y == 1, x2 + 1, x2)\n  )\n\nsim_data %>% \n  ggplot() +\n  aes(x1, x2, color = y) +\n  geom_point() +\n  labs(title = \"Trying to make a hyperplane classifier\",\n       subtitle = \"simulated data\",\n       caption = \"R4DS book club\") +\n  theme_minimal()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# generated this using their process then saved it to use here.\ntest_data <- readRDS(\"data/09-testdat.rds\") %>% \n  rename(x1 = x.1, x2 = x.2)\n\ntest_data %>% \n  ggplot() +\n  aes(x1, x2, color = y) +\n  geom_point() +\n  labs(title = \"Trying to make a hyperplane classifier\",\n       subtitle = \"simulated data\",\n       caption = \"R4DS book club\") +\n  theme_minimal()\n```\n:::\n\n\nWe create a `spec` for a model, which we'll update throughout this lab with different costs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_linear_spec <- svm_poly(degree = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\", scaled = FALSE)\n```\n:::\n\n\nThen we do a couple fits with manual cost.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_linear_fit_10 <- svm_linear_spec %>% \n  set_args(cost = 10) %>%\n  fit(y ~ ., data = sim_data)\nsvm_linear_fit_10\n\nsvm_linear_fit_10 %>%\n  extract_fit_engine() %>%\n  plot()\n\nsvm_linear_fit_01 <- svm_linear_spec %>% \n  set_args(cost = 0.1) %>%\n  fit(y ~ ., data = sim_data)\nsvm_linear_fit_01\nsvm_linear_fit_01 %>%\n  extract_fit_engine() %>%\n  plot()\n\nsvm_linear_fit_001 <- svm_linear_spec %>% \n  set_args(cost = 0.01) %>%\n  fit(y ~ ., data = sim_data)\nsvm_linear_fit_001\nsvm_linear_fit_001 %>%\n  extract_fit_engine() %>%\n  plot()\n```\n:::\n\n\n### Tuning\n\nLet's find the best cost.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_linear_wf <- workflow() %>%\n  add_model(\n    svm_linear_spec %>% set_args(cost = tune())\n  ) %>%\n  add_formula(y ~ .)\n\nset.seed(1234)\nsim_data_fold <- vfold_cv(sim_data, strata = y)\n\nparam_grid <- grid_regular(cost(), levels = 10)\n\n# Our grid isn't identical to the book, but it's close enough.\nparam_grid\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_res <- tune_grid(\n  svm_linear_wf, \n  resamples = sim_data_fold, \n  grid = param_grid\n)\n\n# We ran this locally and then saved it so everyone doesn't need to wait for\n# this to process each time they build the book.\n\n# saveRDS(tune_res, \"data/09-tune_res.rds\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tune_res)\n```\n:::\n\n\n\nTune can pull out the best result for us.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_cost <- select_best(tune_res, metric = \"accuracy\")\n\nsvm_linear_final <- finalize_workflow(svm_linear_wf, best_cost)\n\nsvm_linear_fit <- svm_linear_final %>% fit(sim_data)\n\nsvm_linear_fit %>% \n  augment(new_data = test_data) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n```\n:::\n\n\n$$\\text{accuracy} = \\frac{9 + 8}{9 + 1 + 2 + 8} = 0.85$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_linear_fit_001 %>% \n  augment(new_data = test_data) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n```\n:::\n\n\n$$\\text{accuracy} = \\frac{11 + 3}{11 + 6 + 0 + 3} = 0.70$$\n\n### Linearly separable data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_data_sep <- sim_data %>% \n  mutate(\n    x1 = ifelse(y == 1, x1 + 0.5, x1),\n    x2 = ifelse(y == 1, x2 + 0.5, x2)\n  )\n\nsim_data_sep %>% \n  ggplot() +\n  aes(x1, x2, color = y) +\n  geom_point()\n\nsvm_fit_sep_1e5 <- svm_linear_spec %>% \n  set_args(cost = 1e5) %>%\n  fit(y ~ ., data = sim_data_sep)\n\nsvm_fit_sep_1e5\nsvm_fit_sep_1e5 %>%\n  extract_fit_engine() %>%\n  plot()\n\nsvm_fit_sep_1 <- svm_linear_spec %>% \n  set_args(cost = 1) %>%\n  fit(y ~ ., data = sim_data_sep)\n\nsvm_fit_sep_1\nsvm_fit_sep_1 %>%\n  extract_fit_engine() %>%\n  plot()\n\ntest_data_sep <- test_data %>% \n  mutate(\n    x1 = ifelse(y == 1, x1 + 0.5, x1),\n    x2 = ifelse(y == 1, x2 + 0.5, x2)\n  )\n\nsvm_fit_sep_1e5 %>% \n  augment(new_data = test_data_sep) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n```\n:::\n\n\n$$\\text{accuracy} = \\frac{9 + 8}{8 + 1 + 2 + 8} = 0.85$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvm_fit_sep_1 %>% \n  augment(new_data = test_data_sep) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n```\n:::\n\n\n$$\\text{accuracy} = \\frac{9 + 9}{9 + 0 + 2 + 9} = 0.90$$\n\n<!--\n\n[obsolete link, but the resource could be good if found]\n\n## Lab: Support Vector Machine (non-linear kernel)\n\nNow that we've seen one that's comparable, I want to focus on the tidymodels version. For the rest of the meeting, see [Emil's version](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/support-vector-machines.html#support-vector-machine)\n\n-->\n\n## Exercises (Conceptual)\n\n<details>\n  <summary>ggplot setup</summary>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 50 #resolution\nx <- seq(-10, 10, length.out = N)\ny <- seq(-10, 10, length.out = N)\n\ndf <- expand.grid(x,y)\ncolnames(df) <- c(\"xval\", \"yval\")\n\neuclidean_distance <- function(x1, y1, x2, y2){\n  # computes the Euclidean distance between (x1, y1) and (x2, y2)\n  sqrt( (x2 - x1)^2 + (y2 - y1)^2 )\n}\n\naccuracy_calculation <- function(confusion_matrix){\n  # computes the accuracy revealed by a 2x2 confusion matrix\n  Q <- confusion_matrix\n  (Q[1,1] + Q[2,2]) / (Q[1,1] + Q[1,2] + Q[2,1] + Q[2,2])\n}\n```\n:::\n\n\n</details> \n\n### Conceptual Task 1\n\nThis problem involves hyperplanes in two dimensions.\n\n(a) \n\n* blue: $1 + 3X_{1} - X_{2} > 0$\n* red: $1 + 3X_{1} - X_{2} < 0$\n\n <details>\n  <summary>code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\ndf1 <- df |>\n  # math function\n  mutate(shade = ifelse(yval > 3*xval + 1, \"blue\", \"red\"))\n\ndf1 |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # main line\n  geom_segment(aes(x = -(11/3), y = -10, xend = 3, yend = 10),\n               color = \"black\", linewidth = 3) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = 3x + 1\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n```\n:::\n\n</details> \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n(b)\n\n* blue: $-2 + X_{1} + 2X_{2} > 0$\n* red: $-2 + X_{1} + 2X_{2} < 0$\n\n <details>\n  <summary>code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\ndf1b <- df |>\n  # math function\n  mutate(shade = ifelse(yval > 1 - 0.5*xval, \"blue\", \"red\"))\n\ndf1b |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # main line\n  geom_segment(aes(x = -10, y = 6, xend = 10, yend = -4),\n               color = \"black\", linewidth = 3) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = 1 - 0.5x\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n```\n:::\n\n</details> \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### Conceptual Task 2\n\nWe now investigate a non-linear decision boundary.\n\n* blue: $(1 + X_{1})^{2} + (2 - X_{2})^{2} > 4$\n* red: $(1 + X_{1})^{2} + (2 - X_{2})^{2} < 4$\n\n <details><summary>code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\ndf2 <- df |>\n  # math function\n  mutate(shade = ifelse(euclidean_distance(xval, yval, -1, 2) > 4, \n                        \"blue\", \"red\"))\n\ndf2 |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"(x+1)^2 + (y-2)^2 = 4\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n```\n:::\n\n</details> \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n(c) To what class is the observation (0, 0) classified? (âˆ’1, 1)? (2, 2)? (3, 8)?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nifelse(euclidean_distance(0, 0, -1, 2) > 4, \"blue\", \"red\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"red\"\n```\n:::\n\n```{.r .cell-code}\nifelse(euclidean_distance(-1, 1, -1, 2) > 4, \"blue\", \"red\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"red\"\n```\n:::\n\n```{.r .cell-code}\nifelse(euclidean_distance(2, 2, -1, 2) > 4, \"blue\", \"red\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"red\"\n```\n:::\n\n```{.r .cell-code}\nifelse(euclidean_distance(3, 8, -1, 2) > 4, \"blue\", \"red\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"blue\"\n```\n:::\n:::\n\n\n(d) While the decision boundary\n\n$$(1 + X_{1})^{2} + (2 - X_{2})^{2} = 4$$\n\nis not linear in $X_{1}$ and $X_{2}$, it is linear in terms of $X_{1}$, $X_{1}^{2}$, $X_{2}$, $X_{2}^{2}$\n\n$$\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}^{2} + \\beta_{4}X_{2}^{2} = 0$$\n\nwith $\\beta_{0} = 1$, $\\beta_{1} = 2$, $\\beta_{2} = -4$, $\\beta_{3} = 1$, and $\\beta_{4} = 1$.\n\n### Conceptual Task 3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs <- 1:7\nxvals <- c(3,2,4,1,2,4,4)\nyvals <- c(4,2,4,4,1,3,1)\nclass_label <- c(\"Red\", \"Red\", \"Red\", \"Red\", \"Blue\", \"Blue\", \"Blue\")\ndf3 <- data.frame(obs, xvals, yvals, class_label)\ndf3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  obs xvals yvals class_label\n1   1     3     4         Red\n2   2     2     2         Red\n3   3     4     4         Red\n4   4     1     4         Red\n5   5     2     1        Blue\n6   6     4     3        Blue\n7   7     4     1        Blue\n```\n:::\n:::\n\n\n(a) We are given $n = 7$ observations in $p = 2$ dimensions. For each\nobservation, there is an associated class label.\n\n <details>\n  <summary>code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\ndf3 |>\n  ggplot() +\n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n  coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Where to Draw the Separating Hyperplane?\",\n       # subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n```\n:::\n\n</details> \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n(b) Sketch the optimal separating hyperplane, and provide the equation\nfor this hyperplane\n\n <details>\n  <summary>code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\ndf3 |>\n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  # margins\n  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n    coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n```\n:::\n\n</details> \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n(c)\n\n* blue: $0.5 - X_{1} + X_{2} < 0$\n* red: $0.5 - X_{1} + X_{2} > 0$\n\n(d) maximal margin in indicated by the dashed lines, with margin\n\n$$M = \\frac{0.5}{\\sqrt{2}} \\approx 0.3536$$\n(e) Indicate the support vectors for the maximal margin classifier.\n\n <details>\n  <summary>code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\ndf3e <- df3 |>\n  mutate(supp_vec = ifelse(obs %in% c(2,3,5,6), \n                           \"support vector\", \"other data\"))\n  \ndf3e$supp_vec <- factor(df3e$supp_vec,\n                        levels = c(\"support vector\", \"other data\"))\n  \ndf3e |>  \n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  # margins\n  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = supp_vec),\n             size = 5) +\n    coord_equal() +\n  scale_color_manual(values = c(\"purple\", \"gray50\")) +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n```\n:::\n\n</details> \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n(f) Argue that a slight movement of the seventh observation would\nnot affect the maximal margin hyperplane.\n\n(g) Sketch a hyperplane that is not the optimal separating hyperplane,\nand provide the equation for this hyperplane.\n\n <details>\n  <summary>code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\ndf3 |>\n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0, y = 1/4, xend = 5, yend = 16/4),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n    coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Separating Hyperplane (not optimal)\",\n       subtitle = \"y = 0.25(3x+1)\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n```\n:::\n\n</details> \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n(h) Draw an additional observation on the plot so that the two\nclasses are no longer separable by a hyperplane.\n\n <details><summary>code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_dot <- data.frame(obs = 8, xvals = 0, yvals = 5, class_label = \"blue\")\ndf3h <- rbind(df3, new_dot)\ndf3h |>\n  ggplot() +\n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n  coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Where to Draw the Separating Hyperplane?\",\n       subtitle = \"new data at (0,5)\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n```\n:::\n\n</details> \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n## Exercises (Applied)\n\nMostly transcibed from [OnMee's solutions](https://onmee.github.io/ISLR-Solutions/).\n\n### Applied Task 4\n\nGenerate a simulated two-class data set with 100 observations and\ntwo features in which there is a visible but non-linear separation between\nthe two classes. Show that in this setting, a support vector\nmachine with a polynomial kernel (with degree greater than 1) or a\nradial kernel will outperform a support vector classifier on the training\ndata. Which technique performs best on the test data? Make\nplots and report training and test error rates in order to back up\nyour assertions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generating a dataset with visible non-linear separation.\nset.seed(3)\nx=matrix(rnorm(100*2), ncol=2)\ny=c(rep(-1,70), rep(1,30))\nx[1:30,]=x[1:30,]+3.3\nx[31:70,]=x[31:70,]-3\ndat=data.frame(x=x, y=as.factor(y))\n# Training and test sets.\nsample.data = sample.split(dat$x.1, SplitRatio = 0.7)\ntrain.set = subset(dat, sample.data==T)\ntest.set = subset(dat, sample.data==F)\nplot(x,col=(2-y), xlab='X1', ylab='X2', main='Dataset with non-linear separation')\n```\n\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n#### Linear Kernel\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Best model.\nset.seed(3)\ntune.out=tune(svm,\n              y ~ .,\n              data = train.set,\n              kernel='linear',\n              ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))\nbestmod=tune.out$best.model\nplot(bestmod, dat)\n```\n\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions on training set.\nypred=predict(bestmod, train.set)\ntable_4_train = table(predict=ypred, truth=train.set$y)\nprint(table_4_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict -1  1\n     -1 50 20\n     1   0  0\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_train), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy: 71.43 percent\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Predictions on the test set.\nypred=predict(bestmod, test.set)\ntable_4_test = table(predict=ypred, truth=test.set$y)\nprint(table_4_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict -1  1\n     -1 20 10\n     1   0  0\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_test), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy: 66.67 percent\"\n```\n:::\n:::\n\n\n#### Radial Kernel\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Best model using cross validaitonon a set of values for cost and gamma.\nset.seed(3)\ntune.out=tune(svm, y~., data=train.set, kernel='radial',\nranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))\nbestmod = tune.out$best.model\nplot(bestmod, train.set)\n```\n\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions on training set.\nypred=predict(bestmod, train.set)\ntable_4_train = table(predict=ypred, truth=train.set$y)\nprint(table_4_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict -1  1\n     -1 50  0\n     1   0 20\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_train), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy: 100 percent\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Predictions on the test set.\nypred=predict(bestmod, test.set)\ntable_4_test = table(predict=ypred, truth=test.set$y)\nprint(table_4_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict -1  1\n     -1 20  0\n     1   0 10\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_test), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy: 100 percent\"\n```\n:::\n:::\n\n\n### Applied Task 5\n\nWe have seen that we can fit an SVM with a non-linear kernel in order\nto perform classification using a non-linear decision boundary.We will\nnow see that we can also obtain a non-linear decision boundary by\nperforming logistic regression using non-linear transformations of the\nfeatures.\n\n(a) Generate a data set with n = 500 and p = 2, such that the observations\nbelong to two classes with a quadratic decision boundary between them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 <- runif (500) - 0.5\nx2 <- runif (500) - 0.5\ny <- 1 * (x1^2 - x2^2 > 0)\ndf <- data.frame(x1=x1, x2=x2, y=as.factor(y))\n```\n:::\n\n\n(b) Plot the observations, colored according to their class labels. Your plot should display X1 on the x-axis, and X2 on the yaxis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x1,x2,col = (2 - y))\n```\n\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n(c) Fit a logistic regression model to the data, using X1 and X2 as predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.fit = glm(y~x1+x2, data=df, family = 'binomial')\n# Predictions\nglm.probs = predict(glm.fit, newdata=df, type = 'response')\nglm.preds = rep(0,500)\nglm.preds[glm.probs>0.50] = 1\ntable_5_c <- table(preds=glm.preds, truth=df$y)\nprint(table_5_c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     truth\npreds   0   1\n    0  24  21\n    1 214 241\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_c), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy: 53 percent\"\n```\n:::\n:::\n\n\n(d) Apply this model to the training data in order to obtain a predicted\nclass label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be linear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot using predicted class labels for observations.\nplot(x1,x2,col=2-glm.preds)\n```\n\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n(e) Now fit a logistic regression model to the data using non-linear functions of X1 and X2 as predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.fit = glm(y~I(x1^2)+I(x2^2), data = df, family = 'binomial')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: algorithm did not converge\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n:::\n\n```{.r .cell-code}\nglm.probs = predict(glm.fit, newdata = df, type = 'response')\nglm.preds = rep(0,500)\nglm.preds[glm.probs>0.5] = 1\ntable_5_e <- table(preds=glm.preds, truth=df$y)\nprint(table_5_e)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     truth\npreds   0   1\n    0 238   0\n    1   0 262\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_e), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy: 100 percent\"\n```\n:::\n:::\n\n\n(f) Apply this model to the training data in order to obtain a predicted\nclass label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be obviously non-linear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x1,x2,col=2-glm.preds)\n```\n\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n(g) Fit a support vector classifier to the data with X1 and X2 as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Best model\ntune.out=tune(svm,y~.,data = df,kernel='linear',\nranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))\nbestmod=tune.out$best.model\n\n#Predictions\nypred=predict(bestmod, newdata=df, type='response')\n\ntable_5_g <- table(predict=ypred, truth=df$y)\nprint(table_5_g)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict   0   1\n      0   0   0\n      1 238 262\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_g), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy: 52.4 percent\"\n```\n:::\n\n```{.r .cell-code}\nplot(x1,x2,col=ypred)\n```\n\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n(h) Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune.out=tune(svm, y~., data=df, kernel='radial',\nranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))\nbestmod=tune.out$best.mode\n\n#Predictions\nypred=predict(bestmod, newdata=df, type='response')\n\ntable_5_h <- table(predict=ypred, truth=df$y)\nprint(table_5_h)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict   0   1\n      0 235   0\n      1   3 262\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_h), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy: 99.4 percent\"\n```\n:::\n\n```{.r .cell-code}\nplot(x1,x2,col=ypred)\n```\n\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n\n### Applied Task 6\n\nAt the end of Section 9.6.1, it is claimed that in the case of data that is just barely linearly separable, a support vector classifier with a small value of `cost` that misclassifies a couple of training observations may perform better on test data than one with a huge value of `cost` that does not misclassify any training observations. You will now investigate this claim.\n\n(a) Generate two-class data with p = 2 in such a way that the classes\nare just barely linearly separable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(111)\nx1 = runif(1000,-5,5)\nx2 = runif(1000,-5,5)\nx = cbind(x1,x2)\ny = rep(NA,1000)\n# Classify points above abline(1.5,1) as 1 and below abline(-1.5,1) as -1, and the rest as 0.\n\n#Removing points classed as 0 will created a more widely separated dataset.\n# Actual decision boundary is a line where y=x, which is abline(0,1).\nfor (i in 1:1000)\nif (x[i,2]-x[i,1] > 1.5) y[i]=1 else if (x[i,2]-x[i,1] < -1.5) y[i]=-1 else y[i]=0\n\n# Combine x an y and remove all rows with y=0.\nx = cbind(x,y)\nx = x[x[,3]!=0,]\n\nplot(x[,1],x[,2],col=2-x[,3], xlab=\"X1\", ylab=\"X2\",xlim = c(-5,5), ylim = c(-5,5))\nabline(0,1, col=\"red\")\nabline(1.5,1)\nabline(-1.5,1)\nabline(h=0,v=0)\n```\n\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Generate random points to be used as noise along line y=1.5x(+-0.1).\nx.noise = matrix(NA,100,3)\nx.noise[,1] = runif(100,-5,5)\n\n#Y coordinate values for first 50 points\nx.noise[1:50,2] = (1.5*x.noise[1:50,1])-0.1 \nx.noise[51:100,2] = (1.5*x.noise[51:100,1])+0.1\nx.noise[,3] = c(rep(-1,50), rep(1,50)) # class values for all noise observations\nplot(x[,1],x[,2],col=2-x[,3], xlab='X1', ylab='X2',\nylim = c(-5,5),xlim = c(-5,5),\nmain=\"Dataset that is linearly separable and with added noise\")\npar(new = TRUE)\nplot(x.noise[,1],x.noise[,2],col=2-x.noise[,3], axes=F,\nxlab=\"\", ylab=\"\", ylim = c(-5,5), xlim = c(-5,5))\n#Noise\nabline(0,1.5,col=\"blue\")\n#Actual decision boundary\nabline(0,1,col=\"red\")\n```\n\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n(b) Compute the cross-validation error rates for support vector classifiers with a range of cost values. How many training errors are misclassified for each value of cost considered, and how does this relate to the cross-validation errors obtained?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = rbind(x,x.noise)\ntrain.dat = data.frame(x1=x[,1],x2=x[,2], y=as.factor(x[,3]))\n\n#Linear SVM models with various values of cost.\ntune.out=tune(svm,y~.,data=train.dat,kernel='linear',\nranges=list(cost=c(0.001,0.01,0.1,1,5,10,100,1000)))\nsummary(tune.out)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n  100\n\n- best performance: 0 \n\n- Detailed performance results:\n   cost      error dispersion\n1 1e-03 0.05351161 0.02002264\n2 1e-02 0.05352630 0.01834727\n3 1e-01 0.05352630 0.01834727\n4 1e+00 0.05352630 0.01834727\n5 5e+00 0.04745813 0.02110627\n6 1e+01 0.04138995 0.02317355\n7 1e+02 0.00000000 0.00000000\n8 1e+03 0.00000000 0.00000000\n```\n:::\n:::\n\n\n(c) Generate an appropriate test data set, and compute the test errors corresponding to each of the values of cost considered. Which value of cost leads to the fewest test errors, and how does this compare to the values of cost that yield the fewest training errors and the fewest cross-validation errors?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# New test set\nset.seed(1221)\ntest.x1 = runif(1000,-5,5)\ntest.x2 = runif(1000,-5,5)\ntest.y = rep(NA,1000)\n\n# Actual decision boundary of the train set is y=x,\n# so points above line are classed as 1 and points below -1\nfor (i in 1:1000){\n  if (test.x1[i]-test.x2[i] < 0) test.y[i]=1 else if (test.x1[i]-test.x2[i] > 0) test.y[i]=-1\n}\n\n# Test dataframe\ntest.dat = data.frame(x1=test.x1,x2=test.x2,y=as.factor(test.y))\n\nplot(test.dat$x1,test.dat$x2,col=2-test.y, xlab=\"X1\", ylab=\"X2\")\n```\n\n::: {.cell-output-display}\n![](ch9_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Performance of model with cost of 0.1 on test set.\nsvmfit = svm(y~., data = train.dat, kernel = 'linear', cost = 0.1)\nypred = predict(svmfit, newdata = test.dat, type = 'response')\ntable_6_1 <- table(predict=ypred, truth=test.dat$y)\nprint(table_6_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict  -1   1\n     -1 483  10\n     1    4 503\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_6_1), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy: 98.6 percent\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Performance of model with cost of 10 on test set.\nsvmfit = svm(y~., data = train.dat, kernel = 'linear', cost = 10)\nypred = predict(svmfit, newdata = test.dat, type = 'response')\ntable_6_2 <- table(predict=ypred, truth=test.dat$y)\nprint(table_6_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict  -1   1\n     -1 446  48\n     1   41 465\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_6_2), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy: 91.1 percent\"\n```\n:::\n:::\n\n\n\n### Applied Task 7\n\nIn this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the `Auto` data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(Auto)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t392 obs. of  9 variables:\n $ mpg         : num  18 15 18 16 17 15 14 14 14 15 ...\n $ cylinders   : num  8 8 8 8 8 8 8 8 8 8 ...\n $ displacement: num  307 350 318 304 302 429 454 440 455 390 ...\n $ horsepower  : num  130 165 150 150 140 198 220 215 225 190 ...\n $ weight      : num  3504 3693 3436 3433 3449 ...\n $ acceleration: num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n $ year        : num  70 70 70 70 70 70 70 70 70 70 ...\n $ origin      : num  1 1 1 1 1 1 1 1 1 1 ...\n $ name        : Factor w/ 304 levels \"amc ambassador brougham\",..: 49 36 231 14 161 141 54 223 241 2 ...\n```\n:::\n:::\n\n\n\n(a) Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(222)\nauto.length = length(Auto$mpg)\nmpg.median = median(Auto$mpg)\nmpg01 = rep(NA,auto.length)\n# Class 1 if car's mpg is above median and 0 if below. Results stored in mpg01 variable.\nfor (i in 1:auto.length) if (Auto$mpg[i] > mpg.median) mpg01[i]=1 else mpg01[i]=0\n# Dataframe\nauto.df = Auto\nauto.df$mpg01 = as.factor(mpg01)\n```\n:::\n\n\n(b) Fit a support vector classifier to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using a linear SVM to predict mpg01.\nlinear.tune=tune(svm,mpg01~.,data=auto.df,kernel='linear',\nranges=list(cost=c(0.001,0.01,0.1,1,5,10,100,1000)))\nsummary(linear.tune)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n    1\n\n- best performance: 0.01275641 \n\n- Detailed performance results:\n   cost      error dispersion\n1 1e-03 0.09955128 0.04760888\n2 1e-02 0.07666667 0.04375200\n3 1e-01 0.04596154 0.02359743\n4 1e+00 0.01275641 0.01808165\n5 5e+00 0.01532051 0.01318724\n6 1e+01 0.01788462 0.01234314\n7 1e+02 0.03057692 0.01606420\n8 1e+03 0.03057692 0.01606420\n```\n:::\n\n```{.r .cell-code}\nlinear.tune$best.parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  cost\n4    1\n```\n:::\n\n```{.r .cell-code}\nlinear.tune$best.performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01275641\n```\n:::\n:::\n\n\n(c) Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of `gamma` and `degree` and `cost`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using a radial SVM to predict mpg01.\nradial.tune=tune(svm,mpg01~.,data=auto.df,kernel='radial',\nranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))\n# summary(radial.tune)\n\nradial.tune$best.parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  cost gamma\n3   10   0.5\n```\n:::\n\n```{.r .cell-code}\nradial.tune$best.performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04820513\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using a polynomial SVM to predict mpg01.\npolynomial.tune=tune(svm,mpg01~.,data=auto.df,kernel='polynomial',\nranges=list(cost=c(0.1,1,10,100,1000), degree=c(1,2,3,4,5)))\n# summary(polynomial.tune)\n\npolynomial.tune$best.parameters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  cost degree\n5 1000      1\n```\n:::\n\n```{.r .cell-code}\npolynomial.tune$best.performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01282051\n```\n:::\n:::\n\n\n\n### Applied Task 8\n\nThis problem involves the `OJ` data set which is part of the `ISLR2` package.\n\n(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(131)\n# Training and test sets.\nsample.data = sample.split(OJ$Purchase, SplitRatio = 800/length(OJ$Purchase))\ntrain.set = subset(OJ, sample.data==T)\ntest.set = subset(OJ, sample.data==F)\n```\n:::\n\n\n(b) Fit a support vector classifier to the training data using `cost = 0.01`, with `Purchase` as the response and the other variables as predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvmfit = svm(Purchase~., data = train.set, kernel = \"linear\", cost=0.01)\nsummary(svmfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsvm(formula = Purchase ~ ., data = train.set, kernel = \"linear\", \n    cost = 0.01)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  0.01 \n\nNumber of Support Vectors:  438\n\n ( 220 218 )\n\n\nNumber of Classes:  2 \n\nLevels: \n CH MM\n```\n:::\n:::\n\n\n(c) What are the training and test error rates?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions on training set\nsvm.pred = predict(svmfit, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict  CH  MM\n     CH 435  76\n     MM  53 236\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Error rate: 16.12 percent\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions on test set\nsvm.pred = predict(svmfit, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict  CH  MM\n     CH 151  36\n     MM  14  69\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Error rate: 18.52 percent\"\n```\n:::\n:::\n\n\n(d) Use the `tune()` function to select an optimal cost.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using cross validation to select optimal cost\nset.seed(131)\ntune.out = tune(svm, Purchase~., data = train.set, kernel = \"linear\",\nranges=list(cost=c(0.01,0.1,0.5,1,10)))\n```\n:::\n\n\n(e) Compute the training and test error rates using this new value for `cost`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions on training set\nsvm.pred = predict(tune.out$best.mod, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict  CH  MM\n     CH 432  72\n     MM  56 240\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Error rate: 16 percent\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions on test set\nsvm.pred = predict(tune.out$best.mod, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict  CH  MM\n     CH 151  34\n     MM  14  71\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Error rate: 17.78 percent\"\n```\n:::\n:::\n\n\n(f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for `gamma`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(131)\ntune.out = tune(svm, Purchase~., data = train.set, kernel = \"radial\",\nranges=list(cost=c(0.01,0.1,0.5,1,10)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions on training set\nsvm.pred = predict(tune.out$best.mod, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict  CH  MM\n     CH 450  81\n     MM  38 231\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Error rate: 14.88 percent\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions on test set\nsvm.pred = predict(tune.out$best.mod, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict  CH  MM\n     CH 150  37\n     MM  15  68\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Error rate: 19.26 percent\"\n```\n:::\n:::\n\n\n(g) Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set `degree = 2`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(131)\ntune.out = tune(svm, Purchase~., data = train.set, kernel = \"polynomial\",\nranges=list(cost=c(0.01,0.1,0.5,1,10)), degree=2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions on training set\nsvm.pred = predict(tune.out$best.mod, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict  CH  MM\n     CH 455  76\n     MM  33 236\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Error rate: 13.62 percent\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions on test set\nsvm.pred = predict(tune.out$best.mod, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       truth\npredict  CH  MM\n     CH 148  41\n     MM  17  64\n```\n:::\n\n```{.r .cell-code}\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Error rate: 21.48 percent\"\n```\n:::\n:::\n",
    "supporting": [
      "ch9_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}