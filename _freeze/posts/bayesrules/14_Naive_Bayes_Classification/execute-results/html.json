{
  "hash": "67a33c1ab249c82156bf6b105483ba9a",
  "result": {
    "markdown": "# Naive Bayes Classification\n\n\n::: {.cell}\n\n:::\n\n\n\n**Learning objectives:**\n\n* explore the pros and cons of naive Bayes classification\n* generalize classification tasks for more than two categories\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"bayesrules\")\nlibrary(\"dplyr\")\nlibrary(\"e1071\")\nlibrary(\"ggplot2\")\nlibrary(\"ggtext\")\nlibrary(\"janitor\")\nlibrary(\"tidyr\")\n\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] tidyr_1.3.0      janitor_2.2.0    ggtext_0.1.2     ggplot2_3.4.2   \n[5] e1071_1.7-13     dplyr_1.1.2      bayesrules_0.0.2\n\nloaded via a namespace (and not attached):\n [1] gridExtra_2.3       inline_0.3.19       rlang_1.1.1        \n [4] magrittr_2.0.3      snakecase_0.11.0    matrixStats_1.0.0  \n [7] compiler_4.3.0      loo_2.6.0           callr_3.7.3        \n[10] vctrs_0.6.2         reshape2_1.4.4      stringr_1.5.0      \n[13] pkgconfig_2.0.3     crayon_1.5.2        fastmap_1.1.1      \n[16] ellipsis_0.3.2      utf8_1.2.3          threejs_0.3.3      \n[19] promises_1.2.0.1    rmarkdown_2.22      markdown_1.7       \n[22] ps_1.7.5            nloptr_2.0.3        purrr_1.0.1        \n[25] xfun_0.39           jsonlite_1.8.4      later_1.3.1        \n[28] parallel_4.3.0      prettyunits_1.1.1   R6_2.5.1           \n[31] dygraphs_1.1.1.6    stringi_1.7.12      StanHeaders_2.26.26\n[34] boot_1.3-28.1       lubridate_1.9.2     Rcpp_1.0.10        \n[37] rstan_2.21.8        knitr_1.43          zoo_1.8-12         \n[40] base64enc_0.1-3     bayesplot_1.10.0    httpuv_1.6.11      \n[43] Matrix_1.5-4        splines_4.3.0       igraph_1.4.3       \n[46] timechange_0.2.0    tidyselect_1.2.0    rstudioapi_0.14    \n[49] codetools_0.2-19    miniUI_0.1.1.1      processx_3.8.1     \n[52] pkgbuild_1.4.0      lattice_0.21-8      tibble_3.2.1       \n[55] plyr_1.8.8          shiny_1.7.4         withr_2.5.0        \n[58] groupdata2_2.0.2    evaluate_0.21       survival_3.5-5     \n[61] proxy_0.4-27        RcppParallel_5.1.7  xts_0.13.1         \n[64] xml2_1.3.4          pillar_1.9.0        DT_0.28            \n[67] stats4_4.3.0        shinyjs_2.1.0       generics_0.1.3     \n[70] rstantools_2.3.1    munsell_0.5.0       scales_1.2.1       \n[73] minqa_1.2.5         gtools_3.9.4        xtable_1.8-4       \n[76] class_7.3-21        glue_1.6.2          tools_4.3.0        \n[79] shinystan_2.6.0     lme4_1.1-33         colourpicker_1.2.0 \n[82] grid_4.3.0          crosstalk_1.2.0     colorspace_2.1-0   \n[85] nlme_3.1-162        cli_3.6.1           fansi_1.0.4        \n[88] gtable_0.3.3        digest_0.6.31       htmlwidgets_1.6.2  \n[91] htmltools_0.5.5     lifecycle_1.0.3     mime_0.12          \n[94] rstanarm_2.21.4     gridtext_0.1.5      shinythemes_1.2.0  \n[97] MASS_7.3-58.4      \n```\n:::\n:::\n\n\n\n## Data: Palmer Penguins\n\nThere exist multiple penguin species throughout Antarctica, including the *Adelie*, *Chinstrap*, and *Gentoo*. When encountering one of these penguins on an Antarctic trip, we might *classify* its species\n\n$$Y = \\begin{cases} A & \\text{Adelie} \\\\ C & \\text{Chinstrap} \\\\ G & \\text{Gentoo} \\end{cases}$$\n\n![three species](images/lter_penguins.png)\n\n$X_{1}$ categorical variable: whether the penguin weighs more than the average 4200 grams\n\n$$X_{1} = \\begin{cases} 1 & \\text{above-average weight} \\\\ 0 & \\text{below-average weight} \\end{cases}$$\n\n![AKA culmen length and depth](images/culmen_depth.png)\n\nnumerical variables:\n\n$$\\begin{array}{rcl}\n  X_{2} & = & \\text{bill length (mm)} \\\\\n  X_{3} & = & \\text{flipper length (mm)} \\\\\n\\end{array}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(penguins_bayes)\npenguins <- penguins_bayes\n\nadelie_color = \"#fb7504\"\nchinstrap_color = \"#c65ccc\"\ngentoo_color = \"#067476\"\n\npenguins %>% \n  tabyl(species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   species   n   percent\n    Adelie 152 0.4418605\n Chinstrap  68 0.1976744\n    Gentoo 124 0.3604651\n```\n:::\n:::\n\n\n\n## Naive Bayes Classification\n\nHere, we have *three* categories, whereas logistic regression is limited to classifying *binary* response variables.  As an alternative, **naive Bayes classification** \n\n* can classify categorical response variables $Y$ with two or more categories\n* doesn’t require much theory beyond Bayes’ Rule\n* it’s computationally efficient, i.e., doesn’t require MCMC simulation\n\nBut why is it called \"naive\"?\n\n\n## One Categorical Predictor\n\nSuppose an Antarctic researcher comes across a penguin that weighs less than 4200g with a 195mm-long flipper and 50mm-long bill. Our goal is to help this researcher identify the species of this penguin: Adelie, Chinstrap, or Gentoo\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14_Naive_Bayes_Classification_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n<details>\n<summary>image code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |>\n  drop_na(above_average_weight) |>\n  ggplot(aes(fill = above_average_weight, x = species)) + \n  geom_bar(position = \"fill\") + \n  labs(title = \"<span style = 'color:#067476'>For which species is a<br>below-average weight most likely?</span>\",\n       subtitle = \"(focus on the <span style = 'color:#c65ccc'>below-average</span> category)\",\n       caption = \"R4DS Book Club\") +\n  scale_fill_manual(values = c(\"#c65ccc\", \"#fb7504\")) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n```\n:::\n\n</details>\n\n### Recall: Bayes Rule\n\n$$f(y|x_{1}) = \\frac{\\text{prior}\\cdot\\text{likelihood}}{\\text{normalizing constant}} = \\frac{f(y) \\cdot L(y|x_{1})}{f(x_{1})}$$\nwhere, by the Law of Total Probability,\n\n$$\\begin{array}{rcl}\nf(x_{1} & = & \\displaystyle\\sum_{\\text{all } y'} f(y')L(y'|x_{1}) \\\\\n~ & = & f(y' = A)L(y' = A|x_{1}) + f(y' = C)L(y' = C|x_{1}) + f(y' = G)L(y' = G|x_{1}) \\\\\n\\end{array}$$\n\nover our three penguin species.\n\n### Calculation\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins %>% \n  select(species, above_average_weight) %>% \n  na.omit() %>% \n  tabyl(species, above_average_weight) %>% \n  adorn_totals(c(\"row\", \"col\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   species   0   1 Total\n    Adelie 126  25   151\n Chinstrap  61   7    68\n    Gentoo   6 117   123\n     Total 193 149   342\n```\n:::\n:::\n\n\nPrior probabilities:\n\n$$f(y = A) = \\frac{151}{342}, \\quad f(y = C) = \\frac{68}{342}, \\quad f(y = G) = \\frac{123}{342}$$\n\nLikelihoods:\n\n$$\\begin{array}{rcccl}\n  L(y = A | x_{1} = 0) & = & \\frac{126}{151} & \\approx & 0.8344 \\\\\n  L(y = C | x_{1} = 0) & = & \\frac{61}{68} & \\approx & 0.8971 \\\\\n  L(y = G | x_{1} = 0) & = & \\frac{6}{123} & \\approx & 0.0488 \\\\\n\\end{array}$$\n\nTotal probability:\n\n$$f(x_{1} = 0) = \\frac{151}{342}\\cdot\\frac{126}{151} + \\frac{68}{342}\\cdot\\frac{61}{68} + \\frac{123}{342}\\cdot\\frac{6}{123} = \\frac{193}{342}$$\n\nBayes' Rules:\n\n$$\\begin{array}{rcccccl}\n  f(y = A | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = A | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{151}{342}\\cdot\\frac{126}{151}}{\\frac{193}{342}} & \\approx & 0.6528 \\\\\n  f(y = C | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = C | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{68}{342}\\cdot\\frac{61}{68}}{\\frac{193}{342}} & \\approx & 0.3161 \\\\\n  f(y = G | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = G | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{123}{342}\\cdot\\frac{6}{123}}{\\frac{193}{342}} & \\approx & 0.0311 \\\\\n\\end{array}$$\n\nThe posterior probability that this penguin is an Adelie is more than double that of the other two species\n\n\n## One Numerical Predictor\n\nLet’s ignore the penguin’s weight for now and classify its species using only the fact that it has a 50mm-long bill\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14_Naive_Bayes_Classification_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n<details>\n<summary>image code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins|>\n  ggplot(aes(x = bill_length_mm, fill = species)) + \n  geom_density(alpha = 0.7) + \n  geom_vline(xintercept = 50, linetype = \"dashed\", linewidth = 3) + \n  labs(title = \"<span style = 'color:#c65ccc'>For which species is a<br>50mm-long bill the most common?</span>\",\n       subtitle = \"one numerical predictor\",\n       caption = \"R4DS Book Club\") +\n  scale_fill_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n```\n:::\n\n</details>\n\nOur data points to our penguin being a Chinstrap\n\n* we must weigh this data against the fact that Chinstraps are the rarest of these three species\n* difficult to compute likelihood $L(y = A | x_{2} = 50)$\n\nThis is where one “*naive*” part of naive Bayes classification comes into play. The naive Bayes method typically assumes that any quantitative predictor, here $X_{2}$, is **continuous** and **conditionally normal**:\n\n$$\\begin{array}{rcl}\n  X_{2} | (Y = A) & \\sim & N(\\mu_{A}, \\sigma_{A}^{2}) \\\\\n  X_{2} | (Y = C) & \\sim & N(\\mu_{C}, \\sigma_{C}^{2}) \\\\\n  X_{2} | (Y = G) & \\sim & N(\\mu_{G}, \\sigma_{G}^{2}) \\\\\n\\end{array}$$\n\n### Prior Probability Distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate sample mean and sd for each Y group\npenguins %>% \n  group_by(species) %>% \n  summarize(mean = mean(bill_length_mm, na.rm = TRUE), \n            sd = sd(bill_length_mm, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  species    mean    sd\n  <fct>     <dbl> <dbl>\n1 Adelie     38.8  2.66\n2 Chinstrap  48.8  3.34\n3 Gentoo     47.5  3.08\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |>\n  ggplot(aes(x = bill_length_mm, color = species)) + \n  stat_function(fun = dnorm, args = list(mean = 38.8, sd = 2.66), \n                aes(color = \"Adelie\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 48.8, sd = 3.34),\n                aes(color = \"Chinstrap\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 47.5, sd = 3.08),\n                aes(color = \"Gentoo\"), linewidth = 3) +\n  ...\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](14_Naive_Bayes_Classification_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n<details>\n<summary>image code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |>\n  ggplot(aes(x = bill_length_mm, color = species)) + \n  stat_function(fun = dnorm, args = list(mean = 38.8, sd = 2.66), \n                aes(color = \"Adelie\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 48.8, sd = 3.34),\n                aes(color = \"Chinstrap\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 47.5, sd = 3.08),\n                aes(color = \"Gentoo\"), linewidth = 3) + \n  geom_vline(xintercept = 50, linetype = \"dashed\") + \n  labs(title = \"<span style = 'color:#c65ccc'>Prior Probabilities</span>\",\n       subtitle = \"conditionally normal\",\n       caption = \"R4DS Book Club\") +\n  scale_color_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n```\n:::\n\n\n</details>\n\nComputing the likelihoods in `R`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# L(y = A | x_2 = 50) = 2.12e-05\ndnorm(50, mean = 38.8, sd = 2.66)\n\n# L(y = C | x_2 = 50) = 0.112\ndnorm(50, mean = 48.8, sd = 3.34)\n\n# L(y = G | x_2 = 50) = 0.09317\ndnorm(50, mean = 47.5, sd = 3.08)\n```\n:::\n\n\nTotal probability:\n\n$$f(x_{2} = 50) = \\frac{151}{342} \\cdot 0.0000212 + \\frac{68}{342} \\cdot 0.112 + \\frac{123}{342} \\cdot 0.09317 \\approx 0.05579$$\n\nBayes' Rules:\n\n$$\\begin{array}{rcccccl}\n  f(y = A | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = A | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{151}{342} \\cdot 0.0000212}{0.05579} & \\approx & 0.0002 \\\\\n  f(y = C | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = C | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{68}{342} \\cdot 0.112}{0.05579} & \\approx & 0.3992 \\\\\n  f(y = G | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = G | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{123}{342} \\cdot 0.09317}{0.05579} & \\approx & 0.6006 \\\\\n\\end{array}$$\n\nThough a 50mm-long bill is relatively less common among <span style = 'color:#067476'>Gentoo</span> than among <span style = 'color:#c65ccc'>Chinstrap</span>, it follows that our naive Bayes classification, based on our prior information and penguin’s bill length alone, is that this penguin is a <span style = 'color:#067476'>Gentoo</span> – it has the highest posterior probability.\n\nWe’ve now made two naive Bayes classifications of our penguin’s species, one based solely on the fact that our penguin has below-average weight and the other based solely on its 50mm-long bill (in addition to our prior information). And these classifications **disagree**: we classified the penguin as Adelie in the former analysis and Gentoo in the latter. This discrepancy indicates that there’s *room for improvement* in our naive Bayes classification method.\n\n\n## Two Predictor Variables\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14_Naive_Bayes_Classification_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n<details>\n<summary>image code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |>\nggplot(aes(x = flipper_length_mm, y = bill_length_mm, \n           color = species)) + \n  geom_point(size = 3) + \n  geom_segment(aes(x = 195, y = 30, xend = 195, yend = 50),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 170, y = 50, xend = 195, yend = 50),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  labs(title = \"<span style = 'color:#c65ccc'>Two Predictor Variables</span>\",\n       subtitle = \"50mm-long bill and 195mm-long flipper\",\n       caption = \"R4DS Book Club\") +\n  scale_color_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n```\n:::\n\n</details>\n\nGeneralizing Bayes' Rule:\n\n$$f(y | x_{2}, x_{3}) = \\frac{f(y) \\cdot L(y | x_{2}, x_{3})}{\\sum_{y'} f(y') \\cdot L(y' | x_{2}, x_{3})}$$\n\nAnother \"naive\" assumption of **conditionally independent**:\n\n$$L(y | x_{2}, x_{3}) = f(x_{2}, x_{3} | y) = f(x_{2} | y) \\cdot f(x_{3} | y)$$\n\n* mathematically efficient\n* but what about correlation?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample statistics of x_3: flipper length\npenguins %>% \n  group_by(species) %>% \n  summarize(mean = mean(flipper_length_mm, na.rm = TRUE), \n            sd = sd(flipper_length_mm, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  species    mean    sd\n  <fct>     <dbl> <dbl>\n1 Adelie     190.  6.54\n2 Chinstrap  196.  7.13\n3 Gentoo     217.  6.48\n```\n:::\n:::\n\n\nLikelihoods of a flipper length of 195 mm:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# L(y = A | x_3 = 195) = 0.04554\ndnorm(195, mean = 190, sd = 6.54)\n\n# L(y = C | x_3 = 195) = 0.05541\ndnorm(195, mean = 196, sd = 7.13)\n\n# L(y = G | x_3 = 195) = 0.0001934\ndnorm(195, mean = 217, sd = 6.48)\n```\n:::\n\n\nTotal probability:\n\n$$f(x_{2} = 50, x_{3} = 195) = \\frac{151}{342} \\cdot 0.0000212 \\cdot 0.04554 + \\frac{68}{342} \\cdot 0.112 \\cdot 0.05541 + \\frac{123}{342} \\cdot 0.09317 \\cdot 0.0001931 \\approx 0.001241$$\n\nBayes' Rules:\n\n$$\\begin{array}{rcccl}\n  f(y = A | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{151}{342} \\cdot 0.0000212 \\cdot 0.04554}{0.0001931} & \\approx & 0.0003 \\\\\n  f(y = C | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{68}{342} \\cdot 0.112 \\cdot 0.05541}{0.0001931} & \\approx & 0.9944 \\\\\n  f(y = G | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{123}{342} \\cdot 0.09317 \\cdot 0.0001931}{0.0001931} & \\approx & 0.0052 \\\\\n\\end{array}$$\n\nIn conclusion, our penguin is *almost certainly* a <span style = 'color:#c65ccc'>Chinstrap</span>.\n\n\n## Implementation\n\nTo implement naive Bayes classification in `R`, we’ll use the `naiveBayes()` function in the `e1071` package (Meyer et al. 2021)\n\n### Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# two models\nnaive_model_1 <- naiveBayes(species ~ bill_length_mm, data = penguins)\nnaive_model_2 <- naiveBayes(species ~ bill_length_mm + flipper_length_mm, \n                            data = penguins)\n\n# our penguin to classify\nour_penguin <- data.frame(bill_length_mm = 50, flipper_length_mm = 195)\n```\n:::\n\n\n### Predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(naive_model_1, newdata = our_penguin, type = \"raw\") |>\nround(6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Adelie Chinstrap Gentoo\n[1,] 0.000169  0.397831  0.602\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(naive_model_1, newdata = our_penguin)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] Gentoo\nLevels: Adelie Chinstrap Gentoo\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(naive_model_2, newdata = our_penguin, type = \"raw\") |>\nround(6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Adelie Chinstrap   Gentoo\n[1,] 0.000345  0.994868 0.004787\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(naive_model_2, newdata = our_penguin)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] Chinstrap\nLevels: Adelie Chinstrap Gentoo\n```\n:::\n:::\n\n\n## Validation\n\n### Confusion Matrices\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins <- penguins %>% \n  mutate(class_1 = predict(naive_model_1, newdata = .),\n         class_2 = predict(naive_model_2, newdata = .))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(84735)\npenguins %>% \n  sample_n(4) %>% \n  select(bill_length_mm, flipper_length_mm, species, class_1, class_2) %>% \n  rename(bill = bill_length_mm, flipper = flipper_length_mm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 5\n   bill flipper species   class_1 class_2  \n  <dbl>   <int> <fct>     <fct>   <fct>    \n1  47.5     199 Chinstrap Gentoo  Chinstrap\n2  40.9     214 Gentoo    Adelie  Gentoo   \n3  41.3     194 Adelie    Adelie  Adelie   \n4  38.5     190 Adelie    Adelie  Adelie   \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Confusion matrix for naive_model_1\npenguins %>% \n  tabyl(species, class_1) %>% \n  adorn_percentages(\"row\") %>% \n  adorn_pct_formatting(digits = 2) %>%\n  adorn_ns()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   species       Adelie Chinstrap       Gentoo\n    Adelie 95.39% (145) 0.00% (0)  4.61%   (7)\n Chinstrap  5.88%   (4) 8.82% (6) 85.29%  (58)\n    Gentoo  6.45%   (8) 4.84% (6) 88.71% (110)\n```\n:::\n:::\n\n\n* accuracy: 76 percent\n* 85 percent of Chinstap penguins are misclassified as Gentoo!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Confusion matrix for naive_model_2\npenguins %>% \n  tabyl(species, class_2) %>% \n  adorn_percentages(\"row\") %>% \n  adorn_pct_formatting(digits = 2) %>%\n  adorn_ns()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   species       Adelie   Chinstrap       Gentoo\n    Adelie 96.05% (146)  2.63%  (4)  1.32%   (2)\n Chinstrap  7.35%   (5) 86.76% (59)  5.88%   (4)\n    Gentoo  0.81%   (1)  0.81%  (1) 98.39% (122)\n```\n:::\n:::\n\n\n* accuracy: 95 percent\n\n### Cross-Validation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 10-fold cross-validation\nset.seed(84735)\ncv_model_2 <- naive_classification_summary_cv(\n  model = naive_model_2, data = penguins, y = \"species\", k = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_model_2$cv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   species       Adelie   Chinstrap       Gentoo\n    Adelie 96.05% (146)  2.63%  (4)  1.32%   (2)\n Chinstrap  7.35%   (5) 86.76% (59)  5.88%   (4)\n    Gentoo  0.81%   (1)  0.81%  (1) 98.39% (122)\n```\n:::\n:::\n\n\n## Summary\n\nNaive Bayes\n\n$$f(y | x_{1}, x_{2}, ..., x_{p}) = \\frac{f(y) \\cdot L(y | x_{1}, x_{2}, ..., x_{p})}{\\sum_{y'} f(y') \\cdot L(y' | x_{1}, x_{2}, ..., x_{p})}$$\n\n* conditionally independent $\\rightarrow$ computationally efficient\n* generalizes to more than two categories\n* assumptions violated commonly in practice\n\nLogistic Regression\n\n$$\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{p}$$\n\n* binary classification\n* coefficients $\\rightarrow$ illumination of the relationships among these variables\n\n\n\n\n\n\n## Meeting Videos\n\n### Cohort 1\n\n<iframe src=\"https://www.youtube.com/embed/HeCaORJNdJM\" width=\"100%\" height=\"400px\" data-external=\"1\"></iframe>\n\n<details>\n<summary> Meeting chat log </summary>\n\n```\n00:44:10\tdefuneste:\tplant@net\n00:44:28\tLisa:\thttps://identify.plantnet.org/\n00:48:49\tBrendan Lam:\tThanks everyone!\n```\n</details>\n\n### Cohort 2\n\n<iframe src=\"https://www.youtube.com/embed/6Yma6P5h0IA\" width=\"100%\" height=\"400px\" data-external=\"1\"></iframe>\n\n\n### Cohort 4\n\n<iframe src=\"https://www.youtube.com/embed/URL\" width=\"100%\" height=\"400px\" data-external=\"1\"></iframe>\n\n<details>\n<summary> Meeting chat log </summary>\n\n```\nLOG\n```\n</details>\n",
    "supporting": [
      "14_Naive_Bayes_Classification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}