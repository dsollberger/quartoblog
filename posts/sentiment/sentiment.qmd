---
title: "Sentiment Analysis"
author: "Derek Sollberger"
date: "2024-06-24"
format: html
---

I want to perform some sentiment analysis for a personal project. In the past few years, there have been a few code packages in the R universe that can perhaps perform the sentiment analyses.

For the workflow, my draft thoughts are

(1) Download a backup file of my journal (on Penzu)
(2) Convert the PDF file into a text file
(3) Extract the journal entries---separated by dates
(4) Use a code package to perform sentiment analysis

```{r}
#| message: false
#| warning: false
library("pdftools")
library("tidyverse")
```


## Reprex

The first two items on the list (along with the forth) can be performed with third-party software.  Currently my bottleneck is with the third step about extracting the journal entries from one large text file.  I want to ask people for help with this, so I will produce a `reprex` (**repr**oducible **ex**ample) for posting on the DLSC Slack workspace (`Data Science Learning Community`). First, I need some fake data.

```{r}
raw_data <- "
    6/22/2024
    
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
    
    6/23/2024
    
    Risus viverra adipiscing at in tellus integer feugiat.
    
    6/24/2024
    
    Praesent semper feugiat nibh sed pulvinar proin gravida hendrerit lectus.
"
```

Next, I envision the data tibble that I want in the end of this step.

```{r}
journal_tibble <- tibble::tibble(
  dates <- c("6/22/2024", "6/23/2024", "6/24/2024"),
  texts <- c("Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.", "Risus viverra adipiscing at in tellus integer feugiat.", "Praesent semper feugiat nibh sed pulvinar proin gravida hendrerit lectus."))

colnames(journal_tibble) <- c("dates", "texts")

# print
journal_tibble
```

## Loading the Data

I write my journal in Penzu, and a user can get a copy of their archive as a PDF file. We can then load the file and extract the text (following this [blog post](https://finnstats.com/2021/06/15/extract-text-from-pdf-in-r-and-word-detection/))

```{r}
raw_data <- pdftools::pdf_text("Penzu_Export_-_20210414.pdf")
```

## Regular Expressions

Thanks to the help from the (DSLC) Data Science Learning Community, we could then wrangle the `raw_data` into an easy-to-use tibble.

I needed to distinguish between journal entry dates and any other dates found in the text. Each journal entry included "by Derek Sollberger" in its start, so I made that a part of the search pattern.

```{r}
#| eval: false
date_pattern <- "\\d+/\\d+/\\d+ by Derek Sollberger"

dates <- str_extract_all(raw_data, date_pattern) %>% unlist()

texts <- str_split(raw_data, pattern = date_pattern) %>%
  unlist() %>%
  str_trim() %>%
  discard(~ .x == "")

if(!length(dates) == length(texts)) stop("Dimensions don't match")

# tibble(date = dates, text = texts)
```

User `Jack` at the DSLC wisely put in that dimension check in their response.  At the moment, `texts` has more than twice as many elements than `dates`, and `dates` is working as desired.

Using commands like `head(texts)`, I realized that the string split was nicely separating the titles and contents of my journal entries, and I probably should keep that structure somehow.  Moreover, I realized that the string split was also taking place at the page breaks in the PDF document.  However, I don't want splits there that happened arbitrarily for longer journal entries.

```{r}
#| eval: false
texts <- raw_data |>
  paste0(collapse = "'") |>
  str_split(pattern = date_pattern) |>
  unlist()
```

## Data Frame

I try to use the tidyverse to solve my data wrangling tasks.  My `raw_data` set here is not too large, so I will turn it into a data frame and attach columns.

User Anthony Durrant over at the DSLC gave great advice about using `fill` and `group_by` (which will hopefully bring together the multi-page journal entries).

I probably want to also grab the day of the week (Mon., Tue., etc.) in the regular expression to make later steps easier.

```{r}
date_pattern <- ".{5}\\d+/\\d+/\\d+ by Derek Sollberger"

journal_df <- data.frame(raw_data) |>
  
  # remove empty pages
  mutate(n_char = nchar(raw_data)) |>
  filter(n_char > 0) |>
  
  # find out when new entries started in file
  mutate(new_entry = str_detect(raw_data, date_pattern),
         time_stamp = str_extract(raw_data, date_pattern)) |>
   
  # find out when new entries started in calendar
  mutate(entry_date = str_extract(time_stamp, pattern = "\\d+/\\d+/\\d+")) |>
  fill(entry_date) |>
  
  # extract entry title
  mutate(entry_title = if_else(new_entry,
                               str_sub(raw_data,
                                       1, str_locate(raw_data,
                                                     date_pattern)[,1] - 2),
                               "")) |>

  # extract entry contents
      # if new time stamp: grab contents after time stamp
  mutate(page_contents = if_else(new_entry,
                               str_sub(raw_data,
                                       str_locate(raw_data,
                                                  date_pattern)[,2] + 2,
                                       n_char),
                               raw_data)) |>
  
  # combine multiple-page entries
  group_by(entry_date) |>
  mutate(entry_contents = paste0(page_contents, collapse = " ")) |>
  ungroup() |>
  
  # retain only metadata
  select(entry_date, entry_contents) |>
  distinct() |>
  mutate(word_count = str_count(entry_contents, " ") + 1) |>
  select(entry_date, word_count)
```

