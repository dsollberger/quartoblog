{
  "hash": "1a1f64ef80c3efde52b2da68339fb1ce",
  "result": {
    "engine": "knitr",
    "markdown": "# 9 Linear Discriminant Analysis\n\n**Learning objectives:**\n\n- Purpose of classifiers\n- What are generative classifiers\n- Gaussian Discriminant Analysis lead to curved decision boundaries\n- Linear Discriminant Analysis and linear decision boundaries\n- ScikitLearn approaches for Linear / Gaussian Discriminant Analysis\n- Explore Naive Bayes classification\n- Discuss Fisher's linear discriminant analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"bayesrules\")\nlibrary(\"dplyr\")\nlibrary(\"e1071\")\nlibrary(\"ggplot2\")\nlibrary(\"ggtext\")\nlibrary(\"gt\")\nlibrary(\"janitor\")\nlibrary(\"magrittr\") #need for \".\" in pipe acts\n# library(\"MASS\") #carefully use lda() later to not overwrite select()\nlibrary(\"patchwork\")\nlibrary(\"tidyr\")\n\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.3.2 (2023-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidyr_1.3.1      patchwork_1.1.2  magrittr_2.0.3   janitor_2.2.0   \n [5] gt_0.9.0         ggtext_0.1.2     ggplot2_3.4.3    e1071_1.7-13    \n [9] dplyr_1.1.4      bayesrules_0.0.2\n\nloaded via a namespace (and not attached):\n [1] gridExtra_2.3      inline_0.3.19      rlang_1.1.1        snakecase_0.11.0  \n [5] matrixStats_1.0.0  compiler_4.3.2     loo_2.6.0          callr_3.7.3       \n [9] vctrs_0.6.5        reshape2_1.4.4     stringr_1.5.1      pkgconfig_2.0.3   \n[13] crayon_1.5.2       fastmap_1.1.1      ellipsis_0.3.2     utf8_1.2.4        \n[17] threejs_0.3.3      promises_1.2.1     rmarkdown_2.24     markdown_1.8      \n[21] ps_1.7.5           nloptr_2.0.3       purrr_1.0.2        xfun_0.40         \n[25] jsonlite_1.8.7     later_1.3.1        parallel_4.3.2     prettyunits_1.2.0 \n[29] R6_2.5.1           dygraphs_1.1.1.6   stringi_1.8.3      StanHeaders_2.32.5\n[33] boot_1.3-28.1      lubridate_1.9.3    Rcpp_1.0.11        rstan_2.32.5      \n[37] knitr_1.43         zoo_1.8-12         base64enc_0.1-3    bayesplot_1.10.0  \n[41] httpuv_1.6.11      Matrix_1.6-1.1     splines_4.3.2      igraph_1.4.3      \n[45] timechange_0.3.0   tidyselect_1.2.0   rstudioapi_0.15.0  codetools_0.2-19  \n[49] miniUI_0.1.1.1     curl_5.0.2         processx_3.8.1     pkgbuild_1.4.0    \n[53] lattice_0.21-9     tibble_3.2.1       plyr_1.8.8         shiny_1.7.5       \n[57] withr_3.0.0        groupdata2_2.0.2   evaluate_0.21      survival_3.5-7    \n[61] proxy_0.4-27       RcppParallel_5.1.7 xml2_1.3.6         xts_0.13.1        \n[65] pillar_1.9.0       DT_0.28            stats4_4.3.2       shinyjs_2.1.0     \n[69] generics_0.1.3     rstantools_2.3.1   munsell_0.5.0      scales_1.2.1      \n[73] minqa_1.2.5        gtools_3.9.4       xtable_1.8-4       class_7.3-22      \n[77] glue_1.6.2         tools_4.3.2        shinystan_2.6.0    lme4_1.1-33       \n[81] colourpicker_1.2.0 grid_4.3.2         QuickJSR_1.1.3     crosstalk_1.2.0   \n[85] colorspace_2.1-0   nlme_3.1-163       cli_3.6.1          fansi_1.0.6       \n[89] V8_4.3.0           gtable_0.3.4       digest_0.6.33      htmlwidgets_1.6.2 \n[93] htmltools_0.5.6    lifecycle_1.0.4    mime_0.12          rstanarm_2.21.4   \n[97] gridtext_0.1.5     shinythemes_1.2.0  MASS_7.3-60       \n```\n\n\n:::\n:::\n\n\n## Tooling\n\nScikitlearn:\n\n- [API](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.discriminant_analysis)\n- and [chapter](https://scikit-learn.org/stable/modules/lda_qda.html) \n\n... on Linear / Gaussian Discriminant Analysis\n\n## Classification Models {-}\n\n$p(y=c|x,\\theta) = \\frac{p(x|y=c,\\theta)p(y=c|\\theta)}{\\Sigma_{c'}{p(x|y=c',\\theta)p(y=c'|\\theta)}}$\n\nNote:\n\n- Prior $p(y=c|\\theta) = \\pi_c$\n- Class conditional density $p(x|y=c,\\theta)$\n- Generative classifier\n\nDiscriminative classifiers directly model $p(y|x,\\theta)$\n\n## Linear discriminant analysis\n\nWe want the posterior over classes to be a linear function of $x$\n\n$\\log{p(y=c|x,\\theta)}=w^Tx+const$\n\n## Gaussian discriminant analysis\n\n$p(x|y = c, θ) = N(x|µ_c , Σ_c )$\n\nHence,\n\n$p(y = c|x, θ) ∝ π_c N (x|µ_c , Σ_c )$\n\n## Gaussian discriminants -> Quadratic decision boundaries\n\nLog posterior is \"The discriminant function\"\n\n$\\log p(y = c|x, θ) = \\log π_c − (1/2)\\log|2πΣ_c | − (x − µ_c )^T Σ^{−1}_c (x − µ_c ) + const$\n\nLet $p(y=c|x,\\theta)=p(y=c'|x,\\theta)$\n\nThen\n\n$(x − µ_c )^TΣ^{−1}_c(x − µ_c) - (x − µ_{c'} )^T Σ^{−1}_{c'} (x − µ_{c'})=f(\\pi_c,\\pi_c',\\Sigma_c,\\Sigma_c')$\n\nSo the decision boundaries between classes are quadratic in $x$.\n\nGOTO [workbook](https://github.com/probml/pyprobml/blob/master/notebooks/book1/09/discrim_analysis_dboundaries_plot2.ipynb)\n\n## Tied covariance matrices -> Linear decision boundaries\n\n$\\log p(y = c|x, θ) = \\log π_c − (x − µ_c )^T Σ^{−1} (x − µ_c ) + const$\n\n$= \\log π_c − µ^T_c Σ^{−1} µ_c +x^T Σ^−1 µ_c +const − x^T Σ^{−1} x$\n\n$=\\gamma_c + x^T\\beta_c + \\kappa$\n\nSo the decision boundaries occur when\n\n$\\gamma_c + x^T\\beta_c + \\kappa = \\gamma_{c'} + x^T\\beta_{c'} + \\kappa$\n\n$x^T(\\beta_c - \\beta_{c'}) = \\gamma_{c'} - \\gamma_{c}$\n\nGOTO [workbook](https://github.com/probml/pyprobml/blob/master/notebooks/book1/09/discrim_analysis_dboundaries_plot2.ipynb)\n\n## LDA & Logistic regression\n\nGOTO textbook\n\n\n## Naive Bayes Example\n\n### Data: Palmer Penguins\n\nThere exist multiple penguin species throughout Antarctica, including the *Adelie*, *Chinstrap*, and *Gentoo*. When encountering one of these penguins on an Antarctic trip, we might *classify* its species\n\n$$Y = \\begin{cases} A & \\text{Adelie} \\\\ C & \\text{Chinstrap} \\\\ G & \\text{Gentoo} \\end{cases}$$\n\n![three species](images/lter_penguins.png)\t\n\nExample comes from chapter 14 of *Bayes Rules!*\n\n![Bayes Rules! textbook](images/bayes_rules_textbook.png)\t\n\n\n\n\n$X_{1}$ categorical variable: whether the penguin weighs more than the average 4200 grams\n\n$$X_{1} = \\begin{cases} 1 & \\text{above-average weight} \\\\ 0 & \\text{below-average weight} \\end{cases}$$\n\n![AKA culmen length and depth](images/culmen_depth.png)\n\nNumerical variables:\n\n$$\\begin{array}{rcl}\n  X_{2} & = & \\text{bill length (mm)} \\\\\n  X_{3} & = & \\text{flipper length (mm)} \\\\\n\\end{array}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(penguins_bayes)\npenguins <- penguins_bayes\n\nadelie_color = \"#fb7504\"\nchinstrap_color = \"#c65ccc\"\ngentoo_color = \"#067476\"\n\npenguins |>\n  tabyl(species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   species   n   percent\n    Adelie 152 0.4418605\n Chinstrap  68 0.1976744\n    Gentoo 124 0.3604651\n```\n\n\n:::\n:::\n\n\n### Motivation\n\nHere, we have *three* categories, whereas logistic regression is limited to classifying *binary* response variables.  As an alternative, **naive Bayes classification** \n\n* can classify categorical response variables $Y$ with two or more categories\n* doesn’t require much theory beyond Bayes’ Rule\n* it’s computationally efficient, i.e., doesn’t require MCMC simulation\n\nBut why is it called \"naive\"?\n\n### One Categorical Predictor\n\nSuppose an Antarctic researcher comes across a penguin that weighs less than 4200g with a 195mm-long flipper and 50mm-long bill. Our goal is to help this researcher identify the species of this penguin: Adelie, Chinstrap, or Gentoo\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_linear-discriminant-analysis_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n<details>\n<summary>image code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |>\n  drop_na(above_average_weight) |>\n  ggplot(aes(fill = above_average_weight, x = species)) + \n  geom_bar(position = \"fill\") + \n  labs(title = \"<span style = 'color:#067476'>For which species is a<br>below-average weight most likely?</span>\",\n       subtitle = \"(focus on the <span style = 'color:#c65ccc'>below-average</span> category)\",\n       caption = \"R4DS Book Club\") +\n  scale_fill_manual(values = c(\"#c65ccc\", \"#fb7504\")) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n```\n:::\n\n</details>\n\n### Recall: Bayes Rule\n\n$$f(y|x_{1}) = \\frac{\\text{prior}\\cdot\\text{likelihood}}{\\text{normalizing constant}} = \\frac{f(y) \\cdot L(y|x_{1})}{f(x_{1})}$$\nwhere, by the Law of Total Probability,\n\n$$\\begin{array}{rcl}\nf(x_{1} & = & \\displaystyle\\sum_{\\text{all } y'} f(y')L(y'|x_{1}) \\\\\n~ & = & f(y' = A)L(y' = A|x_{1}) + f(y' = C)L(y' = C|x_{1}) + f(y' = G)L(y' = G|x_{1}) \\\\\n\\end{array}$$\n\nover our three penguin species.\n\n### Calculation\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |> \n  select(species, above_average_weight) |> \n  na.omit() |> \n  tabyl(species, above_average_weight) |> \n  adorn_totals(c(\"row\", \"col\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   species   0   1 Total\n    Adelie 126  25   151\n Chinstrap  61   7    68\n    Gentoo   6 117   123\n     Total 193 149   342\n```\n\n\n:::\n:::\n\n\nPrior probabilities:\n\n$$f(y = A) = \\frac{151}{342}, \\quad f(y = C) = \\frac{68}{342}, \\quad f(y = G) = \\frac{123}{342}$$\n\nLikelihoods:\n\n$$\\begin{array}{rcccl}\n  L(y = A | x_{1} = 0) & = & \\frac{126}{151} & \\approx & 0.8344 \\\\\n  L(y = C | x_{1} = 0) & = & \\frac{61}{68} & \\approx & 0.8971 \\\\\n  L(y = G | x_{1} = 0) & = & \\frac{6}{123} & \\approx & 0.0488 \\\\\n\\end{array}$$\n\nTotal probability:\n\n$$f(x_{1} = 0) = \\frac{151}{342}\\cdot\\frac{126}{151} + \\frac{68}{342}\\cdot\\frac{61}{68} + \\frac{123}{342}\\cdot\\frac{6}{123} = \\frac{193}{342}$$\n\nBayes' Rules:\n\n$$\\begin{array}{rcccccl}\n  f(y = A | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = A | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{151}{342}\\cdot\\frac{126}{151}}{\\frac{193}{342}} & \\approx & 0.6528 \\\\\n  f(y = C | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = C | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{68}{342}\\cdot\\frac{61}{68}}{\\frac{193}{342}} & \\approx & 0.3161 \\\\\n  f(y = G | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = G | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{123}{342}\\cdot\\frac{6}{123}}{\\frac{193}{342}} & \\approx & 0.0311 \\\\\n\\end{array}$$\n\nThe posterior probability that this penguin is an Adelie is more than double that of the other two species\n\n\n### One Numerical Predictor\n\nLet’s ignore the penguin’s weight for now and classify its species using only the fact that it has a 50mm-long bill\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_linear-discriminant-analysis_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n<details>\n<summary>image code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins|>\n  ggplot(aes(x = bill_length_mm, fill = species)) + \n  geom_density(alpha = 0.7) + \n  geom_vline(xintercept = 50, linetype = \"dashed\", linewidth = 3) + \n  labs(title = \"<span style = 'color:#c65ccc'>For which species is a<br>50mm-long bill the most common?</span>\",\n       subtitle = \"one numerical predictor\",\n       caption = \"R4DS Book Club\") +\n  scale_fill_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n```\n:::\n\n</details>\n\nOur data points to our penguin being a Chinstrap\n\n* we must weigh this data against the fact that Chinstraps are the rarest of these three species\n* difficult to compute likelihood $L(y = A | x_{2} = 50)$\n\nThis is where one “*naive*” part of naive Bayes classification comes into play. The naive Bayes method typically assumes that any quantitative predictor, here $X_{2}$, is **continuous** and **conditionally normal**:\n\n$$\\begin{array}{rcl}\n  X_{2} | (Y = A) & \\sim & N(\\mu_{A}, \\sigma_{A}^{2}) \\\\\n  X_{2} | (Y = C) & \\sim & N(\\mu_{C}, \\sigma_{C}^{2}) \\\\\n  X_{2} | (Y = G) & \\sim & N(\\mu_{G}, \\sigma_{G}^{2}) \\\\\n\\end{array}$$\n\n### Prior Probability Distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate sample mean and sd for each Y group\npenguins |> \n  group_by(species) |> \n  summarize(mean = mean(bill_length_mm, na.rm = TRUE), \n            sd = sd(bill_length_mm, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  species    mean    sd\n  <fct>     <dbl> <dbl>\n1 Adelie     38.8  2.66\n2 Chinstrap  48.8  3.34\n3 Gentoo     47.5  3.08\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |>\n  ggplot(aes(x = bill_length_mm, color = species)) + \n  stat_function(fun = dnorm, args = list(mean = 38.8, sd = 2.66), \n                aes(color = \"Adelie\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 48.8, sd = 3.34),\n                aes(color = \"Chinstrap\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 47.5, sd = 3.08),\n                aes(color = \"Gentoo\"), linewidth = 3) +\n  ...\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_linear-discriminant-analysis_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n<details>\n<summary>image code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |>\n  ggplot(aes(x = bill_length_mm, color = species)) + \n  stat_function(fun = dnorm, args = list(mean = 38.8, sd = 2.66), \n                aes(color = \"Adelie\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 48.8, sd = 3.34),\n                aes(color = \"Chinstrap\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 47.5, sd = 3.08),\n                aes(color = \"Gentoo\"), linewidth = 3) + \n  geom_vline(xintercept = 50, linetype = \"dashed\") + \n  labs(title = \"<span style = 'color:#c65ccc'>Prior Probabilities</span>\",\n       subtitle = \"conditionally normal\",\n       caption = \"R4DS Book Club\") +\n  scale_color_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n```\n:::\n\n\n</details>\n\nComputing the likelihoods in `R`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# L(y = A | x_2 = 50) = 2.12e-05\ndnorm(50, mean = 38.8, sd = 2.66)\n\n# L(y = C | x_2 = 50) = 0.112\ndnorm(50, mean = 48.8, sd = 3.34)\n\n# L(y = G | x_2 = 50) = 0.09317\ndnorm(50, mean = 47.5, sd = 3.08)\n```\n:::\n\n\nTotal probability:\n\n$$f(x_{2} = 50) = \\frac{151}{342} \\cdot 0.0000212 + \\frac{68}{342} \\cdot 0.112 + \\frac{123}{342} \\cdot 0.09317 \\approx 0.05579$$\n\nBayes' Rules:\n\n$$\\begin{array}{rcccccl}\n  f(y = A | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = A | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{151}{342} \\cdot 0.0000212}{0.05579} & \\approx & 0.0002 \\\\\n  f(y = C | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = C | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{68}{342} \\cdot 0.112}{0.05579} & \\approx & 0.3992 \\\\\n  f(y = G | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = G | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{123}{342} \\cdot 0.09317}{0.05579} & \\approx & 0.6006 \\\\\n\\end{array}$$\n\nThough a 50mm-long bill is relatively less common among <span style = 'color:#067476'>Gentoo</span> than among <span style = 'color:#c65ccc'>Chinstrap</span>, it follows that our naive Bayes classification, based on our prior information and penguin’s bill length alone, is that this penguin is a <span style = 'color:#067476'>Gentoo</span> – it has the highest posterior probability.\n\nWe’ve now made two naive Bayes classifications of our penguin’s species, one based solely on the fact that our penguin has below-average weight and the other based solely on its 50mm-long bill (in addition to our prior information). And these classifications **disagree**: we classified the penguin as Adelie in the former analysis and Gentoo in the latter. This discrepancy indicates that there’s *room for improvement* in our naive Bayes classification method.\n\n\n### Two Predictor Variables\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_linear-discriminant-analysis_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n<details>\n<summary>image code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins |>\nggplot(aes(x = flipper_length_mm, y = bill_length_mm, \n           color = species)) + \n  geom_point(size = 3) + \n  geom_segment(aes(x = 195, y = 30, xend = 195, yend = 50),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 170, y = 50, xend = 195, yend = 50),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  labs(title = \"<span style = 'color:#c65ccc'>Two Predictor Variables</span>\",\n       subtitle = \"50mm-long bill and 195mm-long flipper\",\n       caption = \"R4DS Book Club\") +\n  scale_color_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n```\n:::\n\n</details>\n\nGeneralizing Bayes' Rule:\n\n$$f(y | x_{2}, x_{3}) = \\frac{f(y) \\cdot L(y | x_{2}, x_{3})}{\\sum_{y'} f(y') \\cdot L(y' | x_{2}, x_{3})}$$\n\nAnother \"naive\" assumption of **conditionally independent**:\n\n$$L(y | x_{2}, x_{3}) = f(x_{2}, x_{3} | y) = f(x_{2} | y) \\cdot f(x_{3} | y)$$\n\n* mathematically efficient\n* but what about correlation?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample statistics of x_3: flipper length\npenguins |> \n  group_by(species) |> \n  summarize(mean = mean(flipper_length_mm, na.rm = TRUE), \n            sd = sd(flipper_length_mm, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  species    mean    sd\n  <fct>     <dbl> <dbl>\n1 Adelie     190.  6.54\n2 Chinstrap  196.  7.13\n3 Gentoo     217.  6.48\n```\n\n\n:::\n:::\n\n\nLikelihoods of a flipper length of 195 mm:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# L(y = A | x_3 = 195) = 0.04554\ndnorm(195, mean = 190, sd = 6.54)\n\n# L(y = C | x_3 = 195) = 0.05541\ndnorm(195, mean = 196, sd = 7.13)\n\n# L(y = G | x_3 = 195) = 0.0001934\ndnorm(195, mean = 217, sd = 6.48)\n```\n:::\n\n\nTotal probability:\n\n$$f(x_{2} = 50, x_{3} = 195) = \\frac{151}{342} \\cdot 0.0000212 \\cdot 0.04554 + \\frac{68}{342} \\cdot 0.112 \\cdot 0.05541 + \\frac{123}{342} \\cdot 0.09317 \\cdot 0.0001931 \\approx 0.001241$$\n\nBayes' Rules:\n\n$$\\begin{array}{rcccl}\n  f(y = A | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{151}{342} \\cdot 0.0000212 \\cdot 0.04554}{0.0001931} & \\approx & 0.0003 \\\\\n  f(y = C | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{68}{342} \\cdot 0.112 \\cdot 0.05541}{0.0001931} & \\approx & 0.9944 \\\\\n  f(y = G | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{123}{342} \\cdot 0.09317 \\cdot 0.0001931}{0.0001931} & \\approx & 0.0052 \\\\\n\\end{array}$$\n\nIn conclusion, our penguin is *almost certainly* a <span style = 'color:#c65ccc'>Chinstrap</span>.\n\n\n## Naive Bayes in R\n\nTo implement naive Bayes classification in `R`, we’ll use the `naiveBayes()` function in the `e1071` package (Meyer et al. 2021)\n\n### Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# two models\nnaive_model_1 <- e1071::naiveBayes(species ~ bill_length_mm, \n                                   data = penguins)\nnaive_model_2 <- e1071::naiveBayes(species ~ bill_length_mm + flipper_length_mm, \n                            data = penguins)\n\n# our penguin to classify\nour_penguin <- data.frame(bill_length_mm = 50, flipper_length_mm = 195)\n```\n:::\n\n\n### Predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(naive_model_1, newdata = our_penguin, type = \"raw\") |>\nround(6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Adelie Chinstrap Gentoo\n[1,] 0.000169  0.397831  0.602\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(naive_model_1, newdata = our_penguin)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] Gentoo\nLevels: Adelie Chinstrap Gentoo\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(naive_model_2, newdata = our_penguin, type = \"raw\") |>\nround(6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Adelie Chinstrap   Gentoo\n[1,] 0.000345  0.994868 0.004787\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(naive_model_2, newdata = our_penguin)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] Chinstrap\nLevels: Adelie Chinstrap Gentoo\n```\n\n\n:::\n:::\n\n\n### Validation\n\n### Confusion Matrices\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins <- penguins %>% # keep magrittr pipe?\n  mutate(class_1 = predict(naive_model_1, newdata = .),\n         class_2 = predict(naive_model_2, newdata = .))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(84735)\npenguins |> \n  sample_n(4) |> \n  select(bill_length_mm, flipper_length_mm, species, class_1, class_2) |> \n  rename(bill = bill_length_mm, flipper = flipper_length_mm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n   bill flipper species   class_1 class_2  \n  <dbl>   <int> <fct>     <fct>   <fct>    \n1  47.5     199 Chinstrap Gentoo  Chinstrap\n2  40.9     214 Gentoo    Adelie  Gentoo   \n3  41.3     194 Adelie    Adelie  Adelie   \n4  38.5     190 Adelie    Adelie  Adelie   \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Confusion matrix for naive_model_1\npenguins |> \n  tabyl(species, class_1) |> \n  adorn_percentages(\"row\") |> \n  adorn_pct_formatting(digits = 2) |>\n  adorn_ns()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   species       Adelie Chinstrap       Gentoo\n    Adelie 95.39% (145) 0.00% (0)  4.61%   (7)\n Chinstrap  5.88%   (4) 8.82% (6) 85.29%  (58)\n    Gentoo  6.45%   (8) 4.84% (6) 88.71% (110)\n```\n\n\n:::\n:::\n\n\n* accuracy: 76 percent\n* 85 percent of Chinstap penguins are misclassified as Gentoo!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Confusion matrix for naive_model_2\npenguins |> \n  tabyl(species, class_2) |> \n  adorn_percentages(\"row\") |> \n  adorn_pct_formatting(digits = 2) |>\n  adorn_ns()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   species       Adelie   Chinstrap       Gentoo\n    Adelie 96.05% (146)  2.63%  (4)  1.32%   (2)\n Chinstrap  7.35%   (5) 86.76% (59)  5.88%   (4)\n    Gentoo  0.81%   (1)  0.81%  (1) 98.39% (122)\n```\n\n\n:::\n:::\n\n\n* accuracy: 95 percent\n\n### Cross-Validation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 10-fold cross-validation\nset.seed(84735)\ncv_model_2 <- naive_classification_summary_cv(\n  model = naive_model_2, data = penguins, y = \"species\", k = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_model_2$cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   species       Adelie   Chinstrap       Gentoo\n    Adelie 96.05% (146)  2.63%  (4)  1.32%   (2)\n Chinstrap  7.35%   (5) 86.76% (59)  5.88%   (4)\n    Gentoo  0.81%   (1)  0.81%  (1) 98.39% (122)\n```\n\n\n:::\n:::\n\n\n## NBC Math\n\n### MLEs\n\n* binary features\n\n$$\\hat{\\theta}_{dc} = \\frac{N_{dc}}{N_{c}}$$\n\n* discrete features\n\n$$\\hat{\\theta}_{dck} = \\displaystyle\\frac{N_{dck}}{N_{c}}$$\n\n* numerical features\n\n$$\\begin{array}{rcl}\n  \\hat{\\mu}_{dc} & = & \\displaystyle\\frac{1}{N_{dc}} \\displaystyle\\sum_{n:y_{n} = c} x_{nd} \\\\\n  \\hat{\\sigma}_{dc}^{2} & = & \\displaystyle\\frac{1}{N_{dc}} \\displaystyle\\sum_{n:y_{n} = c} (x_{nd} - \\hat{\\mu}_{dc})^{2} \\\\\n\\end{array}$$\n\n* MAP: add-one smoothing\n\n$$\\begin{array}{rcl}\n  \\bar{\\theta}_{dc} & = & \\displaystyle\\frac{1 + N_{dc1}}{2 + N_{dc}} \\\\\n  p(y = c|\\vec{x}, D) & \\propto & \\bar{\\pi}_{c}\\displaystyle\\prod_{d}\\prod_{k} \\bar{\\theta}_{dck} \\cdot I(x_{d} = k) \\\\\n\\end{array}$$\n\n### Imputation\n\nSuppose that we are missing the value of $x_{j}$\n\n* Gaussian discriminant analysis\n\n$$p(y=c|\\vec{x}_{i \\neq j}, \\vec{\\theta}) = p(y = c)\\displaystyle\\sum_{x_{j}} p(x_{j}, \\vec{x}_{i \\neq j}|y = c, \\vec{\\theta})$$\n\n* Naive Bayes classifier\n\n$$\\displaystyle\\sum_{x_{j}} p(x_{j}, x_{i \\neq j} | y = c, \\vec{\\theta}) = \\displaystyle\\prod_{i \\neq j}^{D} p(x_{i}|\\vec{\\theta}_{dc})$$\n\n\n## Bayes and Logistic Regression\n\nThe class posterior distribution for a Naive Bayes classification model has the same form as multinomial logistic regression:\n\n$$p(y = c|\\vec{x}, \\vec{\\theta}) = \\displaystyle\\frac{e^{\\beta_{c}^{T}\\vec{x} + \\gamma_{c}}}{\\displaystyle\\sum_{c'=1}^{C} e^{\\beta_{c}^{T}\\vec{x} + \\gamma_{c}}}$$\n\n### Naive Bayes\n\n$$f(y | x_{1}, x_{2}, ..., x_{p}) = \\frac{f(y) \\cdot L(y | x_{1}, x_{2}, ..., x_{p})}{\\sum_{y'} f(y') \\cdot L(y' | x_{1}, x_{2}, ..., x_{p})}$$\n\n* conditionally independent $\\rightarrow$ computationally efficient\n* generalizes to more than two categories\n* assumptions violated commonly in practice\n* optimizes joint likelihood $\\displaystyle\\prod_{n} p(y_{n},\\vec{x}_{n}|\\vec{\\theta})$\n\n### Logistic Regression\n\n$$\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{p}$$\n\n* binary classification\n* coefficients $\\rightarrow$ illumination of the relationships among these variables\n* optimizes conditional likelihood $\\displaystyle\\prod_{n} p(y_{n}|\\vec{x}_{n},\\vec{\\theta})$\n\n\n## Covariance Revisited\n\n* Naive Bayes ignored covariance (assumed conditional independence)\n* discriminant analyses (generative approach): fit multivariate Gaussians\n* want: dimensionality reduction\n\nFisher's linear discriminant analysis (FLDA) is a hybrid of discriminative and generative techniques, but limited to\n\n$$K \\leq C - 1 \\text{ dimensions}$$\n\n* $C$: number of classes in response variable\n* $D$: number of dimensions in projected space\n\n## FLDA Ideas\n\n* $S_{B}, S_{W}$: scatter matrices (estimate covariance)\n* $W$: projection matrix from $D$ to $K$ dimensions\n\nObjective: maximize\n\n$$J(W) = \\displaystyle\\frac{|W^{T}S_{B}W|}{|W^{T}S_{W}W|}$$\n\n* eigenvalue scenario instead of gradient descent\n\n## PCA Example\n\n<details>\n<summary>R setup</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_2_class <- penguins |>\n  filter(species %in% c(\"Chinstrap\", \"Gentoo\")) |>\n  na.omit()\n\npenguin_2_class |>\nggplot(aes(x = flipper_length_mm, y = bill_length_mm, \n           color = species)) + \n  geom_point(size = 3) + \n  labs(title = \"Two Predictor Variables\",\n       subtitle = \"50mm-long bill and 195mm-long flipper\",\n       caption = \"Data Science Learning Community\") +\n  scale_color_manual(values = c(chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\ntrain_set <- penguin_2_class |>\n  select(flipper_length_mm, bill_length_mm)\n```\n:::\n\n</details>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_linear-discriminant-analysis_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npca_results <- prcomp(train_set, center = TRUE, scale. = TRUE)\n```\n:::\n\n\n<details>\n<summary>PCA math</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\ndel_x <- pca_results$rotation[1,1]\ndel_y <- pca_results$rotation[2,1]\npca_slope <- del_y / del_x\n\nxbar <- mean(train_set$flipper_length_mm, na.rm = TRUE)\nybar <- mean(train_set$bill_length_mm, na.rm = TRUE)\npca_intercept <- ybar - pca_slope * xbar\n\npca_plot_1 <- penguin_2_class |>\nggplot(aes(x = flipper_length_mm, y = bill_length_mm)) + \n  geom_point(size = 3) + \n  geom_abline(slope = pca_slope, intercept = pca_intercept,\n              color = adelie_color, linewidth = 3) +\n  labs(title = \"Principal Component Analysis\",\n       subtitle = \"<span style = 'color:#fb7504'>first principal component</span>\",\n       caption = \"Data Science Learning Community\") +\n  # scale_color_manual(values = c(chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 14),\n        plot.subtitle = element_markdown(size = 12))\n\npca_plot_1\n```\n:::\n\n</details>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_linear-discriminant-analysis_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n* PC1 captures variance of the *entire* data set\n\n<details>\n<summary>Projection math</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_mat <- as.matrix(train_set)\nproj_mat  <- as.matrix(pca_results$rotation[,1])\nprojection_data <- train_mat %*% proj_mat\nprojection_df <- cbind(penguin_2_class, projection_data)\n\npca_plot_2 <- projection_df |>\n  ggplot(aes(x = projection_data)) +\n  geom_density(aes(fill = species),\n               alpha = 0.5) + \n  labs(title = \"Classification via <br><span style = 'color:#fb7504'>Principal Component Analysis</span>\",\n       subtitle = \"\",\n       caption = \"Data Science Learning Community\",\n       x = \"(PC1) first principal component\",\n       y = \"\") +\n  scale_fill_manual(values = c(chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(axis.title.y  = element_blank(),\n        axis.text.y   = element_blank(),\n        axis.ticks.y  = element_blank(),\n        plot.title    = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\npca_plot_2\n```\n:::\n\n</details>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_linear-discriminant-analysis_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\n## FLDA Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda_model <- MASS::lda(bill_length_mm ~ flipper_length_mm, \n                       data = train_set)\n```\n:::\n\n\n<details>\n<summary>LDA math</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\ndel_x <- 1\ndel_y <- lda_model$scaling[1,1]\nlda_slope <- del_y / del_x\n\nxbar <- mean(train_set$flipper_length_mm, na.rm = TRUE)\nybar <- mean(train_set$bill_length_mm, na.rm = TRUE)\nlda_intercept <- ybar - lda_slope * xbar\n\nlda_plot_1 <- penguin_2_class |>\nggplot(aes(x = flipper_length_mm, y = bill_length_mm, \n           color = species)) + \n  geom_point(size = 3) + \n  geom_abline(slope = lda_slope, intercept = lda_intercept,\n              color = adelie_color, linewidth = 3) +\n  labs(title = \"Linear Discriminant Analysis\",\n       subtitle = \"<span style = 'color:#fb7504'>first linear discriminant</span>\",\n       caption = \"Data Science Learning Community\") +\n  scale_color_manual(values = c(chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\nlda_plot_1\n```\n:::\n\n</details>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_linear-discriminant-analysis_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n<details>\n<summary>Projection math</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_mat <- as.matrix(train_set)\nproj_mat  <- as.matrix(c(1, lda_model$scaling[1,1]))\nprojection_data <- train_mat %*% proj_mat\nprojection_df <- cbind(penguin_2_class, projection_data)\n\nlda_plot_2 <- projection_df |>\n  ggplot(aes(x = projection_data)) +\n  geom_density(aes(fill = species),\n               alpha = 0.5) + \n  labs(title = \"Classification via <br><span style = 'color:#fb7504'>Linear Discriminant Analysis</span>\",\n       subtitle = \"\",\n       caption = \"Data Science Learning Community\",\n       x = \"(LDA1) first linear discriminant\",\n       y = \"\") +\n  scale_fill_manual(values = c(chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(axis.title.y  = element_blank(),\n        axis.text.y   = element_blank(),\n        axis.ticks.y  = element_blank(),\n        plot.title    = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\nlda_plot_2\n```\n:::\n\n\n</details>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_linear-discriminant-analysis_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\n## FLDA vs PCA\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_linear-discriminant-analysis_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n\n## Discriminative vs Generative\n\n### Advantages of discriminative classifiers\n\n* Better predictive accuracy\n* Can handle feature preprocessing\n* Well-calibrated probabilities\n\n### Advantages of generative classifiers\n\n* Easy to fit\n* Can easily handle missing input features\n* Can fit classes separately\n* Can handle unlabeled training data\n* May be more robust to spurious features\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"wsldsosjak\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#wsldsosjak table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#wsldsosjak thead, #wsldsosjak tbody, #wsldsosjak tfoot, #wsldsosjak tr, #wsldsosjak td, #wsldsosjak th {\n  border-style: none;\n}\n\n#wsldsosjak p {\n  margin: 0;\n  padding: 0;\n}\n\n#wsldsosjak .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#wsldsosjak .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#wsldsosjak .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#wsldsosjak .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#wsldsosjak .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#wsldsosjak .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#wsldsosjak .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#wsldsosjak .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#wsldsosjak .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#wsldsosjak .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#wsldsosjak .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#wsldsosjak .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#wsldsosjak .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#wsldsosjak .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#wsldsosjak .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#wsldsosjak .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#wsldsosjak .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#wsldsosjak .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#wsldsosjak .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#wsldsosjak .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#wsldsosjak .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#wsldsosjak .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#wsldsosjak .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#wsldsosjak .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#wsldsosjak .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#wsldsosjak .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#wsldsosjak .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#wsldsosjak .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#wsldsosjak .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#wsldsosjak .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#wsldsosjak .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#wsldsosjak .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#wsldsosjak .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#wsldsosjak .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#wsldsosjak .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#wsldsosjak .gt_left {\n  text-align: left;\n}\n\n#wsldsosjak .gt_center {\n  text-align: center;\n}\n\n#wsldsosjak .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#wsldsosjak .gt_font_normal {\n  font-weight: normal;\n}\n\n#wsldsosjak .gt_font_bold {\n  font-weight: bold;\n}\n\n#wsldsosjak .gt_font_italic {\n  font-style: italic;\n}\n\n#wsldsosjak .gt_super {\n  font-size: 65%;\n}\n\n#wsldsosjak .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#wsldsosjak .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#wsldsosjak .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#wsldsosjak .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#wsldsosjak .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#wsldsosjak .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#wsldsosjak .gt_indent_5 {\n  text-indent: 25px;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_heading\">\n      <td colspan=\"2\" class=\"gt_heading gt_title gt_font_normal gt_bottom_border\" style>Types of Classification Techniques<span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"><sup>1</sup></span></td>\n    </tr>\n    \n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Discriminative Classifiers\">Discriminative Classifiers</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Generative Classifiers\">Generative Classifiers</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"discriminative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #F9E3D6;\">Logistic regression</td>\n<td headers=\"generative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #E0FFFF;\">Naive Bayes</td></tr>\n    <tr><td headers=\"discriminative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #F9E3D6;\">Support vector machines</td>\n<td headers=\"generative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #E0FFFF;\">Bayesian networks</td></tr>\n    <tr><td headers=\"discriminative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #F9E3D6;\">Neural networks</td>\n<td headers=\"generative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #E0FFFF;\">Markov random fields</td></tr>\n    <tr><td headers=\"discriminative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #F9E3D6;\">Nearest neighbor</td>\n<td headers=\"generative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #E0FFFF;\">Hidden Markov Models</td></tr>\n    <tr><td headers=\"discriminative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #F9E3D6;\">Conditional Random Fields</td>\n<td headers=\"generative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #E0FFFF;\">Latent Dirichlet Allocation</td></tr>\n    <tr><td headers=\"discriminative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #F9E3D6;\">Random Forests</td>\n<td headers=\"generative_classifiers\" class=\"gt_row gt_center\" style=\"background-color: #E0FFFF;\">Generative Adversarial Networks</td></tr>\n  </tbody>\n  \n  <tfoot class=\"gt_footnotes\">\n    <tr>\n      <td class=\"gt_footnote\" colspan=\"2\"><span class=\"gt_footnote_marks\" style=\"white-space:nowrap;font-style:italic;font-weight:normal;\"><sup>1</sup></span> Source: https://www.analyticsvidhya.com/blog/2021/07/deep-understanding-of-discriminative-and-generative-models-in-machine-learning/</td>\n    </tr>\n  </tfoot>\n</table>\n</div>\n```\n\n:::\n:::\n\n\n\n\n## Meeting Videos {-}\n\n### Cohort 1 {-}\n\n<iframe src=\"https://www.youtube.com/embed/URL\" width=\"100%\" height=\"400px\" data-external=\"1\"></iframe>\n\n<details>\n<summary> Meeting chat log </summary>\n\n```\n00:51:52    Heidi L Wallace:    I have to jump before my next meeting, thank you so much  for this!\n```\n</details>\n",
    "supporting": [
      "09_linear-discriminant-analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}