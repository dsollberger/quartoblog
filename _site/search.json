[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "quartoblog",
    "section": "",
    "text": "30 Day Map Challenge\n\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nSome Quick Baseball Stats\n\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nAI Tools Fall Working Group\n\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression Demonstration\n\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nResources about Artificial Intelligence Tools\n\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nUsing nbgitpuller\n\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nHands-On Exploration of UDL Modalities\n\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nFonts\n\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nQuarto Resources\n\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nInequalities\n\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nTidyModels Trees\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\ngraphviz\n\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nSex in a Biological Context\n\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nMath Biology Video Project\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nDerek Sollberger, Emily Weigel\n\n\n\n\n\n\n  \n\n\n\n\nhospital_data\n\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nJupyterHub Showcase\n\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nCalifornia Weather\n\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\ncurly operator\n\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nStardew Valley Crops\n\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nCanvas_Roster\n\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nSeasons Greetings by Riinu Pius\n\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nAnyway Heres Wonderwall\n\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nPatchwork\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nSettlement Survival Start\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nCoefficient of Variation\n\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nIntentional Walks\n\n\n\n\n\n\n\nbaseball\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nOblicubes\n\n\n\n\n\n\n\nfonts\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nTypewriter\n\n\n\n\n\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nBump\n\n\n\n\n\n\n\nsports\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\noverviewR\n\n\n\n\n\n\n\nexploratory data analysis\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nquarto\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nDerek Sollberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nToday I followed Albert Rapp’s guide to creating a Quarto blog.\nFor me, it was a matter of making\n\na GitHub repository, called it quartoblog, and cloned it to my computer\ndeleted the quartoblog folder on my computer\nstarted a new project in RStudio to select Quarto web site (note that Quarto is already bundled in newer versions of the RStudio IDE)\ngot Netlify and GitHub to play nicely with each other and share the quartoblog repository\nupdated my domain redirect to the Netlify site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/bump/bump.html",
    "href": "posts/bump/bump.html",
    "title": "Bump",
    "section": "",
    "text": "Today I want to try to make a bump plot while practicing with sports data. The Lahman data set has a lot of historical data about Major League Baseball. Data scientists have been using bump plots for a few years now, but currently I wish to credit this code by Albert Rapp.\n\nlibrary(\"ggbump\")\nlibrary(\"Lahman\")\nlibrary(\"tidyverse\")\n\nFor today’s easy foray, let us seek out the wins and losses of teams in the Teams data frame (I tend to call my data frames df for typing ease).\n\ndf &lt;- Teams\n\nThere are about 3000 observations and 48 variables. I will need some of the column names.\n\ncolnames(df)\n\n [1] \"yearID\"         \"lgID\"           \"teamID\"         \"franchID\"      \n [5] \"divID\"          \"Rank\"           \"G\"              \"Ghome\"         \n [9] \"W\"              \"L\"              \"DivWin\"         \"WCWin\"         \n[13] \"LgWin\"          \"WSWin\"          \"R\"              \"AB\"            \n[17] \"H\"              \"X2B\"            \"X3B\"            \"HR\"            \n[21] \"BB\"             \"SO\"             \"SB\"             \"CS\"            \n[25] \"HBP\"            \"SF\"             \"RA\"             \"ER\"            \n[29] \"ERA\"            \"CG\"             \"SHO\"            \"SV\"            \n[33] \"IPouts\"         \"HA\"             \"HRA\"            \"BBA\"           \n[37] \"SOA\"            \"E\"              \"DP\"             \"FP\"            \n[41] \"name\"           \"park\"           \"attendance\"     \"BPF\"           \n[45] \"PPF\"            \"teamIDBR\"       \"teamIDlahman45\" \"teamIDretro\"   \n\n\nTo make a quick exploration, let us filter for the past 10 seasons of baseball (2012 to 2021) and select the columns I will use later.\n\ndf &lt;- Teams |&gt;\n  filter(yearID &gt;= 2012) |&gt;\n  select(yearID, lgID, franchID, divID, Rank)\nhead(df)\n\n  yearID lgID franchID divID Rank\n1   2012   NL      ARI     W    3\n2   2012   NL      ATL     E    2\n3   2012   AL      BAL     E    2\n4   2012   AL      BOS     E    5\n5   2012   AL      CHW     C    2\n6   2012   NL      CHC     C    5\n\n\nTo be honest, I thought I was going to have to code up some function to rank team wins within the MLB divisions, but the Lahman database already has that!\n\ndf_left &lt;- df |&gt; filter(yearID == 2012 & lgID == \"NL\")\ndf_right &lt;- df |&gt; filter(yearID == 2021 & lgID == \"NL\")\n\n\ndf |&gt;\n  filter(lgID == \"NL\") |&gt;\n  ggplot(aes(x = yearID, y = -Rank, color = franchID)) +\n  geom_bump(size = 2) +\n  geom_point(aes(x = yearID, y = -Rank, color = franchID),\n             size = 5) +\n  geom_label(aes(x = yearID, y = -Rank, label = franchID), data = df_left) +\n  geom_label(aes(x = yearID, y = -Rank, label = franchID), data = df_right) +\n  facet_wrap(. ~ divID, ncol = 1) +\n  labs(title = \"National League Standings\",\n       subtitle = \"early draft of bump plot\",\n       caption = \"Derek Sollberger\") +\n  theme(legend.position = \"none\",\n        panel.background = element_blank())\n\nWarning in f(...): 'StatBump' needs at least two observations per group\n\n\n\n\n\n\n\n\nbump plot"
  },
  {
    "objectID": "posts/typewriter/typewriter.html",
    "href": "posts/typewriter/typewriter.html",
    "title": "Typewriter",
    "section": "",
    "text": "Well, I was nerd sniped. Ryan Timpe tweeted an image that made it (at first glance) appear that the gtExtras package allowed users to display a gt table as if it was made by a typewriter on classic notebook paper. That might exist in the near future, but I thought I would try to make a similar image manually.\n\nlibrary(\"gt\")\n\n\ndf &lt;- data.frame(\n  Period = c(\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"),\n  Course = c(\"Academic Decathlon\", \"Spanish 4 AP\", \"Calculus AP BC\", \"English 3 Honors\", \"Computer Applications\", \"Physics AP\", \"US History\")\n)\n\n\n# https://gt.rstudio.com/reference/cell_borders.html\ndf |&gt;\n  gt() |&gt;\n  tab_options(\n    # table.background.color = \"#E8EBE6\",\n    table.font.names = \"Courier New\"\n  ) |&gt;\n  tab_style(\n    locations = cells_body(\n      columns = everything(),\n      rows = everything()\n    ),\n    style = list(\n      cell_borders(\n        sides = c(\"top\", \"bottom\"),\n        color = \"cyan\",\n        style = \"solid\"\n      ),\n      cell_fill(color = \"#E8EBE6\") #https://www.crispedge.com/color/e8ebe6/\n    )\n  ) |&gt;\n  tab_style(\n    locations = cells_body(\n      columns = \"Course\",\n      rows = everything()\n    ),\n    style = list(\n      cell_borders(\n        sides = c(\"left\"),\n        color = \"red\",\n        style = \"solid\"\n      ),\n      cell_fill(color = \"#E8EBE6\")\n    )\n  )\n\n\n\n\n\n\n\n\nPeriod\nCourse\n\n\n\n\n0\nAcademic Decathlon\n\n\n1\nSpanish 4 AP\n\n\n2\nCalculus AP BC\n\n\n3\nEnglish 3 Honors\n\n\n4\nComputer Applications\n\n\n5\nPhysics AP\n\n\n6\nUS History\n\n\n\n\n\n\n\n\n\n\nhigh school junior year schedule"
  },
  {
    "objectID": "posts/overviewR/overviewR.html",
    "href": "posts/overviewR/overviewR.html",
    "title": "overviewR",
    "section": "",
    "text": "Today I wanted to try out the overviewR package.\n\ncheat sheet: https://github.com/cosimameyer/overviewR/blob/master/man/figures/CheatSheet_overviewR.pdf\n\n\nlibrary(\"overviewR\")\nlibrary(\"palmerpenguins\")\n\nAt this moment, I misread what overviewR does (I thought it would summarize everything). Instead, I will just try out one tool for now.\n\npenguins_raw |&gt;\n  overview_na()"
  },
  {
    "objectID": "posts/oblicubes/oblicubes.html",
    "href": "posts/oblicubes/oblicubes.html",
    "title": "Oblicubes",
    "section": "",
    "text": "I always like to have something silly for the course image in the Canvas LMS for each of the classes that I teach. Today, I want to see if the oblicubes package by trevorld will continue the silly.\n\nhttps://github.com/trevorld/oblicubes\n\n\nlibrary(\"bittermelon\")\n\n\nAttaching package: 'bittermelon'\n\n\nThe following object is masked from 'package:base':\n\n    which\n\nlibrary(\"grid\")\nlibrary(\"oblicubes\")\n\n\n# example from\n# https://github.com/trevorld/oblicubes\nfont_file &lt;- system.file(\"fonts/spleen/spleen-8x16.hex.gz\", package = \"bittermelon\")\nfont &lt;- read_hex(font_file)\nbml &lt;- as_bm_list(\"RSTATS\", font = font)\n# Add a shadow effect and border\nbm &lt;- (3 * bml) |&gt;\n  bm_pad(sides = 2L) |&gt;\n  bm_shadow(value = 2L) |&gt;\n  bm_call(cbind) |&gt;\n  bm_extend(sides = 1L, value = 1L)\ncol &lt;- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"grey20\", \"lightblue\", \"darkblue\")\n})\ncoords &lt;- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Bio 18\nbml &lt;- as_bm_list(\"BIO 18\", font = font)\n\nbm &lt;- (3 * bml) |&gt;\n  bm_pad(sides = 2L) |&gt;\n  bm_shadow(value = 2L) |&gt;\n  bm_call(cbind) |&gt;\n  bm_extend(sides = 1L, value = 1L)\ncol &lt;- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords &lt;- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Math 32\nbml &lt;- as_bm_list(\"MATH32\", font = font)\n\nbm &lt;- (3 * bml) |&gt;\n  bm_pad(sides = 2L) |&gt;\n  bm_shadow(value = 2L) |&gt;\n  bm_call(cbind) |&gt;\n  bm_extend(sides = 1L, value = 1L)\ncol &lt;- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords &lt;- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Spark\nbml &lt;- as_bm_list(\"SPORTS\", font = font)\n\nbm &lt;- (3 * bml) |&gt;\n  bm_pad(sides = 2L) |&gt;\n  bm_shadow(value = 2L) |&gt;\n  bm_call(cbind) |&gt;\n  bm_extend(sides = 1L, value = 1L)\ncol &lt;- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords &lt;- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\nand then I cropped an resized each image to 262 by 146 pixels\n\nhttps://unomaha.instructure.com/courses/33506/pages/create-a-canvas-dashboard-image-for-your-course"
  },
  {
    "objectID": "posts/oblicubes.html",
    "href": "posts/oblicubes.html",
    "title": "Oblicubes",
    "section": "",
    "text": "https://github.com/trevorld/oblicubes\n\n\nlibrary(\"bittermelon\")\n\n\nAttaching package: 'bittermelon'\n\n\nThe following object is masked from 'package:base':\n\n    which\n\nlibrary(\"grid\")\nlibrary(\"oblicubes\")\n\n\n# example from\n# https://github.com/trevorld/oblicubes\nfont_file <- system.file(\"fonts/spleen/spleen-8x16.hex.gz\", package = \"bittermelon\")\nfont <- read_hex(font_file)\nbml <- as_bm_list(\"RSTATS\", font = font)\n# Add a shadow effect and border\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"grey20\", \"lightblue\", \"darkblue\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Bio 18\nbml <- as_bm_list(\"BIO 18\", font = font)\n\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Math 32\nbml <- as_bm_list(\"MATH32\", font = font)\n\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))\n\n\n\n\n\n# Spark\nbml <- as_bm_list(\"SPORTS\", font = font)\n\nbm <- (3 * bml) |>\n  bm_pad(sides = 2L) |>\n  bm_shadow(value = 2L) |>\n  bm_call(cbind) |>\n  bm_extend(sides = 1L, value = 1L)\ncol <- apply(bm + 1L, c(1, 2), function(i) {\n  switch(i, \"white\", \"#5b5b5b\", \"#DAA900\", \"#002856\")\n  # switch(i, \"#DAA900\", \"#5b5b5b\", \"#0091b3\", \"#002856\")\n})\ncoords <- xyz_heightmap(bm, col = col, flipy=FALSE)\ngrid.oblicubes(coords, width=unit(2.2, \"mm\"))"
  },
  {
    "objectID": "posts/intentional_walks/ibb.html",
    "href": "posts/intentional_walks/ibb.html",
    "title": "Intentional Walks",
    "section": "",
    "text": "Today’s exploration and visualization practive was inspired by this tweet:\n“Barry Bonds has over 200 more intentional walks than the Rays entire franchise.” — Cespedes Family BBQ, Dec. 2, 2013\n“Almost nine years later and Bonds still has 28 more intentional walks than the Rays franchise.” — Jim Passon, Aug. 25, 2022\nlibrary(\"ggtext\")\nlibrary(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/intentional_walks/ibb.html#gathering-the-data",
    "href": "posts/intentional_walks/ibb.html#gathering-the-data",
    "title": "Intentional Walks",
    "section": "Gathering the Data",
    "text": "Gathering the Data\nAt first, I thought that I could reasonably type in all of the data, but Bonds’ career was quite long. I copy-and-pasted the batting table from Baseball Reference and focused on the Year and IBB columns.\nThe franchise page for the Tampa Bay Rays did not have intentional walks quickly accessible, so I simply went through the year-by-year pages and gathered the IBB total (fortunately easy to find visually as the last column).\n\ndf_bonds &lt;- readxl::read_xlsx(\"ibb.xlsx\", sheet = \"Bonds\")\ndf_rays  &lt;- readxl::read_xlsx(\"ibb.xlsx\", sheet = \"Rays\")"
  },
  {
    "objectID": "posts/intentional_walks/ibb.html#data-wrangling",
    "href": "posts/intentional_walks/ibb.html#data-wrangling",
    "title": "Intentional Walks",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nWhile I could probably affect the horizontal axis later in a ggplot visualization, I prefer today to have the data in one data frame. Let me merge the data.\n\ndf &lt;- df_bonds |&gt;\n  full_join(df_rays, by = \"Year\") |&gt;\n  rename(\"IBB_Bonds\" = \"IBB.x\",\n        \"IBB_Rays\" = \"IBB.y\")\n\nThe naturally missing values probably would not affect the calculations much, but for peace of mind, let us impute those values to be zeroes.\n\ndf$IBB_Bonds[is.na(df$IBB_Bonds)] &lt;- 0\ndf$IBB_Rays[is.na(df$IBB_Rays)]   &lt;- 0\n\nIn the spirit of the tweet, we will need to talk about cumulative totals.\n\ndf &lt;- df |&gt;\n  mutate(IBB_Bonds_total = cumsum(IBB_Bonds),\n         IBB_Rays_total  = cumsum(IBB_Rays))"
  },
  {
    "objectID": "posts/intentional_walks/ibb.html#data-visualization",
    "href": "posts/intentional_walks/ibb.html#data-visualization",
    "title": "Intentional Walks",
    "section": "Data Visualization",
    "text": "Data Visualization\nWhile line graphs (with areas filled) would probably be the most visually appealing, it might be better to treat seasons as discrete entries in a bar plot.\n\ndf |&gt;\n  ggplot() +\n  geom_bar(aes(x = Year, y = IBB_Bonds_total),\n           stat = \"identity\")\n\n\n\n\nNow I am curious what the bars will look like in team colors.\n\nbaseplot &lt;- df |&gt;\n  ggplot() +\n  \n  # https://teamcolorcodes.com/san-francisco-giants-color-codes/\n  geom_bar(aes(x = Year, y = IBB_Bonds_total),\n           color = \"#27251F\", fill = \"#FD5A1E\",\n           stat = \"identity\") +\n  \n  # https://teamcolorcodes.com/tampa-bay-rays-color-codes/\n  geom_bar(aes(x = Year, y = IBB_Rays_total),\n           color = \"#8FBCE6\", fill = \"#092C5C\",\n           stat = \"identity\")\n\n# print\nbaseplot\n\n\n\n\nMoving toward aesthetic beauty, I will update some of the theme elements.\n\n# some ideas from\n# https://github.com/nikopech/TidyTuesday/blob/master/R/2022-08-09/2022_08_09_ferris_wheels.R\n\ncurrent_plot &lt;- baseplot +\n  labs(title = \"\",\n       subtitle = \"\",\n       caption = \"Derek Sollberger | August 26, 2022\",\n       x = \"season\",\n       y = \"intentional walk cumulative total\") +\n  theme(legend.position = \"none\",\n        panel.background = element_blank(),\n        plot.background = element_rect(\n            fill = \"#FFFFFF\", \n            color = \"#27251F\"\n        ),\n        plot.title = element_markdown(face = \"italic\",\n                                      margin = margin(b = 5),\n                                      size = 14),\n        plot.title.position = \"plot\",\n        plot.subtitle = element_markdown(face = \"italic\",\n                                         margin = margin(b = 5),\n                                         size = 12),\n        plot.caption = element_markdown(margin = margin(t = 0), \n                                        size = 10),\n        plot.caption.position = \"plot\",\n        plot.margin = margin(50, 50, 50, 50))\n\n# print\ncurrent_plot\n\n\n\n\nNow, I am going to attempt to add arrows (and later: labels) to highlight certain areas of the graph. This is still very new to me, so fingers crossed.\n\ncurrent_plot +\n  # In 2004, Barry Bonds received 120 intentional walks\n  annotate(color = \"#FD5A1E\",\n           geom = \"curve\",\n           size = 0.5,\n           x = 1999, xend = 2004,\n           y = 800, yend = 604) +\n  geom_textbox(aes(x = 1999, y = 800,\n                   color = \"#000000\",\n                   label = \"In 2004, Barry Bonds received 120 intentional walks\"),\n               size = 2) +\n  \n  # Start of Barry Bonds' MLB career\n  annotate(color = \"#FD5A1E\",\n           geom = \"curve\",\n           size = 0.5,\n           x = 1988, xend = 1986,\n           y = 200, yend = 0) +\n  geom_textbox(aes(x = 1988, y = 200,\n                   color = \"#000000\",\n                   label = \"Start of Barry Bonds' MLB career\"),\n               size = 2)\n\n\n\n# print\n# current_plot\n\nAt the moment, I am having difficulty getting labels to naturally appear beyond the panel.\nInstead, I will focus on the title and caption.\n\n# some ideas from\n# https://github.com/nikopech/TidyTuesday/blob/master/R/2022-08-09/2022_08_09_ferris_wheels.R\n\ncurrent_plot &lt;- baseplot +\n  labs(title = \"From 1986 to 2007, Barry Bonds accumulated 688 intentional walks\",\n       subtitle = \"From 1998 to present, the Tampa Bay Rays have accumulated 28 fewer walks\",\n       caption = \"Derek Sollberger | August 26, 2022\",\n       x = \"season\",\n       y = \"intentional walk cumulative total\") +\n  theme(legend.position = \"none\",\n        panel.background = element_blank(),\n        plot.background = element_rect(\n            fill = \"#FFFFFF\", \n            color = \"#27251F\"\n        ),\n        plot.title = element_text(color = \"#FD5A1E\", size = 14, hjust = 0.5),\n        plot.title.position = \"plot\",\n        plot.subtitle = element_text(color = \"#092C5C\", size = 12, hjust = 0.5),\n        plot.caption = element_markdown(margin = margin(t = 0), \n                                        size = 10),\n        plot.caption.position = \"plot\",\n        plot.margin = margin(20, 20, 20, 20))\n\n# print\ncurrent_plot"
  },
  {
    "objectID": "posts/coefficient_of_variation.html",
    "href": "posts/coefficient_of_variation.html",
    "title": "Coefficient of Variation",
    "section": "",
    "text": "For a future lecture in my Sports Analytics course, I want an example of a baseball statistic where the averages for two players are similar, but their variances in that same statistic are quite different. It is still early in the semester, so I am looking for an easy-to-understand statistic. Therefore, I will explore home runs per season."
  },
  {
    "objectID": "posts/coefficient_of_variation.html#a-walk-through-the-data",
    "href": "posts/coefficient_of_variation.html#a-walk-through-the-data",
    "title": "Coefficient of Variation",
    "section": "A Walk Through the Data",
    "text": "A Walk Through the Data\nThe Lahman database contains a lot of baseball statistics, and today I will focus on the Batting data frame.\n\nlibrary(\"Lahman\")\nlibrary(\"tidyverse\")\n\nFor interest, I will filter the observations to retain the players from the past 18 seasons (since my students are about 18 years old, haha) and player-seasons that included at least 100 at-bats.\n\ndf <- Batting |>\n  filter(yearID >= (2021 - 18)) |>\n  filter(AB >= 100)\n\nTo be illustrative, permit me to select mainly the seasons and home run columns.\n\ndf <- df |>\n  select(playerID, yearID, AB, HR)\n\nAt the moment, here is what our data looks like.\n\nhead(df)\n\n   playerID yearID  AB HR\n1 abreubo01   2003 577 20\n2 alfoned01   2003 514 13\n3 almoner01   2003 100  1\n4 alomaro01   2003 263  2\n5 alomaro01   2003 253  3\n6 alomasa02   2003 194  5\n\n\nSince I am concerned with season averages, I am going to group_by the player name. From there, let us then compute the averages and standard deviations for home runs.\n\ndf <- df |>\n  group_by(playerID) |>\n  mutate(xbar = mean(HR, na.rm = TRUE),\n         s    = sd(HR, na.rm = TRUE)) |>\n  ungroup()\nhead(df)\n\n# A tibble: 6 × 6\n  playerID  yearID    AB    HR  xbar      s\n  <chr>      <int> <int> <int> <dbl>  <dbl>\n1 abreubo01   2003   577    20 14.3   8.94 \n2 alfoned01   2003   514    13  8.67  5.86 \n3 almoner01   2003   100     1  1    NA    \n4 alomaro01   2003   263     2  2.67  0.577\n5 alomaro01   2003   253     3  2.67  0.577\n6 alomasa02   2003   194     5  2.33  2.52 \n\n\nFor various reasons, some players only appear once in this subset of data, so their variance is effectively zero (missing in the computation). To answer my original inquiry mathematically, I will now compute the coefficient of variation (and avoid a divide-by-zero error from those players who hit zero home runs).\n\\[CoV = \\frac{s}{\\bar{x}}\\]\n\ndf <- df |>\n  # filter(!is.na(s)) |>\n  filter(xbar > 0) |>\n  mutate(CoV = s/xbar) |>\n  arrange(desc(CoV))\n\nIn this metric, the top scores are\n\nhead(df)\n\n# A tibble: 6 × 7\n  playerID  yearID    AB    HR  xbar     s   CoV\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>\n1 gathrjo01   2005   203     0  0.2  0.447  2.24\n2 gathrjo01   2006   154     0  0.2  0.447  2.24\n3 gathrjo01   2006   229     1  0.2  0.447  2.24\n4 gathrjo01   2007   228     0  0.2  0.447  2.24\n5 gathrjo01   2008   279     0  0.2  0.447  2.24\n6 burriem01   2008   240     1  0.25 0.5    2   \n\n\nand the bottom scores are\n\ntail(df)\n\n# A tibble: 6 × 7\n  playerID  yearID    AB    HR  xbar     s   CoV\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>\n1 wadela01    2021   336    18    18    NA    NA\n2 wallsta01   2021   152     1     1    NA    NA\n3 walshja01   2021   530    29    29    NA    NA\n4 whiteel04   2021   198     6     6    NA    NA\n5 williju02   2021   119     4     4    NA    NA\n6 wisdopa01   2021   338    28    28    NA    NA"
  },
  {
    "objectID": "posts/coefficient_of_variation.html#heavy-hitters",
    "href": "posts/coefficient_of_variation.html#heavy-hitters",
    "title": "Coefficient of Variation",
    "section": "Heavy Hitters",
    "text": "Heavy Hitters\nSo far, these results seem to be fine mathematically, but they might be uninteresting to the casual baseball fan. To further prune down to recognizable players, let me further filter the data down to players with at least 5 of these 100+ at-bat seasons in the past 18 seasons—and had an average of over 10 home runs per season.\n\ndf <- df |>\n  group_by(playerID) |>\n  mutate(seasons = n()) |>\n  ungroup() |>\n  filter(seasons >= 5) |>\n  filter(xbar >= 5) |>\n  arrange(desc(CoV))\n\nNow, the top scores in using the coefficient of variation as my metric are\n\nhead(df)\n\n# A tibble: 6 × 8\n  playerID  yearID    AB    HR  xbar     s   CoV seasons\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>   <int>\n1 santada01   2014   405     7   7.5  10.3  1.38       6\n2 santada01   2015   261     0   7.5  10.3  1.38       6\n3 santada01   2016   233     2   7.5  10.3  1.38       6\n4 santada01   2017   143     3   7.5  10.3  1.38       6\n5 santada01   2019   474    28   7.5  10.3  1.38       6\n6 santada01   2021   116     5   7.5  10.3  1.38       6\n\n\nand the lowest \\(Cov\\) are\n\ntail(df)\n\n# A tibble: 6 × 8\n  playerID  yearID    AB    HR  xbar     s   CoV seasons\n  <chr>      <int> <int> <int> <dbl> <dbl> <dbl>   <int>\n1 delgaca01   2003   570    42  34.5  6.32 0.183       6\n2 delgaca01   2004   458    32  34.5  6.32 0.183       6\n3 delgaca01   2005   521    33  34.5  6.32 0.183       6\n4 delgaca01   2006   524    38  34.5  6.32 0.183       6\n5 delgaca01   2007   538    24  34.5  6.32 0.183       6\n6 delgaca01   2008   598    38  34.5  6.32 0.183       6\n\n\nWell, I do recognize more of the player names, but I realize that the \\(Cov\\) alone does not completely solve my question since I still wanted similar averages between two players."
  },
  {
    "objectID": "posts/coefficient_of_variation.html#a-metric-on-top-of-a-metric",
    "href": "posts/coefficient_of_variation.html#a-metric-on-top-of-a-metric",
    "title": "Coefficient of Variation",
    "section": "A Metric on top of a Metric",
    "text": "A Metric on top of a Metric\nNow, hear me out. If there are two players \\(A\\) and \\(B\\), then what I am looking for is a rate of change of the form\n\\[Y = \\frac{\\text{CoV}_{A} - \\text{CoV}_{B}}{\\bar{x}_{A} - \\bar{x}_{B}}\\]"
  },
  {
    "objectID": "posts/cofficient_of_variation/coefficient_of_variation.html",
    "href": "posts/cofficient_of_variation/coefficient_of_variation.html",
    "title": "Coefficient of Variation",
    "section": "",
    "text": "For a future lecture in my Sports Analytics course, I want an example of a baseball statistic where the averages for two players are similar, but their variances in that same statistic are quite different. It is still early in the semester, so I am looking for an easy-to-understand statistic. Therefore, I will explore home runs per season."
  },
  {
    "objectID": "posts/cofficient_of_variation/coefficient_of_variation.html#a-walk-through-the-data",
    "href": "posts/cofficient_of_variation/coefficient_of_variation.html#a-walk-through-the-data",
    "title": "Coefficient of Variation",
    "section": "A Walk Through the Data",
    "text": "A Walk Through the Data\nThe Lahman database contains a lot of baseball statistics, and today I will focus on the Batting data frame.\n\nlibrary(\"Lahman\")\nlibrary(\"tidyverse\")\n\nFor interest, I will filter the observations to retain the players from the past 18 seasons (since my students are about 18 years old, haha) and player-seasons that included at least 100 at-bats.\n\ndf &lt;- Batting |&gt;\n  filter(yearID &gt;= (2021 - 18)) |&gt;\n  filter(AB &gt;= 100)\n\nTo be illustrative, permit me to select mainly the seasons and home run columns.\n\ndf &lt;- df |&gt;\n  select(playerID, yearID, AB, HR)\n\nAt the moment, here is what our data looks like.\n\nhead(df)\n\n   playerID yearID  AB HR\n1 abreubo01   2003 577 20\n2 alfoned01   2003 514 13\n3 almoner01   2003 100  1\n4 alomaro01   2003 263  2\n5 alomaro01   2003 253  3\n6 alomasa02   2003 194  5\n\n\nSince I am concerned with season averages, I am going to group_by the player name. From there, let us then compute the averages and standard deviations for home runs.\n\ndf &lt;- df |&gt;\n  group_by(playerID) |&gt;\n  mutate(xbar = mean(HR, na.rm = TRUE),\n         s    = sd(HR, na.rm = TRUE)) |&gt;\n  ungroup()\nhead(df)\n\n# A tibble: 6 × 6\n  playerID  yearID    AB    HR  xbar      s\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 abreubo01   2003   577    20 14.3   8.94 \n2 alfoned01   2003   514    13  8.67  5.86 \n3 almoner01   2003   100     1  1    NA    \n4 alomaro01   2003   263     2  2.67  0.577\n5 alomaro01   2003   253     3  2.67  0.577\n6 alomasa02   2003   194     5  2.33  2.52 \n\n\nFor various reasons, some players only appear once in this subset of data, so their variance is effectively zero (missing in the computation). To answer my original inquiry mathematically, I will now compute the coefficient of variation (and avoid a divide-by-zero error from those players who hit zero home runs).\n\\[CoV = \\frac{s}{\\bar{x}}\\]\n\ndf &lt;- df |&gt;\n  # filter(!is.na(s)) |&gt;\n  filter(xbar &gt; 0) |&gt;\n  mutate(CoV = s/xbar) |&gt;\n  arrange(desc(CoV))\n\nIn this metric, the top scores are\n\nhead(df)\n\n# A tibble: 6 × 7\n  playerID  yearID    AB    HR  xbar     s   CoV\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 gathrjo01   2005   203     0  0.2  0.447  2.24\n2 gathrjo01   2006   154     0  0.2  0.447  2.24\n3 gathrjo01   2006   229     1  0.2  0.447  2.24\n4 gathrjo01   2007   228     0  0.2  0.447  2.24\n5 gathrjo01   2008   279     0  0.2  0.447  2.24\n6 burriem01   2008   240     1  0.25 0.5    2   \n\n\nand the bottom scores are\n\ntail(df)\n\n# A tibble: 6 × 7\n  playerID  yearID    AB    HR  xbar     s   CoV\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 wadela01    2021   336    18    18    NA    NA\n2 wallsta01   2021   152     1     1    NA    NA\n3 walshja01   2021   530    29    29    NA    NA\n4 whiteel04   2021   198     6     6    NA    NA\n5 williju02   2021   119     4     4    NA    NA\n6 wisdopa01   2021   338    28    28    NA    NA"
  },
  {
    "objectID": "posts/cofficient_of_variation/coefficient_of_variation.html#heavy-hitters",
    "href": "posts/cofficient_of_variation/coefficient_of_variation.html#heavy-hitters",
    "title": "Coefficient of Variation",
    "section": "Heavy Hitters",
    "text": "Heavy Hitters\nSo far, these results seem to be fine mathematically, but they might be uninteresting to the casual baseball fan. To further prune down to recognizable players, let me further filter the data down to players with at least 5 of these 100+ at-bat seasons in the past 18 seasons—and had an average of over 10 home runs per season.\n\ndf &lt;- df |&gt;\n  group_by(playerID) |&gt;\n  mutate(seasons = n()) |&gt;\n  ungroup() |&gt;\n  filter(seasons &gt;= 5) |&gt;\n  filter(xbar &gt;= 15) |&gt;\n  arrange(desc(CoV))\n\nNow, the top scores in using the coefficient of variation as my metric are\n\nhead(df)\n\n# A tibble: 6 × 8\n  playerID  yearID    AB    HR  xbar     s   CoV seasons\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1 solerjo01   2015   366    10  16.3  14.2 0.869       7\n2 solerjo01   2016   227    12  16.3  14.2 0.869       7\n3 solerjo01   2018   223     9  16.3  14.2 0.869       7\n4 solerjo01   2019   589    48  16.3  14.2 0.869       7\n5 solerjo01   2020   149     8  16.3  14.2 0.869       7\n6 solerjo01   2021   308    13  16.3  14.2 0.869       7\n\n\nand the lowest \\(Cov\\) are\n\ntail(df)\n\n# A tibble: 6 × 8\n  playerID  yearID    AB    HR  xbar     s   CoV seasons\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1 delgaca01   2003   570    42  34.5  6.32 0.183       6\n2 delgaca01   2004   458    32  34.5  6.32 0.183       6\n3 delgaca01   2005   521    33  34.5  6.32 0.183       6\n4 delgaca01   2006   524    38  34.5  6.32 0.183       6\n5 delgaca01   2007   538    24  34.5  6.32 0.183       6\n6 delgaca01   2008   598    38  34.5  6.32 0.183       6\n\n\nWell, I do recognize more of the player names, but I realize that the \\(Cov\\) alone does not completely solve my question since I still wanted similar averages between two players."
  },
  {
    "objectID": "posts/cofficient_of_variation/coefficient_of_variation.html#a-metric-on-top-of-a-metric",
    "href": "posts/cofficient_of_variation/coefficient_of_variation.html#a-metric-on-top-of-a-metric",
    "title": "Coefficient of Variation",
    "section": "A Metric on top of a Metric",
    "text": "A Metric on top of a Metric\nNow, hear me out. If there are two players \\(A\\) and \\(B\\), then what I am looking for is a rate of change of the form\n\\[Y = \\bigg|\\frac{s_{A} - s_{B}}{\\bar{x}_{A} - \\bar{x}_{B}}\\bigg|\\]\nFirst, let me simplify the data frame down to just the player names, home run averages, and their standard deviations.\n\ndf2 &lt;- df |&gt;\n  select(playerID, xbar, s) |&gt;\n  distinct()\nhead(df2)\n\n# A tibble: 6 × 3\n  playerID   xbar     s\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 solerjo01  16.3  14.2\n2 valenjo03  16.2  13.3\n3 muncyma01  20.5  16.6\n4 yelicch01  17.8  13.8\n5 moralke01  16.1  11.7\n6 carpema01  15.5  11.1\n\n\nPresently, I have about 221 observations, so a pair-wise calculations would be computed over about 4.8841^{4} pairs (hopefully manageable by a computer).\n\nN &lt;- nrow(df2)\ndf3 &lt;- data.frame(player1 = rep(NA, N^2),\n                  player2 = rep(NA, N^2),\n                  Y       = rep(NA, N^2))\n\nfor(i in 1:N){\n  for(j in 1:N){\n    row_number = j*(i-1) + j\n    df3$player1[row_number] &lt;- df2$playerID[i]\n    df3$player2[row_number] &lt;- df2$playerID[j]\n    \n    if(i == j){\n      this_Y_value &lt;- 0\n    } else {\n      denominator &lt;- df2$xbar[i] - df2$xbar[j]\n      if(denominator == 0){ denominator &lt;- 0.1 }\n      \n      this_Y_value &lt;- abs((df2$s[i] - df2$s[j]) / denominator)\n    }\n    \n    df3$Y[row_number] &lt;- this_Y_value\n  }\n  \n  # if((row_number %% 10000) == 0){\n  #   print(paste(\"Currently computing row number\", row_number))\n  # }\n}\n\nIn this improvised metric, the top 10 scores were\n\ndf3 |&gt;\n  arrange(desc(Y)) |&gt;\n  top_n(10)\n\nSelecting by Y\n\n\n     player1   player2        Y\n1  mccutan01 ramirjo01 692.3109\n2  polloaj01 utleych01 348.8826\n3  mccutan01 ensbemo01 291.9631\n4  hosmeer01 quentca01 244.8742\n5  mccanbr01  belljo02 209.0920\n6  jacobmi02   bayja01 198.8301\n7   kentje01 giambja01 190.9222\n8  jonesga02 kinslia01 182.2589\n9  contrwi01  shawtr01 180.9609\n10 polloaj01 morrilo01 179.5927\n\n\nUsing the playerInfo() helper function in the Lahman package, we can verify the names of the players.\n\nplayerInfo(\"mccutan01\")\n\n       playerID nameFirst  nameLast\n11832 mccutan01    Andrew McCutchen\n\nplayerInfo(\"ramirjo01\")\n\n       playerID nameFirst nameLast\n14966 ramirjo01      Jose  Ramirez"
  },
  {
    "objectID": "posts/cofficient_of_variation/coefficient_of_variation.html#data-visualization",
    "href": "posts/cofficient_of_variation/coefficient_of_variation.html#data-visualization",
    "title": "Coefficient of Variation",
    "section": "Data Visualization",
    "text": "Data Visualization\nThis whole time, I was hoping for a neat boxplot.\n\ndf |&gt;\n  filter(playerID == \"mccutan01\" | playerID == \"ramirjo01\") |&gt;\n  ggplot(aes(x = playerID, y = HR)) +\n  geom_boxplot(color = c(\"#27251F\", \"#00385D\"),\n               fill = c(\"#FDB827\", \"#E50022\")) +\n  stat_summary(fun=mean, geom=\"point\", shape=20, size=14, color=\"blue\", fill=\"blue\") +\n  scale_x_discrete(labels = c(\"Andrew McCutchen\", \"Jose Ramirez\")) +\n  labs(title = \"Similar Means, Different Variances\",\n       subtitle = stringr::str_wrap(\"Andrew McCutchen and Jose Ramirez have averaged about 20.4 home runs per season, but in different ways (Qualifiers: after 2002 season, 100+ AB seasons, at least 5 100+ AB seasons, home run average over 15 HR/season)\"),\n       caption = \"Derek Sollberger, 2022-09-02\",\n       x = \"MLB Player\",\n       y = \"home runs in a season\") +\n  theme(axis.text.x = element_text(size = 15),\n        legend.position = \"none\",\n        panel.background = element_blank(),\n        plot.background = element_rect(\n            fill = \"#FFFFFF\", \n            color = \"#27251F\"\n        ),\n        plot.title = element_text(color = \"#E50022\", size = 20, hjust = 0.5),\n        plot.title.position = \"plot\",\n        plot.subtitle = element_text(color = \"blue\", size = 12, hjust = 0.0),\n        plot.caption = element_text(color = \"#092C5C\", size = 10, hjust = 1.0),\n        plot.caption.position = \"plot\",\n        plot.margin = margin(20, 20, 20, 20))"
  },
  {
    "objectID": "posts/Settlement_Survival/Settlement_Survival.html",
    "href": "posts/Settlement_Survival/Settlement_Survival.html",
    "title": "Settlement Survival Start",
    "section": "",
    "text": "One of my silly side-projects is to organize the dependency chart in the Settlement Survival video game into a mermaid diagram (or a similar flow chart). Today, I will start with the beginning of the game—that is, what tends to be built in the first “year”.\n\nlibrary(\"DiagrammeR\")\n\nI am going to take some liberties and actually add more prerequisites in the beginning (otherwise, the graph simply starts with about 12 unlinked nodes).\n\nmy_plot &lt;- DiagrammeR::mermaid(\"\n  graph TD\n  start[Select Location]\n  \n  house1[1 House]\n  farms[2 Standard Fields]\n  water[Big Well]\n  forest_hut[Forester's Hut]\n  gather_hut[Gatherer's Hut]\n  hunter_hut[Hunter's Hut]\n  chopping_house[Chopping House]\n  house7[7 Houses]\n  repair_shop[Repair Shop]\n  church[Church]\n  clinic[Clinic]\n  distillery[Distillery]\n  \n  start --&gt; house1\n  start --&gt; farms\n  start --&gt; water\n  start --&gt; gather_hut\n  \n  house1 --&gt; house7\n  farms --&gt; clinic\n  gather_hut --&gt; forest_hut\n  gather_hut --&gt; hunter_hut\n  water --&gt; repair_shop\n  \n  forest_hut --&gt; chopping_house\n  \n  house7 --&gt; church\n  repair_shop --&gt; distillery\n\")\n\n\n# print\nmy_plot"
  },
  {
    "objectID": "posts/patchwork/patchwork.html",
    "href": "posts/patchwork/patchwork.html",
    "title": "Patchwork",
    "section": "",
    "text": "In today’s entry, I am simply trying to remember how patchwork handles titles of each graph and an overall graph.\n\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.1\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ncorrelatedValues = function(x, r = 0.9){\n  r2 = r**2\n  ve = 1-r2\n  SD = sqrt(ve)\n  e  = rnorm(length(x), mean=0, sd=SD)\n  y  = r*x + e\n  return(y)\n}\n\nEarlier tonight, I was making these plots for a quick lecture about correlation, so let me just grab some copies.\n\nx &lt;- rnorm(100, mean = 0, sd = 1)\ny &lt;- correlatedValues(x, r = -0.9)\n\ncor_value &lt;- cor(x,y, use = \"pairwise.complete.obs\")\n\ndf_for_graph &lt;- data.frame(x,y)\np1 &lt;- df_for_graph |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(title = \"Correlation Example\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", strongly and negatively correlated\"),\n       caption = \"Spark 01\")\n\n\nx &lt;- rnorm(100, mean = 0, sd = 1)\ny &lt;- correlatedValues(x, r = -0.5)\n\ncor_value &lt;- cor(x,y, use = \"pairwise.complete.obs\")\n\ndf_for_graph &lt;- data.frame(x,y)\np2 &lt;- df_for_graph |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(title = \"Correlation Example\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", slightly and negatively correlated\"),\n       caption = \"Spark 01\")\n\n\nx &lt;- rnorm(100, mean = 0, sd = 1)\ny &lt;- correlatedValues(x, r = 0)\n\ncor_value &lt;- cor(x,y, use = \"pairwise.complete.obs\")\n\ndf_for_graph &lt;- data.frame(x,y)\np3 &lt;- df_for_graph |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(title = \"Correlation Example\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", virtually uncorrelated\"),\n       caption = \"Spark 01\")\n\n\nx &lt;- rnorm(100, mean = 0, sd = 1)\ny &lt;- correlatedValues(x, r = 0.5)\n\ncor_value &lt;- cor(x,y, use = \"pairwise.complete.obs\")\n\ndf_for_graph &lt;- data.frame(x,y)\np4 &lt;- df_for_graph |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(title = \"Correlation Example\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", slightly and positively correlated\"),\n       caption = \"Spark 01\")\n\n\nx &lt;- rnorm(100, mean = 0, sd = 1)\ny &lt;- correlatedValues(x, r = 0.9)\n\ncor_value &lt;- cor(x,y, use = \"pairwise.complete.obs\")\n\ndf_for_graph &lt;- data.frame(x,y)\np5 &lt;- df_for_graph |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(title = \"Correlation Example\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", strongly and positively correlated\"),\n       caption = \"Spark 01\")\n\nNow for the patchwork\n\n#patchwork\n(p1 + p2 + p3) / (p4 + p5)\n\n\n\n\nNow for the annotation.\n\noverall_plot &lt;- (p1 + p2 + p3) / (p4 + p5)\n\noverall_plot + plot_annotation(\n  title = \"Overall Title\",\n  subtitle = \"overall subtitle\",\n  caption = \"overall caption\"\n)\n\n\n\n\nGreat! Everything is working as planned."
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html",
    "href": "posts/wonderwall/wonderwall.html",
    "title": "Anyway Heres Wonderwall",
    "section": "",
    "text": "library(\"patchwork\")\nlibrary(\"tidyverse\")"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#frets",
    "href": "posts/wonderwall/wonderwall.html#frets",
    "title": "Anyway Heres Wonderwall",
    "section": "Frets",
    "text": "Frets\nI think I will start with the frets. (Disclaimer: I am very much just a beginner with playing guitar, and my descriptions may also be elementary.) I need to represent about 5 rows of space along with 6 columns of strings.\n\ndf &lt;- data.frame(x = 1:6, y = 0:5) #generic data\n\ndf |&gt;\n  ggplot(aes(x = x, y = y))\n\n\n\n\nGuitar chord diagrams are not necessarily scaled with equally spaced units in the horizontal and vertical directions, but that might make things easier for me in this recreational side project. Also, I will apply theme_linedraw temporarily to show what I am thinking.\n\ndf |&gt;\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  theme_linedraw()\n\n\n\n\nThe \\(y\\) axis has the math-class default, but guitar notation goes in the other direction.\n\n# https://stackoverflow.com/questions/70193132/how-to-reverse-y-axis-values-ggplot2-r\ndf |&gt;\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  scale_y_reverse() +\n  theme_linedraw()\n\n\n\n\nWhen I remove theme_linedraw (analogy: removing the training wheels from a bicycle), I will need to (at least)\n\nturn off the background color\nturn off the grid line colors\nturn off the border lines too\n\n\ndf |&gt;\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  scale_y_reverse() +\n  theme(\n    panel.background = element_blank()\n  )\n\n\n\n\nActually, let’s go ahead and give the background a “wood” color.\n\n# https://redketchup.io/color-picker\ndf |&gt;\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  scale_y_reverse() +\n  theme(\n    panel.background = element_rect(fill = \"#2D2C32\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nLet’s see if we can cutoff the plot at the top (“nut”) and right side of the plot.\n\ndf |&gt;\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  scale_y_reverse() +\n  theme(\n    panel.background = element_rect(fill = \"#2D2C32\"),\n    panel.grid.minor = element_blank()\n  ) +\n  xlim(c(1,6)) +\n  ylim(c(0,5))\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nThat did not work (and produced a warning because I affected the \\(y\\) axis twice), so perhaps a better strategy is to make a colorful rectangle manually.\n\ndf |&gt;\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  geom_rect(aes(xmin = 1, xmax = 6, ymin = 0, ymax = 5),\n            fill = \"#2D2C32\") +\n  scale_y_reverse() +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nThis whole time, I have been thinking about making line segments of certain colors for the frets, strings, and the nut. (Note: silver is not a base color in R??)\n\ndf_frets &lt;- data.frame(x1 = 1, x2 = 6, y = 1:5)\ndf_strings &lt;- data.frame(x = 1:6, y1 = 0, y2 = 5)\n\ndf |&gt;\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  geom_rect(aes(xmin = 1, xmax = 6, ymin = 0, ymax = 5),\n            fill = \"#2D2C32\") +\n  geom_segment(aes(x = x1, y = y, xend = x2, yend = y),\n            color = \"gray50\", data = df_frets) +\n  geom_segment(aes(x = 1, y = 0, xend = 6, yend = 0),\n               color = \"tan\", size = 3) +\n  geom_segment(aes(x = x, y = y1, xend = x, yend = y2),\n            color = \"#C0C0C0\", data = df_strings, size = 2) +\n  scale_y_reverse() +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nThis looks great in my opinion. Let’s turn off the axis tick marks and labels, and save this as a base plot.\n\nfret_background &lt;- df |&gt;\n  ggplot(aes(x = x, y = y)) +\n  coord_equal() +\n  geom_rect(aes(xmin = 1, xmax = 6, ymin = 0, ymax = 5),\n            fill = \"#2D2C32\") +\n  geom_segment(aes(x = x1, y = y, xend = x2, yend = y),\n            color = \"gray50\", data = df_frets) +\n  geom_segment(aes(x = 1, y = 0, xend = 6, yend = 0),\n               color = \"tan\", size = 3) +\n  geom_segment(aes(x = x, y = y1, xend = x, yend = y2),\n            color = \"#C0C0C0\", data = df_strings, size = 2) +\n  scale_y_reverse() +\n  theme(\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    panel.background = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n# print\nfret_background"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#first-chord-em",
    "href": "posts/wonderwall/wonderwall.html#first-chord-em",
    "title": "Anyway Heres Wonderwall",
    "section": "First Chord (Em)",
    "text": "First Chord (Em)\nAt first, I was thinking of using geom_label to indicate the finger positions, but I bet that using geom_point and then geom_text has more customization options.\n\ndf_em &lt;- data.frame(x = c(2,3), y = c(2,2) - 0.5, label = c(\"1\", \"2\"))\nfret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_em, size = 15) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_em, size = 10)\n\n\n\n\nLet us go ahead and label this chord. Note that I want the dots to appear in between the frets (hence the “-0.5” in the \\(y\\) coordinates).\n\ndf_em &lt;- data.frame(x = c(2,3), y = c(2,2) - 0.5, label = c(\"1\", \"2\"))\n\nchord_em &lt;- fret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_em, size = 12) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_em, size = 8) +\n  labs(title = \"Em\") +\n  theme(plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\n\n# print\nchord_em"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#the-other-four-chords",
    "href": "posts/wonderwall/wonderwall.html#the-other-four-chords",
    "title": "Anyway Heres Wonderwall",
    "section": "The Other Four Chords",
    "text": "The Other Four Chords\n\ndf_g &lt;- data.frame(x = c(2,1,6), y = c(2,3,3) - 0.5, label = c(\"1\", \"2\", \"4\"))\n\nchord_g &lt;- fret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_g, size = 12) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_g, size = 8) +\n  labs(title = \"G\") +\n  theme(plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\n\n# print\nchord_g\n\n\n\n\nThis is something that I worried about. In guitar chord diagrams, there are indicators to tell the player if a string is played open (indicated with an “O”, that is that no fingers are placed on the strings while the string is strung) or if a string is closed (not strung at all, indicated with an “X”), but these indicators are placed above the nut—for our purposes: outside of the graph.\n\ndf_d &lt;- data.frame(x = c(4,6,5), y = c(2,2,3) - 0.5, label = c(\"1\", \"2\", \"3\"))\n\nchord_d &lt;- fret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_d, size = 12) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_d, size = 8) +\n  geom_text(aes(x = 1, y = -0.2, label = \"X\"), size = 5) +\n  geom_text(aes(x = 2, y = -0.2, label = \"X\"), size = 5) +\n  labs(title = \"D\") +\n  theme(plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\n\n# print\nchord_d\n\n\n\n\n\ndf_a7sus4 &lt;- data.frame(x = c(3,5), y = c(2,3) - 0.5, label = c(\"1\", \"2\"))\n\nchord_a7sus4 &lt;- fret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_a7sus4, size = 12) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_a7sus4, size = 8) +\n  geom_text(aes(x = 1, y = -0.2, label = \"X\"), size = 5) +\n  labs(title = \"A7sus4\") +\n  theme(plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\n\n# print\nchord_a7sus4\n\n\n\n\n\ndf_c &lt;- data.frame(x = c(5,3,2), y = c(1,2,3) - 0.5, label = c(\"1\", \"2\", \"3\"))\n\nchord_c &lt;- fret_background +\n  geom_point(aes(x = x, y = y),\n             color = \"blue4\", data = df_c, size = 12) +\n  geom_text(aes(x = x, y = y, label = label),\n             color = \"tan\", data = df_c, size = 8) +\n  geom_text(aes(x = 1, y = -0.2, label = \"X\"), size = 5) +\n  labs(title = \"C\") +\n  theme(plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\n\n# print\nchord_c"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#first-draft",
    "href": "posts/wonderwall/wonderwall.html#first-draft",
    "title": "Anyway Heres Wonderwall",
    "section": "First Draft",
    "text": "First Draft\nWith 5 chords, I am imagining a layout of plots with about 2 rows.\n\n(chord_em + chord_g + chord_d) / (chord_a7sus4 + chord_c)"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#a-caption-tile",
    "href": "posts/wonderwall/wonderwall.html#a-caption-tile",
    "title": "Anyway Heres Wonderwall",
    "section": "A Caption Tile",
    "text": "A Caption Tile\nYes, I figured that I would want to squeeze in a “square” of text somewhere to even out the image.\n\n# https://statisticsglobe.com/plot-only-text-in-r\ncaption_plot &lt;- ggplot() +\n  annotate(\"text\",\n           x = 1, y = 1,\n           size = 5,\n           label = \"Wonderwall by Oasis\\n \\nsimplified tab\\nvia Ultimate Guitar\\ncapo: 2nd fret\\nkey: F#m\") +\n  coord_equal() +\n  theme(\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    panel.background = element_rect(fill = \"#2D2C32\"),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n# print\ncaption_plot\n\n\n\n\nNow to see if this “caption plot” fits in nicely.\n\n(chord_em + chord_g + chord_d) / (chord_a7sus4 + caption_plot +chord_c)\n\n\n\n\nAt the time of this writing, it is near midnight, and my motivation for refinement is decreasing rapidly, lol.\n\nmain_plot &lt;- (chord_em + chord_g + chord_d) / (chord_a7sus4 + caption_plot +chord_c)"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#strumming-pattern",
    "href": "posts/wonderwall/wonderwall.html#strumming-pattern",
    "title": "Anyway Heres Wonderwall",
    "section": "Strumming Pattern",
    "text": "Strumming Pattern\nI, and I assume other beginners, do not natually pick up a strumming pattern by ear. Fortunately, the tab did include that. But how do I add the pattern to this diagram?\n\n# https://community.rstudio.com/t/up-and-down-arrows-inside-r/76287\n\n# first, I will type into words, and then do a search-and-replace\n# down, pause, down, pause, down, pause, down, up, down, up, down, pause, down, pause, down, up, down, up, down, pause, down, pause, down, up, pause, up, pause, up, down, pause, down, pause\nstrumming_pattern &lt;- intToUtf8(c(8595, 8196, 8595, 8196, 8595, 8196, 8595, 8593, 8595, 8593, 8595, 8196, 8595, 8196, 8595, 8593, 8595, 8593, 8595, 8196, 8595, 8196, 8595, 8593, 8196, 8593, 8196, 8593, 8595, 8196, 8595, 8196))"
  },
  {
    "objectID": "posts/wonderwall/wonderwall.html#annotationg-and-echoing-the-meme",
    "href": "posts/wonderwall/wonderwall.html#annotationg-and-echoing-the-meme",
    "title": "Anyway Heres Wonderwall",
    "section": "Annotationg and Echoing the Meme",
    "text": "Annotationg and Echoing the Meme\nWith patchwork, we can furthermore add a title, subtitle, and/or caption to the overall plot.\n\nfinal_plot &lt;- main_plot +\n  plot_annotation(title = \"Anyway, Here's Wonderwall\",\n                  caption = paste(\"strumming pattern:\", strumming_pattern, \"\\nGraph by @DerekSollberger\"),\n                  theme = theme(plot.title = element_text(\n                    size = 25, hjust = 0.5\n                  ),\n                  plot.caption = element_text(\n                    size = 20, hjust = 0.5\n                  )))\n\n# print\nfinal_plot\n\n\n\n\n\nggsave(\"wonderwall.png\", plot = final_plot, device = \"png\",\n       width = 8, height = 6, units = \"in\")\n\n\n\n\nmeme chart"
  },
  {
    "objectID": "posts/Canvas_Roster/Canvas_Roster.html",
    "href": "posts/Canvas_Roster/Canvas_Roster.html",
    "title": "Canvas_Roster",
    "section": "",
    "text": "With the goal of learning my students’ names faster, I wrote an R script that will convert a roster (“People” in a Canvas course) to a gt table for later offline use.\n\nknitr::opts_chunk$set(eval = FALSE)\n\n\nSetup\nIn your Canvas course, go to People, right-click on the roster, and click “Save Page As”\n\nbe sure to save the “complete” web page (this creates a folder with all of the students’ pictures)\nplace the also downloaded HTML file into the folder\nplace this code script into the folder\n\n\n# change this file name as needed\nHTML_file_name &lt;- \"Course Roster S23-BIO 018 01.htm\"\n\nlibrary(\"gt\")\nlibrary(\"gtExtras\")\nlibrary(\"rvest\")\nlibrary(\"tidyverse\")\n\n\n\nRoster table\nhttps://www.r-bloggers.com/2020/04/scrape-html-table-using-rvest/\n\ncontent &lt;- read_html(HTML_file_name)\ntables &lt;- content |&gt; html_table(fill = TRUE)\nroster_table &lt;- tables[[1]]\n\nroster_df &lt;- roster_table |&gt;\n  select(Name, Section) |&gt;\n  mutate(discussion_tag = str_extract(Section, \"Discussion-\\\\d\\\\dD\")) |&gt;\n  separate(discussion_tag, sep = \"-\", into = c(\"disregard\", \"discussion_section\")) |&gt;\n  select(Name, discussion_section) |&gt;\n  \n  # remove instructor of record\n  filter(!is.na(discussion_section))\n\n\n\nImages\nhttps://community.rstudio.com/t/scraping-images-from-the-web/133239\n\nimage_urls &lt;- content |&gt; html_elements(\"img\")\nnum_images &lt;- length(image_urls)\n\nimage_df_raw &lt;- data.frame(html_raw = rep(NA, num_images))\nfor(img in 1:num_images){\n  image_df_raw$html_raw[img] &lt;- as.character(image_urls[[img]])\n}\n\nimage_df &lt;- image_df_raw |&gt;\n  \n  # skip first two images?\n  slice(3:n()) |&gt;\n  \n  # roundabout because some people's names have more than 2 words\n  separate(col = \"html_raw\", sep = \"alt=\", \n           into = c(\"part1\", \"part2\")) |&gt;\n  \n  separate(col = \"part1\", sep = \" \", into = c(\"tag1\", \"src\")) |&gt;\n  separate(col = \"src\", sep = \"/\", into = c(\"stem\", \"profile_picture\")) |&gt;\n  separate(col = \"part2\", sep = \" aria\", into = c(\"student_name\", \"tag2\")) |&gt;\n  \n  select(profile_picture, student_name)\n\n# remove quotation marks\nimage_df$profile_picture &lt;- str_replace_all(image_df$profile_picture, \"\\\"\", \"\")\nimage_df$student_name &lt;- str_replace_all(image_df$student_name, \"\\\"\", \"\")\n\n\n\nMerge\n\nroster_df &lt;- roster_df |&gt;\n  left_join(image_df, by = c(\"Name\" = \"student_name\")) |&gt;\n  select(profile_picture, Name, discussion_section)\n\n\n\ngt\nhttps://jthomasmock.github.io/gtExtras/reference/gt_img_rows.html\n\nroster_gt &lt;- roster_df |&gt;\n  gt() |&gt;\n  gt_img_rows(columns = profile_picture, img_source = \"local\")\n\n\ngtsave(roster_gt, \"roster.png\")"
  },
  {
    "objectID": "posts/California_weather/California_weather.html",
    "href": "posts/California_weather/California_weather.html",
    "title": "California Weather",
    "section": "",
    "text": "library(\"tidyverse\")\n\nI want to create and visualize a simple data set for my Data Science courses (that I teach in California).\n\nData Source\n\nUniversity of California\nAgriculture and Natural Resources\nStatewide Integrated Pest Management Program\nhttps://ipm.ucanr.edu/WEATHER/wxactstnames.html\n\n\n\nFixed-Width Files\nToday I learned how to read fixed-width files in the Tidyverse. From there, I simply need to give the columns easy-to-use names.\n\nLA_df &lt;- readr::read_fwf(\"LA_2022.txt\")\n\nRows: 365 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\n\nchr  (4): X1, X4, X7, X9\ndbl  (4): X3, X5, X6, X8\ntime (1): X2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(LA_df) &lt;- c(\"date\", \"time\", \"precipitation\",\n                     \"check1\", \"high\", \"low\", \"check2\", \"solar\", \"check3\")\nLA_df$city &lt;- \"Los Angeles\"\n\n\nMerced_df &lt;- readr::read_fwf(\"Merced_2022.txt\")\n\nRows: 365 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\n\nchr   (4): X1, X4, X7, X12\ndbl  (11): X3, X5, X6, X8, X9, X10, X11, X13, X14, X15, X16\ntime  (1): X2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(Merced_df) &lt;- c(\"date\", \"time\", \"precipitation\",\n                         \"check1\", \"high\", \"low\", \"check2\")\nMerced_df$city &lt;- \"Merced\"\n\n\nSF_df &lt;- readr::read_fwf(\"SF_2022.txt\")\n\nRows: 365 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\n\nchr  (3): X1, X4, X7\ndbl  (3): X3, X5, X6\ntime (1): X2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(SF_df) &lt;- c(\"date\", \"time\", \"precipitation\",\n                     \"check1\", \"high\", \"low\", \"check2\")\nSF_df$city &lt;- \"San Francisco\"\n\n\n\nMerge\nSome of the weather stations had collected more information than others. That is, if the weather station was newer, then it had more instruments.\nFor today’s quick exploration, I actually do want to perform a quick rbind, and that requires that each of the 3 data frames have the same number of columns (and should be the same types of information too).\n\nLA_df &lt;- LA_df |&gt;\n  select(city, date, time, high, low, precipitation)\nMerced_df &lt;- Merced_df |&gt;\n  select(city, date, time, high, low, precipitation)\nSF_df &lt;- SF_df |&gt;\n  select(city, date, time, high, low, precipitation)\n\nCA_weather_data &lt;- rbind(LA_df, Merced_df, SF_df)\n\n\n# write_csv(CA_weather_data, \"CA_weather_data.csv\")\n\n\n\nData Viz\nNow, boxplots are easy to make.\n\nCA_weather_data |&gt;\n  ggplot(aes(y = high)) +\n  geom_boxplot() +\n  labs(title = \"California Weather, High Temperatures\",\n       subtitle = \"(all together)\",\n       caption = \"Source: UC\\nAgriculture and Natural Resources\\nStatewide Integrated Pest Management Program\")\n\n\n\n\n\nCA_weather_data |&gt;\n  ggplot(aes(x = city, y = high, fill = city)) +\n  geom_boxplot() +\n  labs(title = \"California Weather, High Temperatures\",\n       subtitle = \"(separate groups)\",\n       caption = \"Source: UC\\nAgriculture and Natural Resources\\nStatewide Integrated Pest Management Program\")\n\n\n\n\n\n\nSample\nFor the creation of a classroom example, I want to randomly select 43 observations from the Merced data.\n\nMerced_sample &lt;- sort(sample(Merced_df$high, 43, replace = FALSE))\ndput(Merced_sample)\n\nc(53, 53, 55, 58, 58, 60, 60, 61, 62, 65, 67, 68, 70, 70, 71, \n72, 74, 75, 77, 82, 82, 83, 84, 84, 87, 87, 88, 90, 91, 91, 92, \n92, 93, 93, 94, 95, 96, 96, 98, 99, 101, 101, 105)"
  },
  {
    "objectID": "posts/Christmas_plots/Christmas_plots.html",
    "href": "posts/Christmas_plots/Christmas_plots.html",
    "title": "Seasons Greetings by Riinu Pius",
    "section": "",
    "text": "a place to collect nerdy holiday plots\n\nlibrary(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n# https://gist.github.com/riinuots/52c0089bd5c41a44389e95cc8cb3943d\none = tibble(x    = c(2, 4, 3, 2.75, 2.75, 3.25, 3.25),\n             y    = c(1, 1, 6, 0,    1,    1,    0),\n            group = c(rep(\"#005a32\", 3), rep(\"#543005\", 4)))\n              \nggplot(one, aes(x, y, fill = group)) +\n  geom_polygon()\n\n\n\n\n\ntwo = tribble(~x,   ~y,\n              2.7,  2,\n              3,    3,\n              3.4,  1.6,\n              3.5,  2.5,\n              3.1,  4,\n              2.95, 5,\n              2.7,  3.7,\n              2.4,  2.45,\n              2.35, 1.7,\n              3.1,  2.1,\n              3.2,  3.1,\n              2.75, 3,\n              2.9, 1.4,\n              2.9, 4.4) %&gt;% \n  mutate(group = \"gold\")\n\nggplot(one, aes(x, y, fill = group)) +\n  geom_polygon() +\n  geom_point(data = two, shape = 21, size = 5)"
  },
  {
    "objectID": "posts/greenhouse/greenhouse.html",
    "href": "posts/greenhouse/greenhouse.html",
    "title": "Stardew Valley Crops",
    "section": "",
    "text": "Here is some quick code to help me plan a crop layout in Stardew Valley.\n\nlibrary(\"figpatch\") #https://bradyajohnston.github.io/figpatch/\nlibrary(\"patchwork\")\n\n\n# load images\n# https://stardewvalleywiki.com/Crops\nancient_fruit &lt;- figpatch::fig(\"Ancient_Fruit.png\")\nblueberry &lt;- figpatch::fig(\"Blueberry.png\")\ncactus_fruit &lt;- figpatch::fig(\"Cactus_Fruit.png\")\ncoffee_bean &lt;- figpatch::fig(\"Coffee_Bean.png\")\ncorn &lt;- figpatch::fig(\"Corn.png\")\ncranberries &lt;- figpatch::fig(\"Cranberries.png\")\ngrape &lt;- figpatch::fig(\"Grape.png\")\ngreen_bean &lt;- figpatch::fig(\"Green_Bean.png\")\nhops &lt;- figpatch::fig(\"Hops.png\")\nhot_pepper &lt;- figpatch::fig(\"Hot_Pepper.png\")\npineapple &lt;- figpatch::fig(\"Pineapple.png\")\nstrawberry &lt;- figpatch::fig(\"Strawberry.png\")\ntea_leaves &lt;- figpatch::fig(\"Tea_Leaves.png\")\ntomato &lt;- figpatch::fig(\"Tomato.png\")\n\n\ngreenhouse_plot &lt;- patchwork::wrap_plots(\n  \n  # row 1\n  grape, hops, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper, hot_pepper,\n  \n  # row 2\n  grape, hops, tomato, tomato, tomato, tomato, tomato, tomato, tomato, tomato, tomato, tomato, \n  \n  # row 3\n  grape, tea_leaves, cranberries, cranberries, cranberries, cranberries, cranberries, cranberries, cranberries, cranberries, cranberries, cranberries, \n  \n  # row 4\n  grape, tea_leaves, strawberry, strawberry, strawberry, strawberry, strawberry, strawberry, strawberry, strawberry, strawberry, strawberry, \n  \n  # row 5\n  grape, tea_leaves, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, cactus_fruit, \n  \n  # row 6\n  green_bean, tea_leaves, corn, corn, corn, corn, corn, corn, corn, corn, corn, corn, \n  \n  # row 7\n  green_bean, tea_leaves, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, ancient_fruit, \n  \n  # row 8\n  green_bean, tea_leaves, blueberry, blueberry, blueberry, blueberry, blueberry, blueberry, blueberry, blueberry, blueberry, blueberry, \n  \n  # row 9\n  green_bean, hops, pineapple, pineapple, pineapple, pineapple, pineapple, pineapple, pineapple, pineapple, pineapple, pineapple, \n  \n  # row 10\n  green_bean, hops, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, coffee_bean, \n  \n  ncol = 12\n)\n\n\ngreenhouse_plot +\n  patchwork::plot_annotation(title = \"Stardew Valley\",\n                  subtitle = \"crops that continue to produce\")"
  },
  {
    "objectID": "posts/curly_operator/curly_operator.html",
    "href": "posts/curly_operator/curly_operator.html",
    "title": "curly operator",
    "section": "",
    "text": "The “curly operator” was added in rlang a few years ago, and I have yet to really use it much. It came in handy during a data analyst consulting gig I had during the summer of 2021, and I should use it more.\n\nlibrary(\"palmerpenguins\")\nlibrary(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nFor instance, can I make a helper function to reduce typing out the same few lines of code that I use often?\n\nsummary_stats &lt;- function(data_frame, grouping_variable, numerical_variable){\n  data_frame |&gt;\n    filter(!is.na({{grouping_variable}})) |&gt;\n    group_by({{grouping_variable}}) |&gt;\n    summarize(min = min({{numerical_variable}}, na.rm = TRUE),\n            xbar = mean({{numerical_variable}}, na.rm = TRUE),\n            med = median({{numerical_variable}}, na.rm = TRUE),\n            s = sd({{numerical_variable}}, na.rm = TRUE),\n            max = max({{numerical_variable}}, na.rm = TRUE))\n}\n\n\nsummary_stats(penguins, species, bill_length_mm)\n\n# A tibble: 3 × 6\n  species     min  xbar   med     s   max\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     32.1  38.8  38.8  2.66  46  \n2 Chinstrap  40.9  48.8  49.6  3.34  58  \n3 Gentoo     40.9  47.5  47.3  3.08  59.6\n\n\n\nsummary_stats(penguins, island, body_mass_g)\n\n# A tibble: 3 × 6\n  island      min  xbar   med     s   max\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Biscoe     2850 4716. 4775   783.  6300\n2 Dream      2700 3713. 3688.  417.  4800\n3 Torgersen  2900 3706. 3700   445.  4700"
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html",
    "title": "JupyterHub Showcase",
    "section": "",
    "text": "library(\"palmerpenguins\")\nlibrary(\"tidyverse\")"
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html#login",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html#login",
    "title": "JupyterHub Showcase",
    "section": "Login",
    "text": "Login\nAt UC Merced, our 2i2c server is located at\n\nhttps://ucmerced.2i2c.cloud\n\nEach user sees a shared and a shared-readwrite folder\n\n\n\nshared and shared-readwrite"
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html#file-structure",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html#file-structure",
    "title": "JupyterHub Showcase",
    "section": "File Structure",
    "text": "File Structure\nFor this demonstration, I have made the following directories in the root directory.\n\nExampleCourse_instructor\nExampleCourse_student\n\nalong with an ExampleCourse inside the shared-readwrite directory.\n\n\n\nfile_structure"
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html#making-an-assignment",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html#making-an-assignment",
    "title": "JupyterHub Showcase",
    "section": "Making an Assignment",
    "text": "Making an Assignment\nThis semester, I taught classes with the R programming language, and here I will continue to use R. I start my work inside of the ExampleCourse-instructor directory. We will create an assignment inside of a Jupyter notebook, but with an R kernel. In other words, click on the “R” button under “Notebook”\nThis script has been named HW1.ipynb.\nThe 2i2c server comes with the Otter Grader tools built by data science faculty at UC Berkeley. Instructors can highly customize the functionality of these files with the initialization cell (a raw code block).\n\n\n\ninitialization\n\n\nMy colleagues and I have discussed ways to ease beginning students into the skills of installing code packages. For the sake of visual brevity, here I will assume that the code packages have been installed already.\n\n# code packages\nlibrary(\"palmerpenguins\")\nlibrary(\"tidyverse\")\n\n\nstr(penguins)\n\n\nPrompts\nInstructions for the student can be made with normal typing augmented by markdown as this notebook environment is built for literate programming.\n\n\n\ninstructions are typed in markdown\n\n\n\n# HW1\n\nIn this assignment, we will build a custom function to compute sample statistics.  Pay attention to the usage of the curly braces `{{...}}`, and we will use the functions on the `palmerpenguins` data set\n\n\n1. Write a custom function called `summary_stats` that takes 3 inputs\n\n    * data_frame\n    * grouping_variable\n    * numerical_variable\n\nand outputs the `summarize` command on the following sample statistics: minimum, mean, median, standard deviation, and maximum.  Please follow the given stencil.\n\n\n\nWriting a Problem\nInside of a code cell, an instructor can type in the intended answer between # BEGIN SOLUTION and # END SOLUTION comments. From there, what the student will see falls between the # BEGIN PROMPT and # END PROMPT comments.\n\n\n\nan example coding task (from the instructors point of view)\n\n\n\n1. Write a custom function called `summary_stats` that takes 3 inputs\n\n    * data_frame\n    * grouping_variable\n    * numerical_variable\n\nand outputs the `summarize` command on the following sample statistics: minimum, mean, median, standard deviation, and maximum.  Please follow the given stencil.\n\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats &lt;- function(data_frame, grouping_variable, numerical_variable){\n  data_frame |&gt;\n    filter(!is.na({{grouping_variable}})) |&gt;\n    group_by({{grouping_variable}}) |&gt;\n    summarize(min = min({{numerical_variable}}, na.rm = TRUE),\n            xbar = mean({{numerical_variable}}, na.rm = TRUE),\n            med = median({{numerical_variable}}, na.rm = TRUE),\n            s = sd({{numerical_variable}}, na.rm = TRUE),\n            max = max({{numerical_variable}}, na.rm = TRUE))\n}\n# END SOLUTION\n. = \" # BEGIN PROMPT\nsummary_stats &lt;- function(data_frame, grouping_variable, numerical_variable){\n  data_frame |&gt;\n    filter(!is.na({{grouping_variable}})) |&gt;\n    group_by({{grouping_variable}}) |&gt;\n    summarize(min = min({{numerical_variable}}, na.rm = TRUE),\n            xbar = _____,\n            med = _____,\n            s = _____,\n            max = _____)\n}\n\" # END PROMPT\n\n\n2. Use your `summary_stats` function with the `penguins` data frame, grouped by the `species` categorical variable, on the `bill_length_mm` numerical variable.\n\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats(penguins, species, bill_length_mm)\n# END SOLUTION\n. = \" # BEGIN PROMPT\nsummary_stats(penguins, _____, _____)\n\" # END PROMPT\n\n\n3. Use your `summary_stats` function with the `penguins` data frame, grouped by the `island` categorical variable, on the `body_mass_g` numerical variable.\n\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats(penguins, island, body_mass_g)\n# END SOLUTION\n. = \" # BEGIN PROMPT\n\n\" # END PROMPT\n\n\n\nTest That\nAdvanced R programmers, especially those that make code packages, use the testthat package to create unit tests to verify that functions are working as intended. The Otter Grader framework continues this idea for making assignments in R.\n\n# If you want to check your code right now, uncomment the following line of code and run it\n# testthat::expect_equal(summary_stats(penguins, island, body_mass_g)$s[1], 782.8557, tol = 0.01)"
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html#assigning-the-assignment",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html#assigning-the-assignment",
    "title": "JupyterHub Showcase",
    "section": "Assigning the Assignment",
    "text": "Assigning the Assignment\nNow we will show the power of Otter Assign! Inside JupyterHub, open a terminal connection\n\nFile –&gt; New –&gt; Terminal\n\nUse the cd Unix command to navigate to the instructor files.\ncd ExampleCourse_instructor/\nSince our assignment is called HW1.ipynb, we will assign that notebook into the shared-readwrite directory. Tip: it is also a good idea to create a new directory for each homework assignment (to later manage solution files and student submissions).\notter assign HW1.ipynb ../shared-readwrite/ExampleCourse/HW1\n\n\n\nOtter Assign\n\n\nVerify that the HW1 directory was created within shared-readwrite/ExampleCourse."
  },
  {
    "objectID": "posts/JupyterHub_showcase/JupyterHub_showcase.html#student-view",
    "href": "posts/JupyterHub_showcase/JupyterHub_showcase.html#student-view",
    "title": "JupyterHub Showcase",
    "section": "Student View",
    "text": "Student View\nWe also find our HW1 directory inside the shared directory. The shared directory is in a read-only state, so students will not be able to edit and save their work there.\nThe easiest route is for a student to download the HW1.ipynb file and then upload it into their ExampleCourse_student directory. Advanced users can use Unix commands for this copy.\ncp shared/ExampleCourse/HW1/student/HW1.ipynb ExampleCourse_student\nNotice how the student receives only the partial prompts and is ready for some homework!\n\n\n\nstudent view"
  },
  {
    "objectID": "posts/Dartmouth_Atlas_data/Dartmouth_Atlas_data.html",
    "href": "posts/Dartmouth_Atlas_data/Dartmouth_Atlas_data.html",
    "title": "hospital_data",
    "section": "",
    "text": "https://data.dartmouthatlas.org/\n\nlibrary(\"sf\")\n\nWarning: package 'sf' was built under R version 4.2.3\n\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(\"tidyverse\")\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nmap_data &lt;- sf::read_sf(\"HRR_Bdry__AK_HI_unmodified/hrr-shapefile/Hrr98Bdry_AK_HI_unmodified.shp\")\n\nmap_data |&gt;\n  ggplot() +\n  geom_sf()"
  },
  {
    "objectID": "posts/JupyterHub_showcase/HW1.html",
    "href": "posts/JupyterHub_showcase/HW1.html",
    "title": "The Median Data Scientist",
    "section": "",
    "text": "BEGIN ASSIGNMENT init_cell: false export_cell: true\n\n# code packages\nlibrary(\"palmerpenguins\")\nlibrary(\"tidyverse\")\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\n\nHW1\nIn this assignment, we will build a custom function to compute sample statistics. Pay attention to the usage of the curly braces {{...}}, and we will use the functions on the palmerpenguins data set\n\nWrite a custom function called summary_stats that takes 3 inputs\n\ndata_frame\ngrouping_variable\nnumerical_variable\n\n\nand outputs the summarize command on the following sample statistics: minimum, mean, median, standard deviation, and maximum. Please follow the given stencil.\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats <- function(data_frame, grouping_variable, numerical_variable){\n  data_frame |>\n    filter(!is.na({{grouping_variable}})) |>\n    group_by({{grouping_variable}}) |>\n    summarize(min = min({{numerical_variable}}, na.rm = TRUE),\n            xbar = mean({{numerical_variable}}, na.rm = TRUE),\n            med = median({{numerical_variable}}, na.rm = TRUE),\n            s = sd({{numerical_variable}}, na.rm = TRUE),\n            max = max({{numerical_variable}}, na.rm = TRUE))\n}\n# END SOLUTION\n. = \" # BEGIN PROMPT\nsummary_stats <- function(data_frame, grouping_variable, numerical_variable){\n  data_frame |>\n    filter(!is.na({{grouping_variable}})) |>\n    group_by({{grouping_variable}}) |>\n    summarize(min = min({{numerical_variable}}, na.rm = TRUE),\n            xbar = _____,\n            med = _____,\n            s = _____,\n            max = _____)\n}\n\" # END PROMPT\n\n\nUse your summary_stats function with the penguins data frame, grouped by the species categorical variable, on the bill_length_mm numerical variable.\n\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats(penguins, species, bill_length_mm)\n# END SOLUTION\n. = \" # BEGIN PROMPT\nsummary_stats(penguins, _____, _____)\n\" # END PROMPT\n\n\n\nA tibble: 3 × 6\n\n    speciesminxbarmedsmax\n    <fct><dbl><dbl><dbl><dbl><dbl>\n\n\n    Adelie   32.138.7913938.802.66340546.0\n    Chinstrap40.948.8338249.553.33925658.0\n    Gentoo   40.947.5048847.303.08185759.6\n\n\n\n\n\nUse your summary_stats function with the penguins data frame, grouped by the island categorical variable, on the body_mass_g numerical variable.\n\n\n# BEGIN SOLUTION NO PROMPT\nsummary_stats(penguins, island, body_mass_g)\n# END SOLUTION\n. = \" # BEGIN PROMPT\n\n\" # END PROMPT\n\n\n\nA tibble: 3 × 6\n\n    islandminxbarmedsmax\n    <fct><int><dbl><dbl><dbl><int>\n\n\n    Biscoe   28504716.0184775.0782.85576300\n    Dream    27003712.9033687.5416.64414800\n    Torgersen29003706.3733700.0445.10794700\n\n\n\n\n\n# If you want to check your code right now, uncomment the following line of code and run it\n# testthat::expect_equal(summary_stats(penguins, island, body_mass_g)$s[1], 782.8557, tol = 0.01)\n\n\n\nSubmission\nOnce you are done with the tasks above,\n\nGo to “File”\nClick “Download as”\nDownload as “Notebook (.ipynb)\n\nThat will download a copy of this notebook onto your computer (probably into your Downloads folder). Please upload the .ipynb file back into our CatCourses site."
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#math-biology-video-project-1",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#math-biology-video-project-1",
    "title": "Math Biology Video Project",
    "section": "",
    "text": "Ecology course (prereq: Intro Bio)\nFall 2018 semester\n91 students (85 students in study)\n24 videos\n\nabout 10 minutes per video\n\n\n\n\n\n\nimage credit: Shutterstock"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#learner-profile",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#learner-profile",
    "title": "Math Biology Video Project",
    "section": "",
    "text": "math background\n\n\n\n\nmostly sophomores\nprior experience in flipped classrooms\nmostly White and Asian students\n77 percent female"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#literature",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#literature",
    "title": "Math Biology Video Project",
    "section": "",
    "text": "ValuesValues 2EmotionsEmotions 2\n\n\n\n\n\nAndrews 2017\n\n\n\n\n\n\n\nMath Biology Values Instrument\n\n\n\n\n\n\n\nWachsmuth 2017\n\n\n\n\n\n\n\nMath Emotions Instrument"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#fall-2018-semester",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#fall-2018-semester",
    "title": "Math Biology Video Project",
    "section": "",
    "text": "Our first glance at the survey data showed no significant results between the pre and post surveys."
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#discretize",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#discretize",
    "title": "Math Biology Video Project",
    "section": "",
    "text": "We grouped students into “unchanged”, “increase”, or “decrease” groups based on their pre- and post-semester survey results for the MBVI queries—where “unchanged” was a difference of -1, 0, or 1 on the 7-point Likert scales"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#math-content",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#math-content",
    "title": "Math Biology Video Project",
    "section": "",
    "text": "One VideoAll Videos"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#math-emotion",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#math-emotion",
    "title": "Math Biology Video Project",
    "section": "Math Emotion",
    "text": "Math Emotion"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#mbvi",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#mbvi",
    "title": "Math Biology Video Project",
    "section": "",
    "text": "Math Biology Values Instrument"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#video-stats",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#video-stats",
    "title": "Math Biology Video Project",
    "section": "",
    "text": "PlaysVisitsPersistence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo some students stop watching videos across term?"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#multiple-views",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#multiple-views",
    "title": "Math Biology Video Project",
    "section": "Multiple Views",
    "text": "Multiple Views\n\nQueryRPos CoeffNeg Coeff\n\n\nWhat influences students to view students multiple times?\n\n\n\n\n\nCall:\nglm(formula = multiple_plays ~ intrigue_cat + fun_cat + appeals_cat + \n    interesting_cat + valuable_cat + important_cat + essential_cat + \n    useful_cat + work_harder_cat + worry_cat + intimidate_cat + \n    easy_hard + complicated_simple + confusing_clear + comfortable_uncomfortable + \n    satisfying_frustrating + challenging_not_challenging + pleasant_unpleasant + \n    chaotic_organized + GPA + pct_math + video_length + video_number, \n    family = \"binomial\", data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3229  -0.9405  -0.4840   1.0090   2.2857  \n\nCoefficients: (1 not defined because of singularities)\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -2.764e+00  1.859e+00  -1.487 0.137076    \nintrigue_catincrease         1.150e-01  1.302e+00   0.088 0.929576    \nintrigue_catdecrease         1.370e+00  1.173e+00   1.168 0.242842    \nfun_catincrease             -8.922e-01  5.231e-01  -1.705 0.088116 .  \nfun_catdecrease              2.311e+00  7.322e-01   3.156 0.001598 ** \nappeals_catincrease          2.869e-01  6.281e-01   0.457 0.647773    \nappeals_catdecrease         -1.698e+00  6.447e-01  -2.634 0.008434 ** \ninteresting_catincrease      1.707e+00  6.576e-01   2.595 0.009462 ** \ninteresting_catdecrease     -2.212e+00  4.147e-01  -5.334 9.62e-08 ***\nvaluable_catincrease        -1.199e+00  7.577e-01  -1.582 0.113597    \nvaluable_catdecrease        -1.618e+01  5.354e+02  -0.030 0.975895    \nimportant_catincrease       -8.767e-01  3.964e-01  -2.212 0.026967 *  \nimportant_catdecrease       -1.594e+01  5.354e+02  -0.030 0.976256    \nessential_catincrease       -3.513e-01  4.322e-01  -0.813 0.416374    \nessential_catdecrease        1.932e+01  5.354e+02   0.036 0.971210    \nuseful_catincrease           4.869e-01  4.568e-01   1.066 0.286416    \nuseful_catdecrease                  NA         NA      NA       NA    \nwork_harder_catincrease      2.774e-01  2.685e-01   1.033 0.301604    \nwork_harder_catdecrease     -2.725e-01  4.216e-01  -0.647 0.517945    \nworry_catincrease            5.338e-01  3.500e-01   1.525 0.127149    \nworry_catdecrease            9.584e-01  3.564e-01   2.689 0.007159 ** \nintimidate_catincrease      -1.419e+00  4.625e-01  -3.068 0.002155 ** \nintimidate_catdecrease      -1.469e+00  3.275e-01  -4.485 7.28e-06 ***\neasy_hard3                   1.374e+00  7.488e-01   1.834 0.066603 .  \neasy_hard4                   2.940e-01  7.214e-01   0.408 0.683598    \neasy_hard5                   1.593e+00  7.100e-01   2.243 0.024890 *  \neasy_hard6                  -3.706e-01  8.303e-01  -0.446 0.655323    \neasy_hard7                  -1.694e-01  1.037e+00  -0.163 0.870239    \ncomplicated_simple          -6.506e-01  1.773e-01  -3.671 0.000242 ***\nconfusing_clear              2.877e-01  1.620e-01   1.776 0.075785 .  \ncomfortable_uncomfortable   -3.927e-01  1.298e-01  -3.026 0.002476 ** \nsatisfying_frustrating       2.317e-01  1.159e-01   1.999 0.045608 *  \nchallenging_not_challenging -1.322e-01  1.641e-01  -0.805 0.420689    \npleasant_unpleasant          7.729e-01  1.475e-01   5.239 1.62e-07 ***\nchaotic_organized            3.058e-01  9.865e-02   3.100 0.001935 ** \nGPA                          2.159e-01  4.056e-01   0.532 0.594511    \npct_math                    -3.338e-03  3.305e-03  -1.010 0.312373    \nvideo_length                 1.076e-04  1.861e-04   0.578 0.563013    \nvideo_number                -4.326e-02  1.217e-02  -3.555 0.000378 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1198.6  on 867  degrees of freedom\nResidual deviance: 1000.0  on 830  degrees of freedom\n  (1089 observations deleted due to missingness)\nAIC: 1076\n\nNumber of Fisher Scoring iterations: 12\n\n\n\n\nViewing multiple times is encouraged by\n\nIt is fun (decrease) to use math to understand biology\nUsing math to understand biology is interesting (increase)\nI worry (decrease) about getting worse grades in a biology course that incorporates math than one that does not\npleasant versus unpleasant\nchaotic versus organized\n\n\n\nViewing multiple times is discouraged by\n\nUsing math to understand biology appeals (decrease) to me\nUsing math to understand biology is interesting (decrease)\nIt is important (increase) for me to be able to do math for my career in the life sciences\nTaking a biology course that incorporates math intimidates (both) me\ncomplicated versus simple\ncomfortable versus uncomfortable\nsatisfying versus frustrating\ntime in semester (video number)"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#viewing-time",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#viewing-time",
    "title": "Math Biology Video Project",
    "section": "Viewing Time",
    "text": "Viewing Time\n\nQueryRPos CoeffNeg Coeff\n\n\nWhat influences students to spend more time watching the videos?\n\n\n\n\n\nCall:\nglm(formula = as.numeric(avg_view_time_when_played) ~ intrigue_cat + \n    fun_cat + appeals_cat + interesting_cat + valuable_cat + \n    important_cat + essential_cat + useful_cat + work_harder_cat + \n    worry_cat + intimidate_cat + easy_hard + complicated_simple + \n    confusing_clear + comfortable_uncomfortable + satisfying_frustrating + \n    challenging_not_challenging + pleasant_unpleasant + chaotic_organized + \n    GPA + pct_math + video_length + video_number, family = \"gaussian\", \n    data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-853.86  -128.02   -24.41   108.91  1139.41  \n\nCoefficients: (1 not defined because of singularities)\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  180.5561   176.8864   1.021 0.307672    \nintrigue_catincrease         153.1457   116.2658   1.317 0.188134    \nintrigue_catdecrease         328.5588   104.4253   3.146 0.001712 ** \nfun_catincrease                2.3447    48.3834   0.048 0.961360    \nfun_catdecrease              -40.4265    74.1309  -0.545 0.585667    \nappeals_catincrease          -22.9577    56.2257  -0.408 0.683149    \nappeals_catdecrease           28.4891    66.2874   0.430 0.667466    \ninteresting_catincrease       85.8513    60.3164   1.423 0.155011    \ninteresting_catdecrease       -6.5632    41.9026  -0.157 0.875575    \nvaluable_catincrease        -153.6194    72.3949  -2.122 0.034136 *  \nvaluable_catdecrease        -872.1688   246.4276  -3.539 0.000424 ***\nimportant_catincrease        -16.3917    39.5645  -0.414 0.678758    \nimportant_catdecrease       -842.0904   255.4479  -3.297 0.001020 ** \nessential_catincrease         78.0939    41.2858   1.892 0.058900 .  \nessential_catdecrease        826.7246   268.3537   3.081 0.002133 ** \nuseful_catincrease           -28.3670    41.7338  -0.680 0.496876    \nuseful_catdecrease                 NA         NA      NA       NA    \nwork_harder_catincrease      -14.9730    27.3546  -0.547 0.584274    \nwork_harder_catdecrease       36.4671    38.6517   0.943 0.345711    \nworry_catincrease             13.4219    37.2188   0.361 0.718474    \nworry_catdecrease             -2.8790    35.4262  -0.081 0.935249    \nintimidate_catincrease        -2.7321    45.7014  -0.060 0.952344    \nintimidate_catdecrease        33.5371    32.8474   1.021 0.307552    \neasy_hard3                   -34.3803    74.1131  -0.464 0.642848    \neasy_hard4                   -25.4733    71.6178  -0.356 0.722167    \neasy_hard5                   -26.0246    69.5317  -0.374 0.708289    \neasy_hard6                  -100.2542    82.3422  -1.218 0.223748    \neasy_hard7                   221.9490    96.7221   2.295 0.021999 *  \ncomplicated_simple           -21.7275    17.1182  -1.269 0.204705    \nconfusing_clear               31.3572    16.0778   1.950 0.051472 .  \ncomfortable_uncomfortable     28.3085    13.1634   2.151 0.031801 *  \nsatisfying_frustrating       -20.8471    12.0115  -1.736 0.083008 .  \nchallenging_not_challenging  -17.2412    16.4447  -1.048 0.294744    \npleasant_unpleasant           -0.3234    14.5043  -0.022 0.982217    \nchaotic_organized            -19.1039     9.7751  -1.954 0.050996 .  \nGPA                           61.4313    39.0669   1.572 0.116225    \npct_math                       0.1931     0.3323   0.581 0.561199    \nvideo_length                   0.2106     0.0188  11.204  &lt; 2e-16 ***\nvideo_number                   3.1522     1.2089   2.607 0.009288 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 51216.36)\n\n    Null deviance: 59330647  on 867  degrees of freedom\nResidual deviance: 42509579  on 830  degrees of freedom\n  (1089 observations deleted due to missingness)\nAIC: 11915\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\nViewing time is encouraged by\n\nUsing math to understand biology intrigues (decrease) me\ncomfortable versus uncomfortable\ntime in semester (video number)\n\n\n\nViewing time is discouraged by\n\nMath is valuable (both) for me for my life science career\nIt is important (decrease) for me to be able to do math for my career in the life sciences"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#take-home-messages",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#take-home-messages",
    "title": "Math Biology Video Project",
    "section": "Take Home Messages",
    "text": "Take Home Messages\nStudents who viewed videos multiple times and/or for longer duration were\n\nmore pessimistic about math emotions\nworried less about their course grade\nnot affected by the proportion of math content"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html#thank-you",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html#thank-you",
    "title": "Math Biology Video Project",
    "section": "Thank You",
    "text": "Thank You\n\n\nDerek Sollberger\n\nData Analyst\nUC Merced\ndsollberger@ucmerced.edu\n\n\nDr Emily Weigel\n\nCourse Instructor\nGeorgia Tech\nemily.weigel@biosci.gatech.edu"
  },
  {
    "objectID": "posts/xDBER/Sollberger_X_DBER_slides.html",
    "href": "posts/xDBER/Sollberger_X_DBER_slides.html",
    "title": "Math Biology Video Project",
    "section": "",
    "text": "Derek Sollberger\n\nData Analyst\nUC Merced\n\n\nDr Emily Weigel\n\nCourse Instructor\nGeorgia Tech\n\n\n\n\n\n\n\n\nEcology course (prereq: Intro Bio)\nFall 2018 semester\n91 students (85 students in study)\n24 videos\n\nabout 10 minutes per video\n\n\n\n\n\n\nimage credit: Shutterstock\n\n\n\n\n\n\n\n\n\n\n\n\nmath background\n\n\n\n\nmostly sophomores\nprior experience in flipped classrooms\nmostly White and Asian students\n77 percent female\n\n\n\n\n\n\n\nValuesValues 2EmotionsEmotions 2\n\n\n\n\n\nAndrews 2017\n\n\n\n\n\n\n\nMath Biology Values Instrument\n\n\n\n\n\n\n\nWachsmuth 2017\n\n\n\n\n\n\n\nMath Emotions Instrument\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur first glance at the survey data showed no significant results between the pre and post surveys.\n\n\n\nWe grouped students into “unchanged”, “increase”, or “decrease” groups based on their pre- and post-semester survey results for the MBVI queries—where “unchanged” was a difference of -1, 0, or 1 on the 7-point Likert scales\n\n\n\n\nOne VideoAll Videos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath Biology Values Instrument\n\n\n\n\n\n\n\n\n\nPlaysVisitsPersistence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo some students stop watching videos across term?"
  },
  {
    "objectID": "posts/sex_bio_context/sex_in_a_biological_context.html",
    "href": "posts/sex_bio_context/sex_in_a_biological_context.html",
    "title": "Sex in a Biological Context",
    "section": "",
    "text": "Goal\n\nEncourage students to use established research materials to explore sex in a biological context\n\nObjectives\n\nComment on an interview with Dr Joan Roughgarden\nSummarize a passage from a book by Dr Joan Roughgarden\nInterpret a couple pieces of data visualizaton from sex studies\n\nContext\n\ndata science course\nmostly biology majors, mostly sophomores\nhomework assignment\n\n\n\n\n\nTaskWord CloudSample Responses\n\n\n\n\nDr Joan Roughgarden was one of the most important researchers in the field of sexual selection theory, and she has written several works on the matters including the book Evolution’s Rainbow. Dr Roughgarden spoke at a WiSE conference (Women in Science and Engineering) a couple of years ago. For this resource , you may focus on the middle of the video from the “Evolution’s Rainbow” segment (at the 9:25 mark) to the “Career track” segment (at the 15:50 mark).\n\nhttps://www.youtube.com/watch?v=hDbsQu0XhPo\nsource: https://www.wisecology.net/speakers/joan-roughgarden/\n\n\n\n\n\nDr Joan Roughgarden\n\n\n\n\n\n\n\n\n\nresponses to the video interview\n\n\n\n\n\n“There is a lot of diversity in sex and gender in not only humans, but animals as well. With a variety of sex and gender in living beings that is identified by Dr. Roughgarden, it may mean that the sexual selection theory is outdated as we expand our knowledge on this topic. The theory needs to be rewritten for this modern era.”\n\n“This excerpt from Dr. Joan Roughgarden discusses her work on her book Evolution’s Rainbow and her personal experience as a (trans) woman in science. She discusses her work on gender and sexuality across all animal kingdoms, argues that sexual selection should be scrapped, and thinks it will collapse altogether. As a woman in science, she has seen that men’s careers in science are linear and are privileged with a sense of authority.”\n“The career track segment of Dr. Joan Roughgarden’s speech at the Women in Science and Engineering conference focused on her experiences as a transgender woman in the field of biology. She shared the challenges she faced while navigating her gender identity in a male-dominated field and the discrimination and bias she encountered. However, she also emphasized the importance of being true to oneself and finding a supportive community. Roughgarden’s message was one of resilience and perseverance in the face of adversity.”\n\n\n\n\n\n\n\n\nTaskWord CloudSample Responses\n\n\n\n\nSkim through chapter 2 “Sex versus Gender” of Evolution’s Rainbow. What is your impression of the writing? Or, what information did you get from this chapter?\n\n\n\n\nEvolution’s Rainbow\n\n\n\n\n\n\n\n\n\nresponses to the book excerpt\n\n\n\n\n\n“While reading the small passage, from Evolution’s Rainbow Chapter 2, it states that male and female are just biological categories. It starts by saying that people use the words, gender and sex wrong stating that the word’s mean male and female. In which the word’s male and female does not relate with have two different meanings biological criteria and in the social criteria.”\n“Looking through the text and readings we can see that Dr. Roughgarden discusses in terms about”male” and “female” instead of “man” and “woman”. As she done so she also discusses about when it comes to humans male and female don’t coincide 100 percent. She also discusses about the social categories rests in society and not in science.”\n\n“This excerpt approaches males and females from the sex, gender, and social perspective. It states that male and female as a \"Sex\" does not make sense since sex refers to the mixing of genes or reproduction, but some reproduction is asexual reproduction without two partners. Gender’s definition is embracing the biological definition but the other attributes of gender –masculinity and femininity– are not defined biologically. The author is writing up and tearing down these words and stripping them of meaning to reveal their inconsistencies.”\n\n\n\n\n\n\n\n\nTask (3)Word Cloud (3)Sample Responses (3)Task (4)Word Cloud (4)Sample Responses (4)\n\n\n\n\nWhat is your impression of the data visualization below? What information can you gather from the figure(s)?\n\nManey DL, Merritt JR, Prichard MR, Horton BM, Yi SV. Inside the supergene of the bird with four sexes. Horm Behav. 2020 Nov;126:104850. doi: 10.1016/j.yhbeh.2020.104850. Epub 2020 Sep 19. PMID: 32937166; PMCID: PMC7725849.\n\n\n\n\nData Viz 1\n\n\n\n\n\n\n\n\n\nresponses to the first graph\n\n\n\n\n\n“One morph performs a few of the aforementioned actions more frequently than the other. For instance, both sexes of WS birds sing more frequently in response to simulated territorial invasions than TS birds do. TS females seldom ever sing, despite the fact that TS males frequently sing loudly. While TS males are more likely to stay in their own territories, WS males are more prone to participate in territorial invasions. TS birds provide nestlings more frequently than their WS counterparts, and males reproduce this tendency more frequently than females. In general, WS birds appear to devote more time to mate-hunting and intrasexual rivalry, whereas TS birds adopt a more parental life-history approach.”\n\n“that data infomration that has been provided above has allowed me to follow what is being studied fairly easy since they would use the the color of the birds that they have to be distinguished and is being used in the boxplots in which helps us know which one is for which. with that in mind the boxplot allows us to understand the ranges that they have within the species while also allowing us to see how long the songs that each of the birds would sing within the ranges of ten minutes which shows us how different they are. while within the other boxplot then they would show the trips per hour in which helps us see how different they are compared the sexes from both of the species since the females TS have the highest trips per hour compared to the others. another thing they would use is the means, medians, and the quartiles so that they could be compared between the species and their sexes to know the differences.”\n“I can see that the tan-striped bird makes more trips but they have shorter songs sung. Compared to the white bird he made fewer trips but sang for longer times than the tan-striped bird. The used a box graph to show the difference.”\n\n\n\n\n\nWhat is your impression of the data visualization below? What information can you gather from the figure(s)?\n\nManey DL, Merritt JR, Prichard MR, Horton BM, Yi SV. Inside the supergene of the bird with four sexes. Horm Behav. 2020 Nov;126:104850. doi: 10.1016/j.yhbeh.2020.104850. Epub 2020 Sep 19. PMID: 32937166; PMCID: PMC7725849.\n\n\n\n\nData Viz 2\n\n\n\n\n\n\n\n\n\nresponses to the second graph\n\n\n\n\n\n“Prolactin has been associated with provisioning behaviors in both male and female songbirds. Females had a higher normalized VIP expression. Song rate is much higher in males.”\n\n“My impression of the data visualization is that is clear to follow, the colors are coordinated and it has just the necessary information. From the figures it can be gathered that WS have a higher VIP expression compared to that of the TS, also that TS females have a shorter range than all the other birds. Then from figures C and D we can observe whether there is correlation between the song rate and the VIP expression. For males there is no correlation but for females there is a slight positive correlation.”\n“The males of different markings have more differences to each other than the male and female of similar markings when looking at songs produced. Adding hormones into the mix helped condense the songs between birds of similar markings, making the plot a lot less scattered.”\n\n\n\n\n\n\n\n\nTaskSample Responses\n\n\n\n\nIf you wanted to discuss the topic of sex (and maybe gender) in a biological context, what other information would you seek out?\n\n\n\n\nresponses to the last prompt\n\n\n\n\n\n\n\n“If I would want to broaden my understanding on this topic, which is something that I will definitely be looking for, is probably toward podcasts and other articles can help me better understand the difference between sex and gender from there experiences.”\n“What are the biological factors of sex? Does mental health come into play?”\n“I would seek out more information on the genetic compositions of organisms and specifically compare genetic maps of female and male organisms. I would also look at asexual organisms and study how reproduction happens with an organism that is neither female nor male or an organism that is both. We could also study the chemicals released on organisms based on gender like how testosterone is released in male organisms and estrogen is released in female organisms.”"
  },
  {
    "objectID": "posts/graphviz/graphviz.html",
    "href": "posts/graphviz/graphviz.html",
    "title": "graphviz",
    "section": "",
    "text": "For years, I have been searching for the ability to make color-coded flowcharts, and I have found “graphviz online!”. We can use apps such as this one.\nThe code is very intuitive and easy to use. Furthermore, there are many attribute options for nodes and edges.\ndigraph G {\n\n\"Egg\" -&gt; \"Fried\\nEgg\"\n\"Potato\" -&gt; \"Hashbrowns\"\n\"Oil\" -&gt; \"Hashbrowns\"\n\"Egg\" -&gt; \"Pancakes\"\n\"Wheat\\nFlour\" -&gt; \"Pancakes\"\n\n\"Fried\\nEgg\" -&gt; \"Complete\\nBreakfast\"\n\"Hashbrowns\" -&gt; \"Complete\\nBreakfast\"\n\"Milk\" -&gt; \"Complete\\nBreakfast\"\n\"Pancakes\" -&gt; \"Complete\\nBreakfast\"\n\n\n\"Egg\" [shape = circle, style = filled, fillcolor = \"#59C9F1\"]\n\"Potato\" [shape = circle, style = filled, fillcolor = \"#59C9F1\"]\n\"Oil\" [shape = square, style = filled, fillcolor = \"#DDA059\"]\n\"Wheat\\nFlour\" [shape = circle, style = filled, fillcolor = \"#59C9F1\"]\n\n\"Fried\\nEgg\" [shape = square, style = filled, fillcolor = \"#DDA059\"]\n\"Milk\" [shape = circle, style = filled, fillcolor = \"#59C9F1\"]\n\"Hashbrowns\" [shape = square, style = filled, fillcolor = \"#DDA059\"]\n\"Pancakes\" [shape = square, style = filled, fillcolor = \"#DDA059\"]\n\"Complete\\nBreakfast\" [shape = hexagon, style = filled, fillcolor = \" #FFD921\"]\n}\n\n\n\nStardew Valley complete breakfast"
  },
  {
    "objectID": "posts/SpatialDataNotes/04_spherical-geometries.html",
    "href": "posts/SpatialDataNotes/04_spherical-geometries.html",
    "title": "The Median Data Scientist",
    "section": "",
    "text": "Learning objectives:\n\nConsider geometries on a sphere\n\n\n\n\nTriangle on a Globe\n\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"maps\")\nlibrary(\"rnaturalearth\")\nlibrary(\"rnaturalearthdata\")\nlibrary(\"s2\")\nlibrary(\"sf\")\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] sf_1.0-12               s2_1.1.3                rnaturalearthdata_0.1.0\n[4] rnaturalearth_0.3.2     maps_3.4.1              ggplot2_3.4.2          \n[7] dplyr_1.1.2            \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10        pillar_1.9.0       compiler_4.2.2     class_7.3-20      \n [5] tools_4.2.2        digest_0.6.31      lattice_0.20-45    jsonlite_1.8.4    \n [9] evaluate_0.21      lifecycle_1.0.3    tibble_3.2.1       gtable_0.3.3      \n[13] pkgconfig_2.0.3    rlang_1.1.0        cli_3.6.0          DBI_1.1.3         \n[17] rstudioapi_0.14    xfun_0.39          fastmap_1.1.1      e1071_1.7-13      \n[21] withr_2.5.0        httr_1.4.5         knitr_1.42         generics_0.1.3    \n[25] vctrs_0.6.1        htmlwidgets_1.6.2  classInt_0.4-9     grid_4.2.2        \n[29] tidyselect_1.2.0   glue_1.6.2         R6_2.5.1           fansi_1.0.4       \n[33] rmarkdown_2.21     sp_1.6-0           magrittr_2.0.3     units_0.8-2       \n[37] scales_1.2.1       htmltools_0.5.4    colorspace_2.1-0   KernSmooth_2.23-20\n[41] utf8_1.2.3         proxy_0.4-27       wk_0.7.3           munsell_0.5.0     \n\n\n\n\n\n\nHow does the GeoJSON format (Butler et al. 2016) define “straight” lines between ellipsoidal coordinates (Section 3.1.1)? Using this definition of straight, how does LINESTRING(0 85,180 85) look like in an Arctic polar projection? How could this geometry be modified to have it cross the North Pole?\n\n\n\nthis_linestring <- st_linestring(matrix(c(0, 85, 180, 85), ncol = 2),\n                                 dim = \"XY\")\nclass(this_linestring)\n\n[1] \"XY\"         \"LINESTRING\" \"sfg\"       \n\n\n\n\n\n\nthis_linestring |>\n  ggplot() +\n  geom_sf(color = \"red\", linewidth = 3) +\n  labs(title = \"This Linestring\",\n       subtitle = \"Where is it?\",\n       caption = \"Spatial Data Science book club\") +\n  theme_minimal()\n\n\n\n\n\n# https://stackoverflow.com/questions/58919102/map-arctic-subarctic-regions-in-ggplot2-with-lambert-conformal-conic-projection\nworld <- ne_countries(scale = \"medium\", returnclass = \"sf\")\nworld_cropped <- world |>\n  st_make_valid() |>\n  st_crop(xmin = -180.0, xmax = 180.0, ymin = 45.0, ymax = 90.0)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# st_sfc(this_linestring) <- st_crs(world_cropped)\n\n# this_linestring_sf <- st_sf(this_linestring, \n#                             st_sfc(this_linestring, crs = \"EPSG:3995\"))\n\nggplot(data = world_cropped) + \n  geom_sf() + \n  # geom_sf(data = this_linestring, color = 'red') +\n  coord_sf(crs = \n             \"+proj=lcc +lat_1=50 +lat_2=70 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0\")\n\n\n\n\n\n\n\nFor a typical polygon on \\(S^2\\), how can you find out ring direction?\n\nA convention here is to define the inside as the left (or right) side of the polygon boundary when traversing its points in sequence. Reversal of the node order then switches inside and outside.\nAdditional resource: ESRI: Polygon page\n\n\n\n\n\nAntarctica_map <- map(fill = TRUE, plot = FALSE) |>\n  st_as_sf() |>\n  filter(ID == \"Antarctica\")\n\nAntarctica_map |> \n  ggplot() + \n  geom_sf() +\n  labs(title = \"Antarctica\", \n       subtitle = \"Think of the latitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nst_bbox(Antarctica_map)\n\n      xmin       ymin       xmax       ymax \n-180.00000  -85.19218  179.62032  -60.52090 \n\n\nwhich clearly does not contain the region (ymin being -90 and xmax 180).\n\nFiji_map <- map(fill = TRUE, plot = FALSE) |>\n  st_as_sf() |>\n  filter(ID == \"Fiji\")\n\nFiji_map |> \n  ggplot() + \n  geom_sf() +\n  labs(title = \"Fiji\", \n       subtitle = \"Think of the longitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nst_bbox(Fiji_map)\n\n      xmin       ymin       xmax       ymax \n-179.86734  -21.70586  180.17769  -12.47695 \n\n\nseems to span most of the Earth\n\n\n\ns2_bounds_cap(Antarctica_map)\n\n  lng lat   angle\n1   0 -90 29.4791\n\n\n\ns2_bounds_rect(Antarctica_map)\n\n  lng_lo lat_lo lng_hi   lat_hi\n1   -180    -90    180 -60.5209\n\n\n\ns2_bounds_rect(Fiji_map)\n\n    lng_lo    lat_lo    lng_hi    lat_hi\n1 174.5872 -21.70586 -178.2511 -12.47695\n\n\n\n\n\n\nMaps of Antarctica should probably display the South Pole. Do the following maps display the South Pole?\n\n\n\n# maps package\nm <- st_as_sf(map(fill=TRUE, plot=FALSE))\nAntarctica_map_A <- m[m$ID == \"Antarctica\", ]\nst_geometry(Antarctica_map_A) |>\n  ggplot() + \n  geom_sf() +\n  labs(title = \"Antarctica\", \n       subtitle = \"Think of the latitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nsf::st_is_valid(Antarctica_map_A)\n\n[1] TRUE\n\n\n\n# Natural Earth package\nne <- ne_countries(returnclass = \"sf\")\nAntarctica_map_B <- ne[ne$region_un == \"Antarctica\", \"region_un\"]\nst_geometry(Antarctica_map_B) |>\n  ggplot() + \n  geom_sf() +\n  labs(title = \"Antarctica\", \n       subtitle = \"Think of the latitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nsf::st_is_valid(Antarctica_map_B)\n\n[1] TRUE\n\n\n\n\n\n\nAntarctica_map_C <- st_geometry(Antarctica_map_A) |>\n  st_transform(3031)\nAntarctica_map_C |> \n  ggplot() + \n  geom_sf() +\n  labs(title = \"Antarctica\", \n       subtitle = \"Think of the latitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nsf::st_is_valid(Antarctica_map_C)\n\n[1] TRUE\n\n\n\nAntarctica_map_D <- st_geometry(Antarctica_map_B) |>\n  st_transform(3031)\nAntarctica_map_D |> \n  ggplot() + \n  geom_sf() +\n  labs(title = \"Antarctica\", \n       subtitle = \"Think of the latitude\",\n       caption = \"Spatial Data Science book club\")\n\n\n\n\n\nsf::st_is_valid(Antarctica_map_D)\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\n\n\nMeeting chat log\n\nLOG"
  },
  {
    "objectID": "posts/TidyModels_trees/Tidymodels_Trees.html",
    "href": "posts/TidyModels_trees/Tidymodels_Trees.html",
    "title": "TidyModels Trees",
    "section": "",
    "text": "Years ago, I would use the caret package to perform a random forest search and plot an example of a decision tree. Can we do that now in the TidyModels module?\nHere I am adapting code from Stack Overflow\n\nlibrary(\"palmerpenguins\")\nlibrary(\"rpart\")\nlibrary(\"rpart.plot\")\nlibrary(\"tidymodels\")\n\n\ndf &lt;- penguins |&gt;\n  mutate(species = factor(species))\n\ndata_split &lt;- initial_split(df)\ndf_train &lt;- training(data_split)\ndf_test &lt;- testing(data_split)\n\n\ndf_recipe &lt;- recipe(species ~ ., data = df) %&gt;%\n  step_normalize(all_numeric())\n\n\n#building model\ntree &lt;- decision_tree() %&gt;%\n   set_engine(\"rpart\") %&gt;%\n   set_mode(\"classification\")\n\n\n#workflow\ntree_wf &lt;- workflow() %&gt;%\n  add_recipe(df_recipe) %&gt;%\n  add_model(tree) %&gt;%\n  fit(df_train) #results are found here \n\n\ntree_fit &lt;- tree_wf |&gt;\n  extract_fit_parsnip()\nrpart.plot(tree_fit$fit, roundint = FALSE)\n\n\n\n\nSo far,\n\nresponse variable has to be a factor type—and hence should be categorical—in a classification setting\nI need to learn what all of those numbers mean!\n\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] yardstick_1.2.0      workflowsets_1.0.1   workflows_1.1.3     \n [4] tune_1.1.1           tidyr_1.3.0          tibble_3.2.1        \n [7] rsample_1.1.1        recipes_1.0.6        purrr_1.0.1         \n[10] parsnip_1.1.0        modeldata_1.1.0      infer_1.0.4         \n[13] ggplot2_3.4.2        dplyr_1.1.2          dials_1.2.0         \n[16] scales_1.2.1         broom_1.0.4          tidymodels_1.1.0    \n[19] rpart.plot_3.1.1     rpart_4.1.19         palmerpenguins_0.1.1\n\nloaded via a namespace (and not attached):\n [1] jsonlite_1.8.4      splines_4.2.2       foreach_1.5.2      \n [4] prodlim_2023.03.31  GPfit_1.0-8         yaml_2.3.7         \n [7] globals_0.16.2      ipred_0.9-14        pillar_1.9.0       \n[10] backports_1.4.1     lattice_0.20-45     glue_1.6.2         \n[13] digest_0.6.31       hardhat_1.3.0       colorspace_2.1-0   \n[16] htmltools_0.5.4     Matrix_1.5-3        timeDate_4022.108  \n[19] pkgconfig_2.0.3     lhs_1.1.6           DiceDesign_1.9     \n[22] listenv_0.9.0       gower_1.0.1         lava_1.7.2.1       \n[25] timechange_0.2.0    generics_0.1.3      ellipsis_0.3.2     \n[28] withr_2.5.0         furrr_0.3.1         nnet_7.3-18        \n[31] cli_3.6.1           survival_3.4-0      magrittr_2.0.3     \n[34] evaluate_0.21       future_1.32.0       fansi_1.0.4        \n[37] parallelly_1.35.0   MASS_7.3-58.1       class_7.3-20       \n[40] tools_4.2.2         data.table_1.14.8   lifecycle_1.0.3    \n[43] munsell_0.5.0       compiler_4.2.2      rlang_1.1.0        \n[46] grid_4.2.2          iterators_1.0.14    rstudioapi_0.14    \n[49] htmlwidgets_1.6.2   rmarkdown_2.21      gtable_0.3.3       \n[52] codetools_0.2-18    R6_2.5.1            lubridate_1.9.2    \n[55] knitr_1.42          fastmap_1.1.1       future.apply_1.10.0\n[58] utf8_1.2.3          parallel_4.2.2      Rcpp_1.0.10        \n[61] vctrs_0.6.1         tidyselect_1.2.0    xfun_0.39"
  },
  {
    "objectID": "posts/ISLR/ch9.html",
    "href": "posts/ISLR/ch9.html",
    "title": "SVMs",
    "section": "",
    "text": "Learning objectives:\n\nImplement a binary classification model using a maximal margin classifier.\nImplement a binary classification model using a support vector classifier.\nImplement a binary classification model using a support vector machine (SVM).\nGeneralize SVM models to multi-class cases.\n\nSupport vector machine (SVM), an approach for classification developed in 1990. SVM is a generalizaion of classifiers methods, in particular:\n\nmaximal margin classifier (it requires that the classes be separable by a linear boundary).\nsupport vector classifier\nsupport vector machine: binary classification setting with two classes\n\n\n\n\n\nlibrary(\"caTools\")\nlibrary(\"dplyr\")\nlibrary(\"e1071\")\nlibrary(\"ggplot2\")\nlibrary(\"ISLR\")\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ISLR_1.4       ggplot2_3.4.2  e1071_1.7-13   dplyr_1.1.2    caTools_1.18.2\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       cli_3.6.1         knitr_1.43        rlang_1.1.1      \n [5] xfun_0.39         generics_0.1.3    jsonlite_1.8.4    glue_1.6.2       \n [9] colorspace_2.1-0  htmltools_0.5.5   scales_1.2.1      fansi_1.0.4      \n[13] rmarkdown_2.22    grid_4.3.0        munsell_0.5.0     evaluate_0.21    \n[17] tibble_3.2.1      bitops_1.0-7      fastmap_1.1.1     yaml_2.3.7       \n[21] lifecycle_1.0.3   compiler_4.3.0    htmlwidgets_1.6.2 pkgconfig_2.0.3  \n[25] rstudioapi_0.14   digest_0.6.31     R6_2.5.1          class_7.3-21     \n[29] tidyselect_1.2.0  utf8_1.2.3        pillar_1.9.0      magrittr_2.0.3   \n[33] withr_2.5.0       gtable_0.3.3      proxy_0.4-27      tools_4.3.0      \n\n\n\n\n\n\n\nimage credit: Deep AI\n\n\n\nA hyperplane is a \\(p-1\\)-dimensional flat subspace of a \\(p\\)-dimensional space. For example, in a 2-dimensional space, a hyperplane is a flat one-dimensional space: a line.\n(standard form) Definition of 2D hyperplane in 3D space: \\[\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3}= 0\\]\n(inner products) Any \\(X\\) s.t. \\(X = (X_{1}, X_{2})^T\\) for which the equation above is satisfied is a point on the hyperplane.\n\nAdditional resource: Deep AI\n\n\n\n\nConsider a matrix X of dimensions \\(n*p\\), and a \\(y_{i} \\in \\{-1, 1\\}\\). We have a new observation, \\(x^*\\), which is a vector \\(x^* = (x^*_{1}...x^*_{p})^T\\) which we wish to classify to one of two groups.\nWe will use a separating hyperplane to classify the observation.\n\n\n\nWe can label the blue observations as \\(y_{i} = 1\\) and the pink observations as \\(y_{i} = -1\\).\nThus, a separating hyperplane has the property s.t. \\(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip} > 0\\) if \\(y_{i} =1\\) and \\(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip} < 0\\) if \\(y_{i} = -1\\).\nIn other words, a separating hyperplane has the property s.t. \\(y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) > 0\\) for all \\(i = 1...n\\).\nConsider also the magnitude of \\(f(x^*)\\). If it is far from zero, we are confident in its classification, whereas if it is close to 0, then \\(x^*\\) is located near the hyperplane, and we are less confident about its classification.\n\n\n\n\n\n\nGenerally, if data can be perfectly separated using a hyperplane, an infinite amount of such hyperplanes exist.\nAn intuitive choice is the maximal margin hyperplane, which is the hyperplane that is farthest from the training data.\nWe compute the perpendicular distance from each training observation to the hyperplane. The smallest of these distances is known as the margin.\nThe maximal margin hyperplane is the hyperplane for which the margin is maximized. We can classify a test observation based on which side of the maximal margin hyperplane it lies on, and this is known as the maximal margin classifier.\nThe maximal margin classifier classifies \\(x^*\\) based on the sign of \\(f(x^*) = \\beta_{0} + \\beta_{1}x^*_{1} + ... + \\beta_{p}x^*_{p}\\).\n\n\n\nNote the 3 training observations that lie on the margin and are equidistant from the hyperplane. These are the support vectors (vectors in \\(p\\)-dimensional space; in this case \\(p=2\\)).\nThey support the hyperplane because if their location was changed, the hyperplane would change.\nThe maximal margin hyperplane depends on these observations, but not the others (unless the other observations were moved at or within the margin).\n\n\n\n\n\nConsider constructing an MMC based on the training observations \\(x_{1}...x_{n} \\in \\mathbb{R}^p\\). This is the solution to the optimization problem:\n\n\\[\\text{max}_{\\beta_{0}...\\beta_{p}, M} \\space M\\] \\[\\text{subject to } \\sum_{j=1}^{p}\\beta_{j}^2 = 1\\] \\[y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) \\geq M \\quad \\forall i = 1...n\\]\n\n\\(M\\) is the margin, and the \\(\\beta\\) coeffients are chosen to maximize \\(M\\).\nThe constraint (3rd equation) ensures that each observation will be correctly classified, as long as M is positive.\n\n\n\nThe 2nd and 3rd equations ensure that each data point is on the correct side of the hyperplane and at least M-distance away from the hyperplane.\nThe perpendicular distance to the hyperplane is given by \\(y_{i}(\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} ... + \\beta_{p}x_{ip})\\).\n\n\nBut what if our data is not separable by a linear hyperplane?\n\n\n\nIndividual data points greatly affect formation of the maximal margin classifier\n\n\n\n\n\n\nWe can’t always use a hyperplane to separate two classes.\nEven if such a classifier does exist, it’s not always desirable, due to overfitting or too much sensitivity to individual observations.\nThus, it might be worthwhile to consider a classifier/hyperplane that misclassifies a few observations in order to improve classification of the remaining data points.\nThe support vector classifier, a.k.a the soft margin classifier, allows some training data to be on the wrong side of the margin or even the hyperplane.\n\n\n\n\n\nThe SVC classifies a test observation based on which side of the hyperplane it lies.\n\n\\[\\text{max}_{\\beta_{0}...\\beta_{p}, \\epsilon_{1}...\\epsilon_{n}, M} \\space M\\] \\[\\text{subject to } \\sum_{j=1}^{p}\\beta_{j}^2 = 1\\] \\[y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) \\geq M(1 - \\epsilon_{i})\\] \\[\\epsilon_{i} \\geq 0, \\quad \\sum_{i=1}^{n}\\epsilon_{i} \\leq C\\]\n\n\\(C\\) is a nonnegative tuning parameter, typically chosen through cross-validation, and can be thought of as the budget for margin violation by the observations.\nThe \\(\\epsilon_{i}\\) are slack variables that allow individual observations to be on the wrong side of the margin or hyperplane. The \\(\\epsilon_{i}\\) indicates where the \\(i^{\\text{th}}\\) observation is located with regards to the margin and hyperplane.\n\nIf \\(\\epsilon_{i} = 0\\), the observation is on the correct side of the margin.\nIf \\(\\epsilon_{i} > 0\\), the observation is on the wrong side of margin\nIf \\(\\epsilon_{i} > 1\\), the observation is on the wrong side of the hyperplane.\n\nSince \\(C\\) constrains the sum of the \\(\\epsilon_{i}\\), it determines the number and magnitude of violations to the margin. If \\(C=0\\), there is no margin for violation, thus all the \\(\\epsilon_{1},...,\\epsilon_{n} = 0\\).\nNote that if \\(C>0\\), no more than \\(C\\) observations can be on wrong side of hyperplane, since in these cases \\(\\epsilon_{i} > 1\\).\n\n\n\n\n\n\nA property of the classifier is that only data points which lie on or violate the margin will affect the hyperplane. These data points are known as support vectors.\n\\(C\\) controls the bias-variance tradeoff of the classifier.\n\nWhen \\(C\\) is large: high bias, low variance\nWhen \\(C\\) is small: low bias, high variance\n\nThe property of the SVC solely being dependent on certain observations in classification differs from other classification methods such as LDA (depends on mean of all observations in each class, as well as each class’s covariance matrix using all observations).\nHowever, logistic regression is more similar to SVC in that it has low sensitivity to observations far from the decision boundary.\n\n\n\n\n\nMany decision boundaries are not linear.\nWe could fit an SVC to the data using \\(2p\\) features (in the case of \\(p\\) features and using a quadratic form).\n\n\\[X_{1}, X_{1}^{2}, \\quad X_{2}, X_{2}^{2}, \\quad\\cdots, \\quad X_{p}, X_{p}^{2}\\]\n\\[\\text{max}_{\\beta_{0},\\beta_{11},\\beta_{12},\\dots,\\beta_{p1},\\beta_{p2} \\epsilon_{1},\\dots,\\epsilon_{n}, M} \\space M\\] \\[\\text{subject to }  y_{i}\\left(\\beta_{0} + \\sum_{j=1}^{p} \\beta_{ji}x_{ji} + \\sum_{j=1}^{p} \\beta_{ji}x_{ji}^{2}\\right) \\geq M(1 - \\epsilon_{i})\\]\n\\[\\epsilon_{i} \\geq 0, \\quad \\sum_{i=1}^{n}\\epsilon_{i} \\leq C, \\quad \\sum_{j=1}^{p}\\sum_{k=1}^{2} \\beta_{jk}^{2} = 1\\]\n\nNote that in the enlarged feature space (here, with the quadratic terms), the decision boundary is linear. But in the original feature space, it is quadratic \\(q(x) = 0\\) (in this example), and generally the solutions are not linear.\nOne could also include interaction terms, higher degree polynomials, etc., and thus the feature space could enlarge quickly and entail unmanageable computations.\n\n\n\n\n\nThe SVM is an extension of the SVC which results from using kernels to enlarge the feature space. A kernel is a function that quantifies the similarity of two data points.\nEssentially, we want to enlarge the feature space to make use of a nonlinear decision boundary, while avoiding getting bogged down in unmanageable calculations.\nThe solution to the SVC problem in the SVM context involves only the inner products (AKA dot products) of the observations.\n\n\\[\\langle x_{i}  \\; , x_{i'} \\; \\rangle = \\sum_{j=1}^{p}x_{ij}x_{i'j}\\]\nIn the context of SVM, the linear support vector classifier is as follows:\n\\[f(x) = \\beta_{0} + \\sum_{i=1}^{n}\\alpha_{i}\\langle \\; x, x_i\\; \\rangle\\]\n\nTo estimate the \\(n\\) \\(\\alpha_{i}\\) coefficients and \\(\\beta_{0}\\), we only need the \\(\\binom{n}{2}\\) inner products between all pairs of training observations.\nNote that in the equation above, in order to compute \\(f(x)\\) for the new point \\(x\\), we need the inner product between the new point and all the training observations. However, \\(\\alpha_{i} = 0\\) for all points that are not on or within the margin (i.e., points that are not support vectors). So we can rewrite the equation as follows, where \\(S\\) is the set of support point indices:\n\n\\[f(x) = \\beta_{0} + \\sum_{i \\in S}\\alpha_{i}\\langle \\; x, x_{i} \\; \\rangle\\]\n\nReplace every inner product with \\(K(x_{i}, x_{i'})\\), where \\(K\\) is a kernel function.\n\\(K(x_{i}, x_{i'}) = \\sum_{j=1}^{p}x_{ij}x_{i'j}\\) is the SVC and is known as a linear kernel since it is linear in the features.\nOne could also have kernel functions of the following form, where \\(d\\) is a positive integer:\n\n\\[K(x_{i}, x_{i'}) = \\left(1 + \\sum_{j=1}^{p}x_{ij}x_{i'j}\\right)^d\\]\n\nThis will lead to a much more flexible decision boundary, and is basically fitting an SVC in a higher-dimensional space involving polynomials of degree \\(d\\), instead of the original feature space.\nWhen an SVC is combined with a nonlinear kernel as above, the result is a support vector machine.\n\n\\[f(x) =  \\beta_{0} + \\sum_{i \\in S}\\alpha_{i}K(x, x_{i})\\]\n\n\n\n\n\n\nimage credit: Manin Bocss\n\n\n\nThere are other options besides polynomial kernel functions, and a popular one is a radial kernel.\n\n\\[K(x, x_{i}) = \\text{exp}\\left(-\\gamma\\sum_{j=1}^p(x_{ij} - x_{i'j})^2\\right), \\quad \\gamma > 0\\]\n\nFor a given test observations \\(x^*\\), if it is far from \\(x_{i}\\), then \\(K(x^*, x_{i})\\) will be small given the negative \\(\\gamma\\) and large \\(\\sum_{j=1}^p(x^*_{j} - x_{ij})^2)\\).\nThus, \\(x_{i}\\) will play little role in \\(f(x^*)\\).\nThe predicted class for \\(x^*\\) is based on the sign of \\(f(x^*)\\), so training observations far from a given test point play little part in determining the label for a test observation.\nThe radial kernel therefore exhibits local behavior with respect to other observations.\n\n\n\n\n\n\n\nimage credit: Manin Bocss\n\n\n\nThe advantage of using a kernel rather than simply enlarging feature space is computational, since it is only necessary to compute \\(\\binom{n}{2}\\) kernel functions.\nFor radial kernels, the feature space is implicit and infinite dimensional, so we could not do the computations in such a space anyways.\n\n\n\n\n\nThe concept of separating hyperplanes does not extend naturally to more than two classes, but there are some ways around this.\nA one-versus-one approach constructs \\(K \\choose 2\\) SVMs, where \\(K\\) is the number of classes. An observation is classified to each of the \\(K \\choose 2\\) classes, and the number of times it appears in each class is counted.\nThe \\(k^\\text{th}\\) class might be coded as +1 versus the \\((k')^\\text{th}\\) class is coded as -1.\nThe data point is classified to the class for which it was most often assigned in the pairwise classifications.\nAnother option is one-versus-all classification. This can be useful when there are a lot of classes.\n\\(K\\) SVMs are fitted, and one of the K classes to the remaining \\(K-1\\) classes.\n\\(\\beta_{0k}...\\beta_{pk}\\) denotes the parameters that results from constructing an SVM comparing the \\(k\\)th class (coded as +1) to the other classes (-1).\nAssign test observation \\(x^*\\) to the class \\(k\\) for which \\(\\beta_{0k} + ... + \\beta_{pk}x^*_{p}\\) is largest.\n\n\n\n\n\n\nlibrary(\"tidymodels\")\nlibrary(\"kernlab\") # We'll use the plot method from this.\n\n\nset.seed(1)\nsim_data <- matrix(\n  rnorm (20 * 2), \n  ncol = 2,\n  dimnames = list(NULL, c(\"x1\", \"x2\"))\n) %>% \n  as_tibble() %>% \n  mutate(\n    y = factor(c(rep(-1, 10), rep(1, 10)))\n  ) %>%\n  mutate(\n    x1 = ifelse(y == 1, x1 + 1, x1),\n    x2 = ifelse(y == 1, x2 + 1, x2)\n  )\n\nsim_data %>% \n  ggplot() +\n  aes(x1, x2, color = y) +\n  geom_point() +\n  labs(title = \"Trying to make a hyperplane classifier\",\n       subtitle = \"simulated data\",\n       caption = \"R4DS book club\") +\n  theme_minimal()\n\n\n# generated this using their process then saved it to use here.\ntest_data <- readRDS(\"data/09-testdat.rds\") %>% \n  rename(x1 = x.1, x2 = x.2)\n\ntest_data %>% \n  ggplot() +\n  aes(x1, x2, color = y) +\n  geom_point() +\n  labs(title = \"Trying to make a hyperplane classifier\",\n       subtitle = \"simulated data\",\n       caption = \"R4DS book club\") +\n  theme_minimal()\n\nWe create a spec for a model, which we’ll update throughout this lab with different costs.\n\nsvm_linear_spec <- svm_poly(degree = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\", scaled = FALSE)\n\nThen we do a couple fits with manual cost.\n\nsvm_linear_fit_10 <- svm_linear_spec %>% \n  set_args(cost = 10) %>%\n  fit(y ~ ., data = sim_data)\nsvm_linear_fit_10\n\nsvm_linear_fit_10 %>%\n  extract_fit_engine() %>%\n  plot()\n\nsvm_linear_fit_01 <- svm_linear_spec %>% \n  set_args(cost = 0.1) %>%\n  fit(y ~ ., data = sim_data)\nsvm_linear_fit_01\nsvm_linear_fit_01 %>%\n  extract_fit_engine() %>%\n  plot()\n\nsvm_linear_fit_001 <- svm_linear_spec %>% \n  set_args(cost = 0.01) %>%\n  fit(y ~ ., data = sim_data)\nsvm_linear_fit_001\nsvm_linear_fit_001 %>%\n  extract_fit_engine() %>%\n  plot()\n\n\n\nLet’s find the best cost.\n\nsvm_linear_wf <- workflow() %>%\n  add_model(\n    svm_linear_spec %>% set_args(cost = tune())\n  ) %>%\n  add_formula(y ~ .)\n\nset.seed(1234)\nsim_data_fold <- vfold_cv(sim_data, strata = y)\n\nparam_grid <- grid_regular(cost(), levels = 10)\n\n# Our grid isn't identical to the book, but it's close enough.\nparam_grid\n\n\ntune_res <- tune_grid(\n  svm_linear_wf, \n  resamples = sim_data_fold, \n  grid = param_grid\n)\n\n# We ran this locally and then saved it so everyone doesn't need to wait for\n# this to process each time they build the book.\n\n# saveRDS(tune_res, \"data/09-tune_res.rds\")\n\n\nautoplot(tune_res)\n\nTune can pull out the best result for us.\n\nbest_cost <- select_best(tune_res, metric = \"accuracy\")\n\nsvm_linear_final <- finalize_workflow(svm_linear_wf, best_cost)\n\nsvm_linear_fit <- svm_linear_final %>% fit(sim_data)\n\nsvm_linear_fit %>% \n  augment(new_data = test_data) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n\\[\\text{accuracy} = \\frac{9 + 8}{9 + 1 + 2 + 8} = 0.85\\]\n\nsvm_linear_fit_001 %>% \n  augment(new_data = test_data) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n\\[\\text{accuracy} = \\frac{11 + 3}{11 + 6 + 0 + 3} = 0.70\\]\n\n\n\n\nsim_data_sep <- sim_data %>% \n  mutate(\n    x1 = ifelse(y == 1, x1 + 0.5, x1),\n    x2 = ifelse(y == 1, x2 + 0.5, x2)\n  )\n\nsim_data_sep %>% \n  ggplot() +\n  aes(x1, x2, color = y) +\n  geom_point()\n\nsvm_fit_sep_1e5 <- svm_linear_spec %>% \n  set_args(cost = 1e5) %>%\n  fit(y ~ ., data = sim_data_sep)\n\nsvm_fit_sep_1e5\nsvm_fit_sep_1e5 %>%\n  extract_fit_engine() %>%\n  plot()\n\nsvm_fit_sep_1 <- svm_linear_spec %>% \n  set_args(cost = 1) %>%\n  fit(y ~ ., data = sim_data_sep)\n\nsvm_fit_sep_1\nsvm_fit_sep_1 %>%\n  extract_fit_engine() %>%\n  plot()\n\ntest_data_sep <- test_data %>% \n  mutate(\n    x1 = ifelse(y == 1, x1 + 0.5, x1),\n    x2 = ifelse(y == 1, x2 + 0.5, x2)\n  )\n\nsvm_fit_sep_1e5 %>% \n  augment(new_data = test_data_sep) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n\\[\\text{accuracy} = \\frac{9 + 8}{8 + 1 + 2 + 8} = 0.85\\]\n\nsvm_fit_sep_1 %>% \n  augment(new_data = test_data_sep) %>%\n  conf_mat(truth = y, estimate = .pred_class)\n\n\\[\\text{accuracy} = \\frac{9 + 9}{9 + 0 + 2 + 9} = 0.90\\]\n\n\n\n\n\n\n\nggplot setup\n\n\nN <- 50 #resolution\nx <- seq(-10, 10, length.out = N)\ny <- seq(-10, 10, length.out = N)\n\ndf <- expand.grid(x,y)\ncolnames(df) <- c(\"xval\", \"yval\")\n\neuclidean_distance <- function(x1, y1, x2, y2){\n  # computes the Euclidean distance between (x1, y1) and (x2, y2)\n  sqrt( (x2 - x1)^2 + (y2 - y1)^2 )\n}\n\naccuracy_calculation <- function(confusion_matrix){\n  # computes the accuracy revealed by a 2x2 confusion matrix\n  Q <- confusion_matrix\n  (Q[1,1] + Q[2,2]) / (Q[1,1] + Q[1,2] + Q[2,1] + Q[2,2])\n}\n\n\n\n\nThis problem involves hyperplanes in two dimensions.\n\n\n\n\nblue: \\(1 + 3X_{1} - X_{2} > 0\\)\nred: \\(1 + 3X_{1} - X_{2} < 0\\)\n\n\n\ncode\n\n\ndf1 <- df |>\n  # math function\n  mutate(shade = ifelse(yval > 3*xval + 1, \"blue\", \"red\"))\n\ndf1 |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # main line\n  geom_segment(aes(x = -(11/3), y = -10, xend = 3, yend = 10),\n               color = \"black\", linewidth = 3) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = 3x + 1\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nblue: \\(-2 + X_{1} + 2X_{2} > 0\\)\nred: \\(-2 + X_{1} + 2X_{2} < 0\\)\n\n\n\ncode\n\n\ndf1b <- df |>\n  # math function\n  mutate(shade = ifelse(yval > 1 - 0.5*xval, \"blue\", \"red\"))\n\ndf1b |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # main line\n  geom_segment(aes(x = -10, y = 6, xend = 10, yend = -4),\n               color = \"black\", linewidth = 3) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = 1 - 0.5x\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWe now investigate a non-linear decision boundary.\n\nblue: \\((1 + X_{1})^{2} + (2 - X_{2})^{2} > 4\\)\nred: \\((1 + X_{1})^{2} + (2 - X_{2})^{2} < 4\\)\n\n\n\ncode\n\n\ndf2 <- df |>\n  # math function\n  mutate(shade = ifelse(euclidean_distance(xval, yval, -1, 2) > 4, \n                        \"blue\", \"red\"))\n\ndf2 |>\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"(x+1)^2 + (y-2)^2 = 4\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo what class is the observation (0, 0) classified? (−1, 1)? (2, 2)? (3, 8)?\n\n\nifelse(euclidean_distance(0, 0, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(-1, 1, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(2, 2, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(3, 8, -1, 2) > 4, \"blue\", \"red\")\n\n[1] \"blue\"\n\n\n\nWhile the decision boundary\n\n\\[(1 + X_{1})^{2} + (2 - X_{2})^{2} = 4\\]\nis not linear in \\(X_{1}\\) and \\(X_{2}\\), it is linear in terms of \\(X_{1}\\), \\(X_{1}^{2}\\), \\(X_{2}\\), \\(X_{2}^{2}\\)\n\\[\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}^{2} + \\beta_{4}X_{2}^{2} = 0\\]\nwith \\(\\beta_{0} = 1\\), \\(\\beta_{1} = 2\\), \\(\\beta_{2} = -4\\), \\(\\beta_{3} = 1\\), and \\(\\beta_{4} = 1\\).\n\n\n\n\nobs <- 1:7\nxvals <- c(3,2,4,1,2,4,4)\nyvals <- c(4,2,4,4,1,3,1)\nclass_label <- c(\"Red\", \"Red\", \"Red\", \"Red\", \"Blue\", \"Blue\", \"Blue\")\ndf3 <- data.frame(obs, xvals, yvals, class_label)\ndf3\n\n  obs xvals yvals class_label\n1   1     3     4         Red\n2   2     2     2         Red\n3   3     4     4         Red\n4   4     1     4         Red\n5   5     2     1        Blue\n6   6     4     3        Blue\n7   7     4     1        Blue\n\n\n\nWe are given \\(n = 7\\) observations in \\(p = 2\\) dimensions. For each observation, there is an associated class label.\n\n\n\ncode\n\n\ndf3 |>\n  ggplot() +\n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n  coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Where to Draw the Separating Hyperplane?\",\n       # subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\nSketch the optimal separating hyperplane, and provide the equation for this hyperplane\n\n\n\ncode\n\n\ndf3 |>\n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  # margins\n  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n    coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\n\n\n\nblue: \\(0.5 - X_{1} + X_{2} < 0\\)\nred: \\(0.5 - X_{1} + X_{2} > 0\\)\n\n\nmaximal margin in indicated by the dashed lines, with margin\n\n\\[M = \\frac{0.5}{\\sqrt{2}} \\approx 0.3536\\] (e) Indicate the support vectors for the maximal margin classifier.\n\n\ncode\n\n\ndf3e <- df3 |>\n  mutate(supp_vec = ifelse(obs %in% c(2,3,5,6), \n                           \"support vector\", \"other data\"))\n  \ndf3e$supp_vec <- factor(df3e$supp_vec,\n                        levels = c(\"support vector\", \"other data\"))\n  \ndf3e |>  \n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  # margins\n  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = supp_vec),\n             size = 5) +\n    coord_equal() +\n  scale_color_manual(values = c(\"purple\", \"gray50\")) +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\nArgue that a slight movement of the seventh observation would not affect the maximal margin hyperplane.\nSketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane.\n\n\n\ncode\n\n\ndf3 |>\n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0, y = 1/4, xend = 5, yend = 16/4),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n    coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Separating Hyperplane (not optimal)\",\n       subtitle = \"y = 0.25(3x+1)\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\nDraw an additional observation on the plot so that the two classes are no longer separable by a hyperplane.\n\n\n\ncode\n\n\nnew_dot <- data.frame(obs = 8, xvals = 0, yvals = 5, class_label = \"blue\")\ndf3h <- rbind(df3, new_dot)\ndf3h |>\n  ggplot() +\n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n  coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Where to Draw the Separating Hyperplane?\",\n       subtitle = \"new data at (0,5)\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\n\n\n\nMostly transcibed from OnMee’s solutions.\n\n\nGenerate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data. Which technique performs best on the test data? Make plots and report training and test error rates in order to back up your assertions.\n\n# Generating a dataset with visible non-linear separation.\nset.seed(3)\nx=matrix(rnorm(100*2), ncol=2)\ny=c(rep(-1,70), rep(1,30))\nx[1:30,]=x[1:30,]+3.3\nx[31:70,]=x[31:70,]-3\ndat=data.frame(x=x, y=as.factor(y))\n# Training and test sets.\nsample.data = sample.split(dat$x.1, SplitRatio = 0.7)\ntrain.set = subset(dat, sample.data==T)\ntest.set = subset(dat, sample.data==F)\nplot(x,col=(2-y), xlab='X1', ylab='X2', main='Dataset with non-linear separation')\n\n\n\n\n\n\n\n# Best model.\nset.seed(3)\ntune.out=tune(svm,\n              y ~ .,\n              data = train.set,\n              kernel='linear',\n              ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))\nbestmod=tune.out$best.model\nplot(bestmod, dat)\n\n\n\n\n\n# Predictions on training set.\nypred=predict(bestmod, train.set)\ntable_4_train = table(predict=ypred, truth=train.set$y)\nprint(table_4_train)\n\n       truth\npredict -1  1\n     -1 50 20\n     1   0  0\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_train), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 71.43 percent\"\n\n\n\n#Predictions on the test set.\nypred=predict(bestmod, test.set)\ntable_4_test = table(predict=ypred, truth=test.set$y)\nprint(table_4_test)\n\n       truth\npredict -1  1\n     -1 20 10\n     1   0  0\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_test), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 66.67 percent\"\n\n\n\n\n\n\n# Best model using cross validaitonon a set of values for cost and gamma.\nset.seed(3)\ntune.out=tune(svm, y~., data=train.set, kernel='radial',\nranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))\nbestmod = tune.out$best.model\nplot(bestmod, train.set)\n\n\n\n\n\n# Predictions on training set.\nypred=predict(bestmod, train.set)\ntable_4_train = table(predict=ypred, truth=train.set$y)\nprint(table_4_train)\n\n       truth\npredict -1  1\n     -1 50  0\n     1   0 20\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_train), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 100 percent\"\n\n\n\n#Predictions on the test set.\nypred=predict(bestmod, test.set)\ntable_4_test = table(predict=ypred, truth=test.set$y)\nprint(table_4_test)\n\n       truth\npredict -1  1\n     -1 20  0\n     1   0 10\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_4_test), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 100 percent\"\n\n\n\n\n\n\nWe have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary.We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features.\n\nGenerate a data set with n = 500 and p = 2, such that the observations belong to two classes with a quadratic decision boundary between them.\n\n\nx1 <- runif (500) - 0.5\nx2 <- runif (500) - 0.5\ny <- 1 * (x1^2 - x2^2 > 0)\ndf <- data.frame(x1=x1, x2=x2, y=as.factor(y))\n\n\nPlot the observations, colored according to their class labels. Your plot should display X1 on the x-axis, and X2 on the yaxis.\n\n\nplot(x1,x2,col = (2 - y))\n\n\n\n\n\nFit a logistic regression model to the data, using X1 and X2 as predictors.\n\n\nglm.fit = glm(y~x1+x2, data=df, family = 'binomial')\n# Predictions\nglm.probs = predict(glm.fit, newdata=df, type = 'response')\nglm.preds = rep(0,500)\nglm.preds[glm.probs>0.50] = 1\ntable_5_c <- table(preds=glm.preds, truth=df$y)\nprint(table_5_c)\n\n     truth\npreds   0   1\n    0  24  21\n    1 214 241\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_c), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 53 percent\"\n\n\n\nApply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be linear.\n\n\n# Plot using predicted class labels for observations.\nplot(x1,x2,col=2-glm.preds)\n\n\n\n\n\nNow fit a logistic regression model to the data using non-linear functions of X1 and X2 as predictors\n\n\nglm.fit = glm(y~I(x1^2)+I(x2^2), data = df, family = 'binomial')\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nglm.probs = predict(glm.fit, newdata = df, type = 'response')\nglm.preds = rep(0,500)\nglm.preds[glm.probs>0.5] = 1\ntable_5_e <- table(preds=glm.preds, truth=df$y)\nprint(table_5_e)\n\n     truth\npreds   0   1\n    0 238   0\n    1   0 262\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_e), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 100 percent\"\n\n\n\nApply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be obviously non-linear.\n\n\nplot(x1,x2,col=2-glm.preds)\n\n\n\n\n\nFit a support vector classifier to the data with X1 and X2 as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.\n\n\n#Best model\ntune.out=tune(svm,y~.,data = df,kernel='linear',\nranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))\nbestmod=tune.out$best.model\n\n#Predictions\nypred=predict(bestmod, newdata=df, type='response')\n\ntable_5_g <- table(predict=ypred, truth=df$y)\nprint(table_5_g)\n\n       truth\npredict   0   1\n      0   0   0\n      1 238 262\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_g), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 52.4 percent\"\n\nplot(x1,x2,col=ypred)\n\n\n\n\n\nFit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.\n\n\ntune.out=tune(svm, y~., data=df, kernel='radial',\nranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))\nbestmod=tune.out$best.mode\n\n#Predictions\nypred=predict(bestmod, newdata=df, type='response')\n\ntable_5_h <- table(predict=ypred, truth=df$y)\nprint(table_5_h)\n\n       truth\npredict   0   1\n      0 235   0\n      1   3 262\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_5_h), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 99.4 percent\"\n\nplot(x1,x2,col=ypred)\n\n\n\n\n\n\n\nAt the end of Section 9.6.1, it is claimed that in the case of data that is just barely linearly separable, a support vector classifier with a small value of cost that misclassifies a couple of training observations may perform better on test data than one with a huge value of cost that does not misclassify any training observations. You will now investigate this claim.\n\nGenerate two-class data with p = 2 in such a way that the classes are just barely linearly separable.\n\n\nset.seed(111)\nx1 = runif(1000,-5,5)\nx2 = runif(1000,-5,5)\nx = cbind(x1,x2)\ny = rep(NA,1000)\n# Classify points above abline(1.5,1) as 1 and below abline(-1.5,1) as -1, and the rest as 0.\n\n#Removing points classed as 0 will created a more widely separated dataset.\n# Actual decision boundary is a line where y=x, which is abline(0,1).\nfor (i in 1:1000)\nif (x[i,2]-x[i,1] > 1.5) y[i]=1 else if (x[i,2]-x[i,1] < -1.5) y[i]=-1 else y[i]=0\n\n# Combine x an y and remove all rows with y=0.\nx = cbind(x,y)\nx = x[x[,3]!=0,]\n\nplot(x[,1],x[,2],col=2-x[,3], xlab=\"X1\", ylab=\"X2\",xlim = c(-5,5), ylim = c(-5,5))\nabline(0,1, col=\"red\")\nabline(1.5,1)\nabline(-1.5,1)\nabline(h=0,v=0)\n\n\n\n\n\n#Generate random points to be used as noise along line y=1.5x(+-0.1).\nx.noise = matrix(NA,100,3)\nx.noise[,1] = runif(100,-5,5)\n\n#Y coordinate values for first 50 points\nx.noise[1:50,2] = (1.5*x.noise[1:50,1])-0.1 \nx.noise[51:100,2] = (1.5*x.noise[51:100,1])+0.1\nx.noise[,3] = c(rep(-1,50), rep(1,50)) # class values for all noise observations\nplot(x[,1],x[,2],col=2-x[,3], xlab='X1', ylab='X2',\nylim = c(-5,5),xlim = c(-5,5),\nmain=\"Dataset that is linearly separable and with added noise\")\npar(new = TRUE)\nplot(x.noise[,1],x.noise[,2],col=2-x.noise[,3], axes=F,\nxlab=\"\", ylab=\"\", ylim = c(-5,5), xlim = c(-5,5))\n#Noise\nabline(0,1.5,col=\"blue\")\n#Actual decision boundary\nabline(0,1,col=\"red\")\n\n\n\n\n\nCompute the cross-validation error rates for support vector classifiers with a range of cost values. How many training errors are misclassified for each value of cost considered, and how does this relate to the cross-validation errors obtained?\n\n\nx = rbind(x,x.noise)\ntrain.dat = data.frame(x1=x[,1],x2=x[,2], y=as.factor(x[,3]))\n\n#Linear SVM models with various values of cost.\ntune.out=tune(svm,y~.,data=train.dat,kernel='linear',\nranges=list(cost=c(0.001,0.01,0.1,1,5,10,100,1000)))\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n  100\n\n- best performance: 0 \n\n- Detailed performance results:\n   cost      error dispersion\n1 1e-03 0.05351161 0.02002264\n2 1e-02 0.05352630 0.01834727\n3 1e-01 0.05352630 0.01834727\n4 1e+00 0.05352630 0.01834727\n5 5e+00 0.04745813 0.02110627\n6 1e+01 0.04138995 0.02317355\n7 1e+02 0.00000000 0.00000000\n8 1e+03 0.00000000 0.00000000\n\n\n\nGenerate an appropriate test data set, and compute the test errors corresponding to each of the values of cost considered. Which value of cost leads to the fewest test errors, and how does this compare to the values of cost that yield the fewest training errors and the fewest cross-validation errors?\n\n\n# New test set\nset.seed(1221)\ntest.x1 = runif(1000,-5,5)\ntest.x2 = runif(1000,-5,5)\ntest.y = rep(NA,1000)\n\n# Actual decision boundary of the train set is y=x,\n# so points above line are classed as 1 and points below -1\nfor (i in 1:1000){\n  if (test.x1[i]-test.x2[i] < 0) test.y[i]=1 else if (test.x1[i]-test.x2[i] > 0) test.y[i]=-1\n}\n\n# Test dataframe\ntest.dat = data.frame(x1=test.x1,x2=test.x2,y=as.factor(test.y))\n\nplot(test.dat$x1,test.dat$x2,col=2-test.y, xlab=\"X1\", ylab=\"X2\")\n\n\n\n\n\n# Performance of model with cost of 0.1 on test set.\nsvmfit = svm(y~., data = train.dat, kernel = 'linear', cost = 0.1)\nypred = predict(svmfit, newdata = test.dat, type = 'response')\ntable_6_1 <- table(predict=ypred, truth=test.dat$y)\nprint(table_6_1)\n\n       truth\npredict  -1   1\n     -1 483  10\n     1    4 503\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_6_1), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 98.6 percent\"\n\n\n\n# Performance of model with cost of 10 on test set.\nsvmfit = svm(y~., data = train.dat, kernel = 'linear', cost = 10)\nypred = predict(svmfit, newdata = test.dat, type = 'response')\ntable_6_2 <- table(predict=ypred, truth=test.dat$y)\nprint(table_6_2)\n\n       truth\npredict  -1   1\n     -1 446  48\n     1   41 465\n\nprint(paste0(\"Accuracy: \", \n            round(100*accuracy_calculation(table_6_2), 2),\n            \" percent\"))\n\n[1] \"Accuracy: 91.1 percent\"\n\n\n\n\n\nIn this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set.\n\nstr(Auto)\n\n'data.frame':   392 obs. of  9 variables:\n $ mpg         : num  18 15 18 16 17 15 14 14 14 15 ...\n $ cylinders   : num  8 8 8 8 8 8 8 8 8 8 ...\n $ displacement: num  307 350 318 304 302 429 454 440 455 390 ...\n $ horsepower  : num  130 165 150 150 140 198 220 215 225 190 ...\n $ weight      : num  3504 3693 3436 3433 3449 ...\n $ acceleration: num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\n $ year        : num  70 70 70 70 70 70 70 70 70 70 ...\n $ origin      : num  1 1 1 1 1 1 1 1 1 1 ...\n $ name        : Factor w/ 304 levels \"amc ambassador brougham\",..: 49 36 231 14 161 141 54 223 241 2 ...\n\n\n\nCreate a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.\n\n\nset.seed(222)\nauto.length = length(Auto$mpg)\nmpg.median = median(Auto$mpg)\nmpg01 = rep(NA,auto.length)\n# Class 1 if car's mpg is above median and 0 if below. Results stored in mpg01 variable.\nfor (i in 1:auto.length) if (Auto$mpg[i] > mpg.median) mpg01[i]=1 else mpg01[i]=0\n# Dataframe\nauto.df = Auto\nauto.df$mpg01 = as.factor(mpg01)\n\n\nFit a support vector classifier to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter.\n\n\n# Using a linear SVM to predict mpg01.\nlinear.tune=tune(svm,mpg01~.,data=auto.df,kernel='linear',\nranges=list(cost=c(0.001,0.01,0.1,1,5,10,100,1000)))\nsummary(linear.tune)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n    1\n\n- best performance: 0.01275641 \n\n- Detailed performance results:\n   cost      error dispersion\n1 1e-03 0.09955128 0.04760888\n2 1e-02 0.07666667 0.04375200\n3 1e-01 0.04596154 0.02359743\n4 1e+00 0.01275641 0.01808165\n5 5e+00 0.01532051 0.01318724\n6 1e+01 0.01788462 0.01234314\n7 1e+02 0.03057692 0.01606420\n8 1e+03 0.03057692 0.01606420\n\nlinear.tune$best.parameters\n\n  cost\n4    1\n\nlinear.tune$best.performance\n\n[1] 0.01275641\n\n\n\nNow repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of gamma and degree and cost.\n\n\n# Using a radial SVM to predict mpg01.\nradial.tune=tune(svm,mpg01~.,data=auto.df,kernel='radial',\nranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))\n# summary(radial.tune)\n\nradial.tune$best.parameters\n\n  cost gamma\n3   10   0.5\n\nradial.tune$best.performance\n\n[1] 0.04820513\n\n\n\n# Using a polynomial SVM to predict mpg01.\npolynomial.tune=tune(svm,mpg01~.,data=auto.df,kernel='polynomial',\nranges=list(cost=c(0.1,1,10,100,1000), degree=c(1,2,3,4,5)))\n# summary(polynomial.tune)\n\npolynomial.tune$best.parameters\n\n  cost degree\n5 1000      1\n\npolynomial.tune$best.performance\n\n[1] 0.01282051\n\n\n\n\n\nThis problem involves the OJ data set which is part of the ISLR2 package.\n\nCreate a training set containing a random sample of 800 observations, and a test set containing the remaining observations.\n\n\nset.seed(131)\n# Training and test sets.\nsample.data = sample.split(OJ$Purchase, SplitRatio = 800/length(OJ$Purchase))\ntrain.set = subset(OJ, sample.data==T)\ntest.set = subset(OJ, sample.data==F)\n\n\nFit a support vector classifier to the training data using cost = 0.01, with Purchase as the response and the other variables as predictors.\n\n\nsvmfit = svm(Purchase~., data = train.set, kernel = \"linear\", cost=0.01)\nsummary(svmfit)\n\n\nCall:\nsvm(formula = Purchase ~ ., data = train.set, kernel = \"linear\", \n    cost = 0.01)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  0.01 \n\nNumber of Support Vectors:  438\n\n ( 220 218 )\n\n\nNumber of Classes:  2 \n\nLevels: \n CH MM\n\n\n\nWhat are the training and test error rates?\n\n\n# Predictions on training set\nsvm.pred = predict(svmfit, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n\n       truth\npredict  CH  MM\n     CH 435  76\n     MM  53 236\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n\n[1] \"Error rate: 16.12 percent\"\n\n\n\n# Predictions on test set\nsvm.pred = predict(svmfit, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n\n       truth\npredict  CH  MM\n     CH 151  36\n     MM  14  69\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n\n[1] \"Error rate: 18.52 percent\"\n\n\n\nUse the tune() function to select an optimal cost.\n\n\n# Using cross validation to select optimal cost\nset.seed(131)\ntune.out = tune(svm, Purchase~., data = train.set, kernel = \"linear\",\nranges=list(cost=c(0.01,0.1,0.5,1,10)))\n\n\nCompute the training and test error rates using this new value for cost.\n\n\n# Predictions on training set\nsvm.pred = predict(tune.out$best.mod, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n\n       truth\npredict  CH  MM\n     CH 432  72\n     MM  56 240\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n\n[1] \"Error rate: 16 percent\"\n\n\n\n# Predictions on test set\nsvm.pred = predict(tune.out$best.mod, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n\n       truth\npredict  CH  MM\n     CH 151  34\n     MM  14  71\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n\n[1] \"Error rate: 17.78 percent\"\n\n\n\nRepeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma.\n\n\nset.seed(131)\ntune.out = tune(svm, Purchase~., data = train.set, kernel = \"radial\",\nranges=list(cost=c(0.01,0.1,0.5,1,10)))\n\n\n# Predictions on training set\nsvm.pred = predict(tune.out$best.mod, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n\n       truth\npredict  CH  MM\n     CH 450  81\n     MM  38 231\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n\n[1] \"Error rate: 14.88 percent\"\n\n\n\n# Predictions on test set\nsvm.pred = predict(tune.out$best.mod, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n\n       truth\npredict  CH  MM\n     CH 150  37\n     MM  15  68\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n\n[1] \"Error rate: 19.26 percent\"\n\n\n\nRepeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set degree = 2.\n\n\nset.seed(131)\ntune.out = tune(svm, Purchase~., data = train.set, kernel = \"polynomial\",\nranges=list(cost=c(0.01,0.1,0.5,1,10)), degree=2)\n\n\n# Predictions on training set\nsvm.pred = predict(tune.out$best.mod, train.set)\ntable_8_train <- table(predict=svm.pred, truth=train.set$Purchase)\nprint(table_8_train)\n\n       truth\npredict  CH  MM\n     CH 455  76\n     MM  33 236\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_train), 2),\n            \" percent\"))\n\n[1] \"Error rate: 13.62 percent\"\n\n\n\n# Predictions on test set\nsvm.pred = predict(tune.out$best.mod, test.set)\ntable_8_test <- table(predict=svm.pred, truth=test.set$Purchase)\nprint(table_8_test)\n\n       truth\npredict  CH  MM\n     CH 148  41\n     MM  17  64\n\nprint(paste0(\"Error rate: \", \n            round(100 - 100*accuracy_calculation(table_8_test), 2),\n            \" percent\"))\n\n[1] \"Error rate: 21.48 percent\""
  },
  {
    "objectID": "posts/inequalities/inequalities.html",
    "href": "posts/inequalities/inequalities.html",
    "title": "Inequalities",
    "section": "",
    "text": "There are probably several ways to graph mathematical inequalities in R. Here, I will simply try a brute force method of literally plotting many dots.\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\n\nHere, I will simply try a brute force method of literally plotting many dots.\n\nN &lt;- 100 #resolution\nx &lt;- seq(-10, 10, length.out = N)\ny &lt;- seq(-10, 10, length.out = N)\n\ndf &lt;- expand.grid(x,y)\ncolnames(df) &lt;- c(\"xval\", \"yval\")\n\nToday’s tasks come from chapter 9 of the popular ISLR textbook\n\nConceptual Task 1a\n\ndf1 &lt;- df |&gt;\n  # math function\n  mutate(shade = ifelse(yval &gt; 3*xval + 1, \"blue\", \"red\"))\n\n\ndf1 |&gt;\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # main line\n  geom_segment(aes(x = -(11/3), y = -10, xend = 3, yend = 10),\n               color = \"black\", linewidth = 3) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = 3x + 1\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\nConceptual Task 1b\n\ndf1b &lt;- df |&gt;\n  # math function\n  mutate(shade = ifelse(yval &gt; 1 - 0.5*xval, \"blue\", \"red\"))\n\n\ndf1b |&gt;\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # main line\n  geom_segment(aes(x = -10, y = 6, xend = 10, yend = -4),\n               color = \"black\", linewidth = 3) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = 1 - 0.5x\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\nConceptual Task 2\n\neuclidean_distance &lt;- function(x1, y1, x2, y2){\n  # computes the Euclidean distance between (x1, y1) and (x2, y2)\n  sqrt( (x2 - x1)^2 + (y2 - y1)^2 )\n}\n\n\ndf2 &lt;- df |&gt;\n  # math function\n  mutate(shade = ifelse(euclidean_distance(xval, yval, -1, 2) &gt; 4, \n                        \"blue\", \"red\"))\n\n\ndf2 |&gt;\n  ggplot() +\n  \n  # shaded regions\n  geom_point(aes(x = xval, y = yval, color = shade),\n             alpha = 0.5) +\n  scale_color_identity() +\n  \n  # axes\n  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),\n               alpha = 0.25, color = \"gray75\", linewidth = 2) +\n  \n  # customization\n  coord_equal() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"(x+1)^2 + (y-2)^2 = 4\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nifelse(euclidean_distance(0, 0, -1, 2) &gt; 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(-1, 1, -1, 2) &gt; 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(2, 2, -1, 2) &gt; 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(3, 8, -1, 2) &gt; 4, \"blue\", \"red\")\n\n[1] \"blue\"\n\n\n\n\nConceptual Task 3\n\n\n\n\nobs &lt;- 1:7\nxvals &lt;- c(3,2,4,1,2,4,4)\nyvals &lt;- c(4,2,4,4,1,3,1)\nclass_label &lt;- c(\"Red\", \"Red\", \"Red\", \"Red\", \"Blue\", \"Blue\", \"Blue\")\ndf3 &lt;- data.frame(obs, xvals, yvals, class_label)\n\n\ndf3 |&gt;\n  ggplot() +\n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n  coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Where to Draw the Separating Hyperplane?\",\n       # subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\ndf3 |&gt;\n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  # margins\n  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n    coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\ndf3e &lt;- df3 |&gt;\n  mutate(supp_vec = ifelse(obs %in% c(2,3,5,6), \n                           \"support vector\", \"other data\"))\n  \ndf3e$supp_vec &lt;- factor(df3e$supp_vec,\n                        levels = c(\"support vector\", \"other data\"))\n  \ndf3e |&gt;  \n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  # margins\n  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = supp_vec),\n             size = 5) +\n    coord_equal() +\n  scale_color_manual(values = c(\"purple\", \"gray50\")) +\n  labs(title = \"Separating Hyperplane\",\n       subtitle = \"y = x - 0.5\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\ndf3 |&gt;\n  ggplot() +\n  \n  # separating hyperplane\n  geom_segment(aes(x = 0, y = 1/4, xend = 5, yend = 16/4),\n               color = \"black\", linetype = 1, linewidth = 3) +\n  \n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n    coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Separating Hyperplane (not optimal)\",\n       subtitle = \"y = 0.25(3x+1)\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\n\n\n\nnew_dot &lt;- data.frame(obs = 8, xvals = 0, yvals = 5, class_label = \"blue\")\ndf3h &lt;- rbind(df3, new_dot)\ndf3h |&gt;\n  ggplot() +\n  geom_point(aes(x = xvals, y = yvals, color = class_label),\n             size = 5) +\n  coord_equal() +\n  scale_color_identity() +\n  labs(title = \"Where to Draw the Separating Hyperplane?\",\n       subtitle = \"new data at (0,5)\",\n       caption = \"ISLR\",\n       x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  xlim(0,5) + ylim(0,5)\n\n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.2 dplyr_1.1.2  \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       cli_3.6.1         knitr_1.43        rlang_1.1.1      \n [5] xfun_0.39         generics_0.1.3    jsonlite_1.8.4    labeling_0.4.2   \n [9] glue_1.6.2        colorspace_2.1-0  htmltools_0.5.5   scales_1.2.1     \n[13] fansi_1.0.4       rmarkdown_2.22    grid_4.3.0        munsell_0.5.0    \n[17] evaluate_0.21     tibble_3.2.1      fastmap_1.1.1     yaml_2.3.7       \n[21] lifecycle_1.0.3   compiler_4.3.0    htmlwidgets_1.6.2 pkgconfig_2.0.3  \n[25] rstudioapi_0.14   farver_2.1.1      digest_0.6.31     R6_2.5.1         \n[29] tidyselect_1.2.0  utf8_1.2.3        pillar_1.9.0      magrittr_2.0.3   \n[33] withr_2.5.0       tools_4.3.0       gtable_0.3.3"
  },
  {
    "objectID": "posts/spatialr/07_introduction-to-sf-and-stars.html",
    "href": "posts/spatialr/07_introduction-to-sf-and-stars.html",
    "title": "The Median Data Scientist",
    "section": "",
    "text": "North Carolina counties\n\n\n]\n\nlibrary(\"cubelyr\")\nlibrary(\"dplyr\") #data wrangling\nlibrary(\"MASS\")  #linear discriminant analysis\nlibrary(\"sf\")    #simple features\nlibrary(\"spDataLarge\") #contains Bristol data\nlibrary(\"stars\") #useful for rasters and data cubes\nlibrary(\"tidyr\") #pivoting\nlibrary(\"units\")\nlibrary(\"xts\")   #time series tools\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] xts_0.13.1        zoo_1.8-12        units_0.8-2       tidyr_1.3.0      \n [5] stars_0.6-1       abind_1.4-5       spDataLarge_2.0.9 sf_1.0-13        \n [9] MASS_7.3-58.4     dplyr_1.1.2       cubelyr_1.0.2    \n\nloaded via a namespace (and not attached):\n [1] lwgeom_0.2-13      jsonlite_1.8.4     compiler_4.3.0     tidyselect_1.2.0  \n [5] Rcpp_1.0.10        parallel_4.3.0     fastmap_1.1.1      lattice_0.21-8    \n [9] R6_2.5.1           generics_0.1.3     classInt_0.4-9     knitr_1.43        \n[13] htmlwidgets_1.6.2  tibble_3.2.1       DBI_1.1.3          pillar_1.9.0      \n[17] rlang_1.1.1        utf8_1.2.3         xfun_0.39          cli_3.6.1         \n[21] magrittr_2.0.3     class_7.3-21       digest_0.6.31      grid_4.3.0        \n[25] rstudioapi_0.14    lifecycle_1.0.3    vctrs_0.6.2        KernSmooth_2.23-20\n[29] proxy_0.4-27       evaluate_0.21      glue_1.6.2         fansi_1.0.4       \n[33] e1071_1.7-13       rmarkdown_2.22     purrr_1.0.1        tools_4.3.0       \n[37] pkgconfig_2.0.3    htmltools_0.5.5   \n\n\n\n\nsf provides simple feature access\n\nreplaces packages sp, rgeos, rgdal\ninterface with tidyverse\ngeometrical operations with GEOS or s2geometry\ncoordinate transformations\n\nsf objects have\n\ndata.frame or tibble\nsfc: geometry list-column\n\n\n\n\nsf_column: the name of the (active) geometry column\nagr: attribute-geometry relationship\n\n\n\n\nhow to read a summary of an sf object\n\n\n\n\n\n\n\n# reading in an sf object\nnc <- st_read(system.file(\"gpkg/nc.gpkg\", package = \"sf\"))\n\nReading layer `nc.gpkg' from data source \n  `C:\\Users\\freex\\AppData\\Local\\R\\win-library\\4.3\\sf\\gpkg\\nc.gpkg' \n  using driver `GPKG'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n\n\n\n\nnc[2:5, 3:7] #records 2-5 and columns 3-7 \n\n\n\n\nsubset of North Carolina\n\n\n\n\nCode\n\n\nnc5 <- nc[1:5, ]\nnc7 <- nc[1:7, ]\nplot(st_geometry(nc7))\nplot(st_geometry(nc5), add = TRUE, border = \"brown\")\ncc = st_coordinates(st_centroid(st_geometry(nc7)))\ntext(cc, labels = 1:nrow(nc7), col = \"blue\")\n\n\n\n\n\n\nthe drop argument is by default FALSE meaning that the geometry column is always selected\nselection with a spatial (sf, sfc, or sfg) object as first argument leads to selection of the features that spatially intersect with that object\n\n\n(i <- st_intersects(nc5, nc7))\n\nSparse geometry binary predicate list of length 5, where the predicate\nwas `intersects'\n 1: 1, 2\n 2: 1, 2, 3\n 3: 2, 3\n 4: 4, 7\n 5: 5, 6\n\n\n\nas.matrix(i)\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]\n[1,]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n[2,]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[3,] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE\n[5,] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n\n\n\n# The object i is of class sgbp (sparse geometrical binary predicate)\nclass(i)\n\n[1] \"sgbp\" \"list\"\n\n\n\nmethods(class = \"sgbp\")\n\n [1] as.data.frame as.matrix     coerce        dim           initialize   \n [6] Ops           print         show          slotsFromS3   t            \nsee '?methods' for accessing help and source code\n\n\n\n\n\n\nPackage sf has tidyverse-style read and write functions, read_sf and write_sf that\n\nreturn a tibble rather than a data.frame\ndo not print any output\noverwrite existing data by default\n\nThe dplyr, ggplot2, and tidyr capabilities are probably familiar for this audience, but there are some extra considerations for sf data below.\n\n\nThe summarise method for sf objects has two special arguments:\n\ndo_union (default TRUE) determines whether grouped geometries are unioned on return, so that they form a valid geometry\nis_coverage (default FALSE) in case the geometries grouped form a coverage (do not have overlaps), setting this to TRUE speeds up the unioning\n\n\n\n\nWe can use filter for spatial predicates. For example, to select all counties less than 50 km away from Orange County,\n\n# filters\norange <- nc |> dplyr::filter(NAME == \"Orange\")\nwd <- st_is_within_distance(nc, orange, \n                            units::set_units(50, km))\no50 <- nc |> dplyr::filter(lengths(wd) > 0)\n\n# plot\nog <- st_geometry(orange)\nbuf50 <- st_buffer(og, units::set_units(50, km))\nall <- c(buf50, st_geometry(o50))\nplot(st_geometry(o50), lwd = 2, extent = all)\nplot(og, col = 'orange', add = TRUE)\nplot(buf50, add = TRUE, col = NA, border = 'brown')\nplot(st_geometry(nc), add = TRUE, border = 'grey')\n\n\n\n\n\n\n\nThe distinct method selects distinct records, where st_equals is used to evaluate distinctness of geometries.\n\n\n\n\n\nThe concepts of “left”, “right”, “inner”, or “full” joins remain\n\nWhen using spatial joins, each record may have several matched records … A way to reduce this complexity may be to select from the matching records the one with the largest overlap with the target geometry.\n\n\nLoading shapefile and grid\n\n\nsystem.file(\"shape/nc.shp\", package=\"sf\") |> \n    read_sf() |>\n    st_transform('EPSG:2264') -> nc\ngr <- st_sf(\n         label = apply(expand.grid(1:10, LETTERS[10:1])[,2:1], 1, paste0, collapse = \"\"),\n         geom = st_make_grid(nc))\ngr$col <- sf.colors(10, categorical = TRUE, alpha = .3)\n# cut, to verify that NA's work out:\ngr <- gr[-(1:30),]\n\n\n\nnc_j <- st_join(nc, gr, largest = TRUE)\n\n\n\n\nspatial join\n\n\n\n\nimage code\n\n\npar(mfrow = c(2,1), mar = rep(0,4))\nplot(st_geometry(nc_j), border = 'grey')\nplot(st_geometry(gr), add = TRUE, col = gr$col)\ntext(st_coordinates(st_centroid(st_geometry(gr))), labels = gr$label, cex = .85)\n# the joined dataset:\nplot(st_geometry(nc_j), border = 'grey', col = nc_j$col)\ntext(st_coordinates(st_centroid(st_geometry(nc_j))), labels = nc_j$label, cex = .7)\nplot(st_geometry(gr), border = '#88ff88aa', add = TRUE)\n\n\n\n\n\nAn sf object can be created from scratch by\n\np1 <- st_point(c(7.35, 52.42))\np2 <- st_point(c(7.22, 52.18))\np3 <- st_point(c(7.44, 52.19))\nsfc <- st_sfc(list(p1, p2, p3), crs = 'OGC:CRS84')\nst_sf(elev = c(33.2, 52.1, 81.2), \n      marker = c(\"Id01\", \"Id02\", \"Id03\"), geom = sfc)\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7.22 ymin: 52.18 xmax: 7.44 ymax: 52.42\nGeodetic CRS:  WGS 84\n  elev marker               geom\n1 33.2   Id01 POINT (7.35 52.42)\n2 52.1   Id02 POINT (7.22 52.18)\n3 81.2   Id03 POINT (7.44 52.19)\n\n\nConvenience functions\n\nst_sample\nst_make_grid\nst_interpoloate_aw\n\n\n\n\n\n\"POINT(50 50.1)\" |> st_as_sfc(crs = \"OGC:CRS84\") -> pt\n\n\n\n\nnew versus old methods\n\n\n\n\nimage code\n\n\npar(mfrow = c(1, 2))\npar(mar = c(2.1, 2.1, 1.2, .5))\northo <- st_crs(\"+proj=ortho +lon_0=50 +lat_0=45\")\npol |> st_transform(ortho) |> plot(axes = TRUE, graticule = TRUE, \n                                   main = 's2geometry')\npt |> st_transform(ortho) |> plot(add = TRUE, pch = 16, col = 'red')\n# second plot:\nplot(pol, axes = TRUE, graticule = TRUE, main = 'GEOS')\nplot(pt, add = TRUE, pch = 16, col = 'red')\n\n\nBy default, sf uses geometrical operations from the s2geometry library, interfaced through the s2 package\n\n\"POLYGON((40 40, 60 40, 60 50, 40 50, 40 40))\" |>\n  st_as_sfc(crs = \"OGC:CRS84\") -> pol\nst_intersects(pt, pol) #TRUE\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `intersects'\n 1: 1\n\n\nIf one wants sf to use ellipsoidal coordinates as if they are Cartesian coordinates, the use of s2 can be switched off\n\nold <- sf_use_s2(FALSE)\n# Spherical geometry (s2) switched off\nst_intersects(pol, pt)\n# although coordinates are longitude/latitude, st_intersects assumes\n# that they are planar\n# Sparse geometry binary predicate list of length 1, where the\n# predicate was `intersects'\n#  1: (empty)\nsf_use_s2(old) # restore\n# Spherical geometry (s2) switched on\n\n\n\n\n\nFor the classic GIS view of raster layers, terra package\n\nCapabilities of stars include\n\nallows for representing dynamic (time varying) raster stacks\naims at being scalable, also beyond local disk size\nprovides a strong integration of raster functions in the GDAL library\nhandles, in addition to regular grids, rotated, sheared, rectilinear, and curvilinear rasters\nprovides a tight integration with package sf\nhandles array data with non-raster spatial dimensions, the vector data cubes\nfollows the tidyverse design principles\n\n\n\n\n\nr <- read_stars(system.file(\"tif/L7_ETMs.tif\", package = \"stars\"))\nr\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max.\nL7_ETMs.tif     1      54     69 68.91242      86  255\ndimension(s):\n     from  to  offset delta                     refsys point x/y\nx       1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny       1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\nband    1   6      NA    NA                         NA    NA    \n\n\n\n\nattributes\n\n\nfrom: starting index\nto: ending index\noffset: dimension value at the start (edge) of the first pixel\ndelta: cell size; negative delta values indicate that pixel index increases with decreasing dimension values\nrefsys: reference system\npoint: logical, indicates whether cell values have point support or cell support\nx/y: indicates whether a dimension is associated with a spatial raster x- or y-axis\n\n\n\n\n\nplot(r)\n\n\n\n\n\npar(mfrow = c(1, 2))\nplot(r, rgb = c(3,2,1), reset = FALSE, main = \"RGB\")    # rgb\nplot(r, rgb = c(4,3,2), main = \"False colour (NIR-R-G)\") # false colour\n\n\n\n\n\n\n\nExample: selects from r\n\nattributes 1-2, index 101-200 for dimension 1, and index 5-10 for dimension 3\nomitting dimension 2 means that no subsetting takes place\n\n\nr[1:2, 101:200,, 5:10]\n\nSelecting discontinuous ranges is supported only when it is a regular sequence\n\nr[,1:100, seq(1, 250, 5), 4] |> dim() #default behavior\n\n   x    y band \n 100   50    1 \n\nr[,1:100, seq(1, 250, 5), 4, drop = TRUE] |> dim()\n\n  x   y \n100  50 \n\n\nFor selecting particular ranges of dimension values, one can use dplyr::filter\n\nfilter(r, x > 289000, x < 290000)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median    Mean 3rd Qu. Max.\nL7_ETMs.tif     5      51     63 64.3337      75  242\ndimension(s):\n     from  to  offset delta                     refsys point x/y\nx       1  35  289004  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny       1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\nband    1   6       1     1                         NA    NA    \n\n\nor slice\n\nslice(r, band, 3)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max.\nL7_ETMs.tif    21      49     63 64.35886      77  255\ndimension(s):\n  from  to  offset delta                     refsys point x/y\nx    1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny    1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n\n\n\n\n\n\nb <- st_bbox(r) |>\n    st_as_sfc() |>\n    st_centroid() |>\n    st_buffer(units::set_units(500, m))\nr[b]\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max. NA's\nL7_ETMs.tif    22      54     66 67.68302   78.25  174 2184\ndimension(s):\n     from  to  offset delta                     refsys point x/y\nx     157 193  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny     159 194 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\nband    1   6      NA    NA                         NA    NA    \n\n\n\n\n\nCircular centre region of the Landsat 7 scene (band 1)\n\n\n\n\nimage code\n\n\nplot(r[b][,,,1], reset = FALSE)\nplot(b, border = 'brown', lwd = 2, col = NA, add = TRUE)\n\nBy default, the resulting raster is cropped to the extent of the selection object; otherwise\n\nr[b, crop = FALSE]\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max.   NA's\nL7_ETMs.tif    22      54     66 67.68302   78.25  174 731280\ndimension(s):\n     from  to  offset delta                     refsys point x/y\nx       1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny       1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\nband    1   6      NA    NA                         NA    NA    \n\n\n\n# we can reset dimension offsets\nr[b] |> st_normalize() |> st_dimensions()\n\n     from to  offset delta                     refsys point x/y\nx       1 37  293222  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny       1 36 9116258 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\nband    1  6      NA    NA                         NA    NA    \n\n\n\nor simply using the stars function made for cropping\n\nst_crop(r, b)\n\n\n\n\n\n\naperm: transposes an array by permuting the order of dimensions\nAttributes and dimensions can be swapped, using split and merge\n\n\n(rs <- split(r))\n\nstars object with 2 dimensions and 6 attributes\nattribute(s):\n    Min. 1st Qu. Median     Mean 3rd Qu. Max.\nX1    47      67     78 79.14772      89  255\nX2    32      55     66 67.57465      79  255\nX3    21      49     63 64.35886      77  255\nX4     9      52     63 59.23541      75  255\nX5     1      63     89 83.18266     112  255\nX6     1      32     60 59.97521      88  255\ndimension(s):\n  from  to  offset delta                     refsys point x/y\nx    1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny    1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n\n\n\nmerge(rs, name = \"band\") |> setNames(\"L7_ETMs\")\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n         Min. 1st Qu. Median     Mean 3rd Qu. Max.\nL7_ETMs     1      54     69 68.91242      86  255\ndimension(s):\n     from  to  offset delta                     refsys point    values x/y\nx       1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE      NULL [x]\ny       1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE      NULL [y]\nband    1   6      NA    NA                         NA    NA X1,...,X6    \n\n\n\nMultiple stars object with identical dimensions can be combined using c\n\n\n\n\nA very common use case for raster data cube analysis is the extraction of values at certain locations, or computing aggregations over certain geometries\n\nset.seed(115517)\npts <- st_bbox(r) |> st_as_sfc() |> st_sample(20)\n(e <- st_extract(r, pts))\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n             Min. 1st Qu. Median     Mean 3rd Qu. Max.\nL7_ETMs.tif    12   41.75     63 60.95833    80.5  145\ndimension(s):\n         from to                     refsys point\ngeometry    1 20 SIRGAS 2000 / UTM zone 25S  TRUE\nband        1  6                         NA    NA\n                                                        values\ngeometry POINT (293002.2 9115516),...,POINT (290941.1 9114128)\nband                                                      NULL\n\n\n\n\n\n\n\n\nRandomly chosen sample locations for training data; red: water, yellow: land\n\n\n\n\nimage code\n\n\nplot(r[,,,1], reset = FALSE)\ncol <- rep(\"yellow\", 20)\ncol[c(8, 14, 15, 18, 19)] = \"red\"\nst_as_sf(e) |> st_coordinates() |> text(labels = 1:20, col = col)\n\n\n\nrs <- split(r)\ntrn <- st_extract(rs, pts)\ntrn$cls <- rep(\"land\", 20)\ntrn$cls[c(8, 14, 15, 18, 19)] <- \"water\"\nmodel <- MASS::lda(cls ~ ., st_drop_geometry(trn)) #linear discriminant analysis\npr <- predict(rs, model)\nplot(pr[1], key.pos = 4, key.width = lcm(3.5), key.length = lcm(2))\n\n\n\n\n\n\n\n\nlogarithmic transformation: log(r)\nmask:\n\n\nr2 <- r\nr2[r < 50] <- NA\n\n\nunmask:\n\n\nr2[is.na(r2)] <- 0\n\n\n\n\nDimension-wise, we can apply functions to selected array dimensions\n\nst_apply(r, c(\"x\", \"y\"), mean)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n      Min.  1st Qu.   Median     Mean 3rd Qu. Max.\nmean  25.5 53.33333 68.33333 68.91242      82  255\ndimension(s):\n  from  to  offset delta                     refsys point x/y\nx    1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny    1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n\n\nExample: NDVI (normalised differenced vegetation index)\n\nndvi <- function(b1, b2, b3, b4, b5, b6) (b4 - b3)/(b4 + b3)\nst_apply(r, c(\"x\", \"y\"), ndvi)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n            Min.    1st Qu.      Median        Mean   3rd Qu.      Max.\nndvi  -0.7534247 -0.2030075 -0.06870229 -0.06432464 0.1866667 0.5866667\ndimension(s):\n  from  to  offset delta                     refsys point x/y\nx    1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\ny    1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n\n\nAggregation over a temporal dimension is done by passing a time variable as the second argument to aggregate, as a\n\nset of time stamps indicating the start of time intervals,\nset of time intervals defined by make_intervals, or\ntime period like \"weeks\", \"5 days\", or \"years\".\n\n\n\n\nWe use a small excerpt from the European air quality data base to illustrate aggregation operations on vector data cubes. The same data source was used by Gräler, Pebesma, and Heuvelink (2016)\n\nload(\"data/air.rda\")\nde_nuts1 <- read_sf(\"data/de_nuts1.gpkg\")\ndim(air)\n\nspace  time \n   70  4383 \n\n\n\nstations |>\n    st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) |>\n    st_geometry() -> st\nd <- st_dimensions(station = st, time = dates)\n(aq <- st_as_stars(list(PM10 = air), dimensions = d))\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n      Min. 1st Qu. Median     Mean 3rd Qu.    Max.   NA's\nPM10     0   9.921 14.792 17.69728  21.992 274.333 157659\ndimension(s):\n        from   to     offset  delta refsys point\nstation    1   70         NA     NA WGS 84  TRUE\ntime       1 4383 1998-01-01 1 days   Date FALSE\n                                                         values\nstation POINT (9.585911 53.67057),...,POINT (9.446661 49.24068)\ntime                                                       NULL\n\n\n\n\n\nsparse space-time diagram of PM10 measurements by time and station\n\n\n\n\nimage code\n\n\npar(mar = c(5.1, 4.1, 0.3, 0.1))\nimage(aperm(log(aq), 2:1), main = NULL)\n\n\n\n\n\nimage code\n\n\nst_as_sf(st_apply(aq, 1, mean, na.rm = TRUE)) |>\n    plot(reset = FALSE, pch = 16, extent = de_nuts1)\nst_union(de_nuts1) |> plot(add = TRUE)\n\n\nExample: aggregate these station time series to area means\n\n(a <- aggregate(aq, de_nuts1, mean, na.rm = TRUE))\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n        Min. 1st Qu.   Median     Mean  3rd Qu.     Max.  NA's\nPM10  1.0752 10.8855 15.31625 17.88823 21.81086 172.2665 25679\ndimension(s):\n     from   to     offset  delta refsys point\ngeom    1   16         NA     NA WGS 84 FALSE\ntime    1 4383 1998-01-01 1 days   Date FALSE\n                                                            values\ngeom MULTIPOLYGON (((9.65046 4...,...,MULTIPOLYGON (((10.77189 ...\ntime                                                          NULL\n\n\nExample: show the maps for six arbitrarily chosen days\n\na |> filter(time >= \"2008-01-01\", time < \"2008-01-07\") |> \n    plot(key.pos = 4)\n\n\n\n\nExample: time series plot of mean values for a single state\n\nplot(as.xts(a)[,4], main = de_nuts1$NAME_1[4])\n\n\n\n\n\n\n\n\n\n\nOrigin destination data zones for Bristol, UK, with zone 33 (E02003043) coloured red\n\n\n\n\nimage code\n\n\nplot(st_geometry(bristol_zones), axes = TRUE, graticule = TRUE)\nplot(st_geometry(bristol_zones)[33], col = 'red', add = TRUE)\n\n\n\nhead(bristol_od)\n\n# A tibble: 6 × 7\n  o         d           all bicycle  foot car_driver train\n  <chr>     <chr>     <dbl>   <dbl> <dbl>      <dbl> <dbl>\n1 E02002985 E02002985   209       5   127         59     0\n2 E02002985 E02002987   121       7    35         62     0\n3 E02002985 E02003036    32       2     1         10     1\n4 E02002985 E02003043   141       1     2         56    17\n5 E02002985 E02003049    56       2     4         36     0\n6 E02002985 E02003054    42       4     0         21     0\n\n\n\n\n\n# create O-D-mode array:\nbristol_tidy <- bristol_od |> \n    dplyr::select(-all) |> #caution: MASS also has select()\n    pivot_longer(3:6, names_to = \"mode\", values_to = \"n\")\nhead(bristol_tidy)\n\n# A tibble: 6 × 4\n  o         d         mode           n\n  <chr>     <chr>     <chr>      <dbl>\n1 E02002985 E02002985 bicycle        5\n2 E02002985 E02002985 foot         127\n3 E02002985 E02002985 car_driver    59\n4 E02002985 E02002985 train          0\n5 E02002985 E02002987 bicycle        7\n6 E02002985 E02002987 foot          35\n\n\nNext, we form the three-dimensional array\n\nfilled with zeroes (at first)\ndimensions are named with the zone names (o, d) and the transportation mode name (mode)\nensure order of observations in bristol_zones and bristol_tidy\n\n\nod <- bristol_tidy |> pull(\"o\") |> unique()\nnod <- length(od)\nmode <- bristol_tidy |> pull(\"mode\") |> unique()\nnmode = length(mode)\na = array(0L,  c(nod, nod, nmode), \n    dimnames = list(o = od, d = od, mode = mode))\n\na[as.matrix(bristol_tidy[c(\"o\", \"d\", \"mode\")])] <-  bristol_tidy$n\n\norder <- match(od, bristol_zones$geo_code)\nzones <- st_geometry(bristol_zones)[order]\n\n\n\n\n\nd <- st_dimensions(o = zones, d = zones, mode = mode)\nodm <- st_as_stars(list(N = a), dimensions = d)\n\n\n# plot, sliced for destination zone 33\nplot(adrop(odm[,,33]) + 1, logz = TRUE)\n\n\n\n\nExample: largest number of travellers as its destination\n\nd <- st_apply(odm, 2, sum)\nwhich.max(d[[1]])\n\n[1] 33\n\n\nExample: Total transportation by OD\n\nst_apply(odm, 1:2, sum)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n     Min. 1st Qu. Median     Mean 3rd Qu. Max.\nsum     0       0      0 19.20636      19 1434\ndimension(s):\n  from  to refsys point\no    1 102 WGS 84 FALSE\nd    1 102 WGS 84 FALSE\n                                                         values\no MULTIPOLYGON (((-2.510462...,...,MULTIPOLYGON (((-2.55007 ...\nd MULTIPOLYGON (((-2.510462...,...,MULTIPOLYGON (((-2.55007 ...\n\n\nExample: Origin totals, by mode\n\nst_apply(odm, c(1,3), sum)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n     Min. 1st Qu. Median     Mean 3rd Qu. Max.\nsum     1    57.5  214.5 489.7623     771 2903\ndimension(s):\n     from  to refsys point\no       1 102 WGS 84 FALSE\nmode    1   4     NA FALSE\n                                                            values\no    MULTIPOLYGON (((-2.510462...,...,MULTIPOLYGON (((-2.55007 ...\nmode                                             bicycle,...,train\n\n\nExample: Destination totals, by mode\n\nst_apply(odm, c(2,3), sum)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n     Min. 1st Qu. Median     Mean 3rd Qu.  Max.\nsum     0      13  103.5 489.7623  408.25 12948\ndimension(s):\n     from  to refsys point\nd       1 102 WGS 84 FALSE\nmode    1   4     NA FALSE\n                                                            values\nd    MULTIPOLYGON (((-2.510462...,...,MULTIPOLYGON (((-2.55007 ...\nmode                                             bicycle,...,train\n\n\nPlotting the origins and destinations:\n\no <- st_apply(odm, 1, sum)\nd <- st_apply(odm, 2, sum)\nx <- (c(o, d, along = list(od = c(\"origin\", \"destination\"))))\nplot(x, logz = TRUE)\n\n\n\n\n\n\n\n\na <- set_units(st_area(st_as_sf(o)), km^2)\no$sum_km <- o$sum / a\nd$sum_km <- d$sum / a\nod <- c(o[\"sum_km\"], d[\"sum_km\"], along = \n        list(od = c(\"origin\", \"destination\")))\nplot(od, logz = TRUE)\n\n\n\n# Total commutes per square km, by area of origin (left) or destination (right)\n\n\n\n\n\n\nfile <- system.file(\"gpkg/nc.gpkg\", package=\"sf\")\nread_sf(file) |> \n  st_geometry() |>\n  st_as_stars() |>\n  plot(key.pos = 4)\n\n\n\n\nRasterising existing features is done using st_rasterize\n\nread_sf(file) |>\n  dplyr::mutate(name = as.factor(NAME)) |>\n  dplyr::select(SID74, SID79, name) |>\n  st_rasterize()\n\nstars object with 2 dimensions and 3 attributes\nattribute(s):\n     SID74            SID79             name       \n Min.   : 0.000   Min.   : 0.000   Sampson :  655  \n 1st Qu.: 3.000   1st Qu.: 3.000   Columbus:  648  \n Median : 5.000   Median : 6.000   Robeson :  648  \n Mean   : 7.892   Mean   : 9.584   Bladen  :  604  \n 3rd Qu.:10.000   3rd Qu.:13.000   Wake    :  590  \n Max.   :44.000   Max.   :57.000   (Other) :30952  \n NA's   :30904    NA's   :30904    NA's    :30904  \ndimension(s):\n  from  to   offset      delta refsys point x/y\nx    1 461 -84.3239  0.0192484  NAD27 FALSE [x]\ny    1 141  36.5896 -0.0192484  NAD27 FALSE [y]\n\n\nSimilarly, line and point geometries can be rasterised\n\nread_sf(file) |>\n  st_cast(\"MULTILINESTRING\") |>\n  dplyr::select(CNTY_ID) |>\n  st_rasterize() |>\n  plot(key.pos = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeeting chat log\n\nLOG"
  },
  {
    "objectID": "posts/bayesrules/14_Naive_Bayes_Classification.html",
    "href": "posts/bayesrules/14_Naive_Bayes_Classification.html",
    "title": "The Median Data Scientist",
    "section": "",
    "text": "Learning objectives:\n\nexplore the pros and cons of naive Bayes classification\ngeneralize classification tasks for more than two categories\n\n\nlibrary(\"bayesrules\")\nlibrary(\"dplyr\")\nlibrary(\"e1071\")\nlibrary(\"ggplot2\")\nlibrary(\"ggtext\")\nlibrary(\"janitor\")\nlibrary(\"tidyr\")\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] tidyr_1.3.0      janitor_2.2.0    ggtext_0.1.2     ggplot2_3.4.2   \n[5] e1071_1.7-13     dplyr_1.1.2      bayesrules_0.0.2\n\nloaded via a namespace (and not attached):\n [1] gridExtra_2.3       inline_0.3.19       rlang_1.1.1        \n [4] magrittr_2.0.3      snakecase_0.11.0    matrixStats_1.0.0  \n [7] compiler_4.3.0      loo_2.6.0           callr_3.7.3        \n[10] vctrs_0.6.2         reshape2_1.4.4      stringr_1.5.0      \n[13] pkgconfig_2.0.3     crayon_1.5.2        fastmap_1.1.1      \n[16] ellipsis_0.3.2      utf8_1.2.3          threejs_0.3.3      \n[19] promises_1.2.0.1    rmarkdown_2.22      markdown_1.7       \n[22] ps_1.7.5            nloptr_2.0.3        purrr_1.0.1        \n[25] xfun_0.39           jsonlite_1.8.4      later_1.3.1        \n[28] parallel_4.3.0      prettyunits_1.1.1   R6_2.5.1           \n[31] dygraphs_1.1.1.6    stringi_1.7.12      StanHeaders_2.26.26\n[34] boot_1.3-28.1       lubridate_1.9.2     Rcpp_1.0.10        \n[37] rstan_2.21.8        knitr_1.43          zoo_1.8-12         \n[40] base64enc_0.1-3     bayesplot_1.10.0    httpuv_1.6.11      \n[43] Matrix_1.5-4        splines_4.3.0       igraph_1.4.3       \n[46] timechange_0.2.0    tidyselect_1.2.0    rstudioapi_0.14    \n[49] codetools_0.2-19    miniUI_0.1.1.1      processx_3.8.1     \n[52] pkgbuild_1.4.0      lattice_0.21-8      tibble_3.2.1       \n[55] plyr_1.8.8          shiny_1.7.4         withr_2.5.0        \n[58] groupdata2_2.0.2    evaluate_0.21       survival_3.5-5     \n[61] proxy_0.4-27        RcppParallel_5.1.7  xts_0.13.1         \n[64] xml2_1.3.4          pillar_1.9.0        DT_0.28            \n[67] stats4_4.3.0        shinyjs_2.1.0       generics_0.1.3     \n[70] rstantools_2.3.1    munsell_0.5.0       scales_1.2.1       \n[73] minqa_1.2.5         gtools_3.9.4        xtable_1.8-4       \n[76] class_7.3-21        glue_1.6.2          tools_4.3.0        \n[79] shinystan_2.6.0     lme4_1.1-33         colourpicker_1.2.0 \n[82] grid_4.3.0          crosstalk_1.2.0     colorspace_2.1-0   \n[85] nlme_3.1-162        cli_3.6.1           fansi_1.0.4        \n[88] gtable_0.3.3        digest_0.6.31       htmlwidgets_1.6.2  \n[91] htmltools_0.5.5     lifecycle_1.0.3     mime_0.12          \n[94] rstanarm_2.21.4     gridtext_0.1.5      shinythemes_1.2.0  \n[97] MASS_7.3-58.4      \n\n\n\n\nThere exist multiple penguin species throughout Antarctica, including the Adelie, Chinstrap, and Gentoo. When encountering one of these penguins on an Antarctic trip, we might classify its species\n\\[Y = \\begin{cases} A & \\text{Adelie} \\\\ C & \\text{Chinstrap} \\\\ G & \\text{Gentoo} \\end{cases}\\]\n\n\n\nthree species\n\n\n\\(X_{1}\\) categorical variable: whether the penguin weighs more than the average 4200 grams\n\\[X_{1} = \\begin{cases} 1 & \\text{above-average weight} \\\\ 0 & \\text{below-average weight} \\end{cases}\\]\n\n\n\nAKA culmen length and depth\n\n\nnumerical variables:\n\\[\\begin{array}{rcl}\n  X_{2} & = & \\text{bill length (mm)} \\\\\n  X_{3} & = & \\text{flipper length (mm)} \\\\\n\\end{array}\\]\n\ndata(penguins_bayes)\npenguins <- penguins_bayes\n\nadelie_color = \"#fb7504\"\nchinstrap_color = \"#c65ccc\"\ngentoo_color = \"#067476\"\n\npenguins %>% \n  tabyl(species)\n\n   species   n   percent\n    Adelie 152 0.4418605\n Chinstrap  68 0.1976744\n    Gentoo 124 0.3604651\n\n\n\n\n\nHere, we have three categories, whereas logistic regression is limited to classifying binary response variables. As an alternative, naive Bayes classification\n\ncan classify categorical response variables \\(Y\\) with two or more categories\ndoesn’t require much theory beyond Bayes’ Rule\nit’s computationally efficient, i.e., doesn’t require MCMC simulation\n\nBut why is it called “naive”?\n\n\n\nSuppose an Antarctic researcher comes across a penguin that weighs less than 4200g with a 195mm-long flipper and 50mm-long bill. Our goal is to help this researcher identify the species of this penguin: Adelie, Chinstrap, or Gentoo\n\n\n\n\n\n\n\nimage code\n\n\npenguins |>\n  drop_na(above_average_weight) |>\n  ggplot(aes(fill = above_average_weight, x = species)) + \n  geom_bar(position = \"fill\") + \n  labs(title = \"<span style = 'color:#067476'>For which species is a<br>below-average weight most likely?</span>\",\n       subtitle = \"(focus on the <span style = 'color:#c65ccc'>below-average</span> category)\",\n       caption = \"R4DS Book Club\") +\n  scale_fill_manual(values = c(\"#c65ccc\", \"#fb7504\")) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\n\n\n\n\\[f(y|x_{1}) = \\frac{\\text{prior}\\cdot\\text{likelihood}}{\\text{normalizing constant}} = \\frac{f(y) \\cdot L(y|x_{1})}{f(x_{1})}\\] where, by the Law of Total Probability,\n\\[\\begin{array}{rcl}\nf(x_{1} & = & \\displaystyle\\sum_{\\text{all } y'} f(y')L(y'|x_{1}) \\\\\n~ & = & f(y' = A)L(y' = A|x_{1}) + f(y' = C)L(y' = C|x_{1}) + f(y' = G)L(y' = G|x_{1}) \\\\\n\\end{array}\\]\nover our three penguin species.\n\n\n\n\npenguins %>% \n  select(species, above_average_weight) %>% \n  na.omit() %>% \n  tabyl(species, above_average_weight) %>% \n  adorn_totals(c(\"row\", \"col\"))\n\n   species   0   1 Total\n    Adelie 126  25   151\n Chinstrap  61   7    68\n    Gentoo   6 117   123\n     Total 193 149   342\n\n\nPrior probabilities:\n\\[f(y = A) = \\frac{151}{342}, \\quad f(y = C) = \\frac{68}{342}, \\quad f(y = G) = \\frac{123}{342}\\]\nLikelihoods:\n\\[\\begin{array}{rcccl}\n  L(y = A | x_{1} = 0) & = & \\frac{126}{151} & \\approx & 0.8344 \\\\\n  L(y = C | x_{1} = 0) & = & \\frac{61}{68} & \\approx & 0.8971 \\\\\n  L(y = G | x_{1} = 0) & = & \\frac{6}{123} & \\approx & 0.0488 \\\\\n\\end{array}\\]\nTotal probability:\n\\[f(x_{1} = 0) = \\frac{151}{342}\\cdot\\frac{126}{151} + \\frac{68}{342}\\cdot\\frac{61}{68} + \\frac{123}{342}\\cdot\\frac{6}{123} = \\frac{193}{342}\\]\nBayes’ Rules:\n\\[\\begin{array}{rcccccl}\n  f(y = A | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = A | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{151}{342}\\cdot\\frac{126}{151}}{\\frac{193}{342}} & \\approx & 0.6528 \\\\\n  f(y = C | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = C | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{68}{342}\\cdot\\frac{61}{68}}{\\frac{193}{342}} & \\approx & 0.3161 \\\\\n  f(y = G | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = G | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{123}{342}\\cdot\\frac{6}{123}}{\\frac{193}{342}} & \\approx & 0.0311 \\\\\n\\end{array}\\]\nThe posterior probability that this penguin is an Adelie is more than double that of the other two species\n\n\n\n\nLet’s ignore the penguin’s weight for now and classify its species using only the fact that it has a 50mm-long bill\n\n\n\n\n\n\n\nimage code\n\n\npenguins|>\n  ggplot(aes(x = bill_length_mm, fill = species)) + \n  geom_density(alpha = 0.7) + \n  geom_vline(xintercept = 50, linetype = \"dashed\", linewidth = 3) + \n  labs(title = \"<span style = 'color:#c65ccc'>For which species is a<br>50mm-long bill the most common?</span>\",\n       subtitle = \"one numerical predictor\",\n       caption = \"R4DS Book Club\") +\n  scale_fill_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\n\nOur data points to our penguin being a Chinstrap\n\nwe must weigh this data against the fact that Chinstraps are the rarest of these three species\ndifficult to compute likelihood \\(L(y = A | x_{2} = 50)\\)\n\nThis is where one “naive” part of naive Bayes classification comes into play. The naive Bayes method typically assumes that any quantitative predictor, here \\(X_{2}\\), is continuous and conditionally normal:\n\\[\\begin{array}{rcl}\n  X_{2} | (Y = A) & \\sim & N(\\mu_{A}, \\sigma_{A}^{2}) \\\\\n  X_{2} | (Y = C) & \\sim & N(\\mu_{C}, \\sigma_{C}^{2}) \\\\\n  X_{2} | (Y = G) & \\sim & N(\\mu_{G}, \\sigma_{G}^{2}) \\\\\n\\end{array}\\]\n\n\n\n# Calculate sample mean and sd for each Y group\npenguins %>% \n  group_by(species) %>% \n  summarize(mean = mean(bill_length_mm, na.rm = TRUE), \n            sd = sd(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  species    mean    sd\n  <fct>     <dbl> <dbl>\n1 Adelie     38.8  2.66\n2 Chinstrap  48.8  3.34\n3 Gentoo     47.5  3.08\n\n\n\npenguins |>\n  ggplot(aes(x = bill_length_mm, color = species)) + \n  stat_function(fun = dnorm, args = list(mean = 38.8, sd = 2.66), \n                aes(color = \"Adelie\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 48.8, sd = 3.34),\n                aes(color = \"Chinstrap\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 47.5, sd = 3.08),\n                aes(color = \"Gentoo\"), linewidth = 3) +\n  ...\n\n\n\n\n\n\n\n\nimage code\n\n\npenguins |>\n  ggplot(aes(x = bill_length_mm, color = species)) + \n  stat_function(fun = dnorm, args = list(mean = 38.8, sd = 2.66), \n                aes(color = \"Adelie\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 48.8, sd = 3.34),\n                aes(color = \"Chinstrap\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 47.5, sd = 3.08),\n                aes(color = \"Gentoo\"), linewidth = 3) + \n  geom_vline(xintercept = 50, linetype = \"dashed\") + \n  labs(title = \"<span style = 'color:#c65ccc'>Prior Probabilities</span>\",\n       subtitle = \"conditionally normal\",\n       caption = \"R4DS Book Club\") +\n  scale_color_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\n\nComputing the likelihoods in R:\n\n# L(y = A | x_2 = 50) = 2.12e-05\ndnorm(50, mean = 38.8, sd = 2.66)\n\n# L(y = C | x_2 = 50) = 0.112\ndnorm(50, mean = 48.8, sd = 3.34)\n\n# L(y = G | x_2 = 50) = 0.09317\ndnorm(50, mean = 47.5, sd = 3.08)\n\nTotal probability:\n\\[f(x_{2} = 50) = \\frac{151}{342} \\cdot 0.0000212 + \\frac{68}{342} \\cdot 0.112 + \\frac{123}{342} \\cdot 0.09317 \\approx 0.05579\\]\nBayes’ Rules:\n\\[\\begin{array}{rcccccl}\n  f(y = A | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = A | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{151}{342} \\cdot 0.0000212}{0.05579} & \\approx & 0.0002 \\\\\n  f(y = C | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = C | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{68}{342} \\cdot 0.112}{0.05579} & \\approx & 0.3992 \\\\\n  f(y = G | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = G | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{123}{342} \\cdot 0.09317}{0.05579} & \\approx & 0.6006 \\\\\n\\end{array}\\]\nThough a 50mm-long bill is relatively less common among Gentoo than among Chinstrap, it follows that our naive Bayes classification, based on our prior information and penguin’s bill length alone, is that this penguin is a Gentoo – it has the highest posterior probability.\nWe’ve now made two naive Bayes classifications of our penguin’s species, one based solely on the fact that our penguin has below-average weight and the other based solely on its 50mm-long bill (in addition to our prior information). And these classifications disagree: we classified the penguin as Adelie in the former analysis and Gentoo in the latter. This discrepancy indicates that there’s room for improvement in our naive Bayes classification method.\n\n\n\n\n\n\n\n\n\n\n\nimage code\n\n\npenguins |>\nggplot(aes(x = flipper_length_mm, y = bill_length_mm, \n           color = species)) + \n  geom_point(size = 3) + \n  geom_segment(aes(x = 195, y = 30, xend = 195, yend = 50),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 170, y = 50, xend = 195, yend = 50),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  labs(title = \"<span style = 'color:#c65ccc'>Two Predictor Variables</span>\",\n       subtitle = \"50mm-long bill and 195mm-long flipper\",\n       caption = \"R4DS Book Club\") +\n  scale_color_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\n\nGeneralizing Bayes’ Rule:\n\\[f(y | x_{2}, x_{3}) = \\frac{f(y) \\cdot L(y | x_{2}, x_{3})}{\\sum_{y'} f(y') \\cdot L(y' | x_{2}, x_{3})}\\]\nAnother “naive” assumption of conditionally independent:\n\\[L(y | x_{2}, x_{3}) = f(x_{2}, x_{3} | y) = f(x_{2} | y) \\cdot f(x_{3} | y)\\]\n\nmathematically efficient\nbut what about correlation?\n\n\n# sample statistics of x_3: flipper length\npenguins %>% \n  group_by(species) %>% \n  summarize(mean = mean(flipper_length_mm, na.rm = TRUE), \n            sd = sd(flipper_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  species    mean    sd\n  <fct>     <dbl> <dbl>\n1 Adelie     190.  6.54\n2 Chinstrap  196.  7.13\n3 Gentoo     217.  6.48\n\n\nLikelihoods of a flipper length of 195 mm:\n\n# L(y = A | x_3 = 195) = 0.04554\ndnorm(195, mean = 190, sd = 6.54)\n\n# L(y = C | x_3 = 195) = 0.05541\ndnorm(195, mean = 196, sd = 7.13)\n\n# L(y = G | x_3 = 195) = 0.0001934\ndnorm(195, mean = 217, sd = 6.48)\n\nTotal probability:\n\\[f(x_{2} = 50, x_{3} = 195) = \\frac{151}{342} \\cdot 0.0000212 \\cdot 0.04554 + \\frac{68}{342} \\cdot 0.112 \\cdot 0.05541 + \\frac{123}{342} \\cdot 0.09317 \\cdot 0.0001931 \\approx 0.001241\\]\nBayes’ Rules:\n\\[\\begin{array}{rcccl}\n  f(y = A | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{151}{342} \\cdot 0.0000212 \\cdot 0.04554}{0.0001931} & \\approx & 0.0003 \\\\\n  f(y = C | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{68}{342} \\cdot 0.112 \\cdot 0.05541}{0.0001931} & \\approx & 0.9944 \\\\\n  f(y = G | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{123}{342} \\cdot 0.09317 \\cdot 0.0001931}{0.0001931} & \\approx & 0.0052 \\\\\n\\end{array}\\]\nIn conclusion, our penguin is almost certainly a Chinstrap.\n\n\n\nTo implement naive Bayes classification in R, we’ll use the naiveBayes() function in the e1071 package (Meyer et al. 2021)\n\n\n\n# two models\nnaive_model_1 <- naiveBayes(species ~ bill_length_mm, data = penguins)\nnaive_model_2 <- naiveBayes(species ~ bill_length_mm + flipper_length_mm, \n                            data = penguins)\n\n# our penguin to classify\nour_penguin <- data.frame(bill_length_mm = 50, flipper_length_mm = 195)\n\n\n\n\n\npredict(naive_model_1, newdata = our_penguin, type = \"raw\") |>\nround(6)\n\n       Adelie Chinstrap Gentoo\n[1,] 0.000169  0.397831  0.602\n\n\n\npredict(naive_model_1, newdata = our_penguin)\n\n[1] Gentoo\nLevels: Adelie Chinstrap Gentoo\n\n\n\npredict(naive_model_2, newdata = our_penguin, type = \"raw\") |>\nround(6)\n\n       Adelie Chinstrap   Gentoo\n[1,] 0.000345  0.994868 0.004787\n\n\n\npredict(naive_model_2, newdata = our_penguin)\n\n[1] Chinstrap\nLevels: Adelie Chinstrap Gentoo\n\n\n\n\n\n\n\n\n\npenguins <- penguins %>% \n  mutate(class_1 = predict(naive_model_1, newdata = .),\n         class_2 = predict(naive_model_2, newdata = .))\n\n\nset.seed(84735)\npenguins %>% \n  sample_n(4) %>% \n  select(bill_length_mm, flipper_length_mm, species, class_1, class_2) %>% \n  rename(bill = bill_length_mm, flipper = flipper_length_mm)\n\n# A tibble: 4 × 5\n   bill flipper species   class_1 class_2  \n  <dbl>   <int> <fct>     <fct>   <fct>    \n1  47.5     199 Chinstrap Gentoo  Chinstrap\n2  40.9     214 Gentoo    Adelie  Gentoo   \n3  41.3     194 Adelie    Adelie  Adelie   \n4  38.5     190 Adelie    Adelie  Adelie   \n\n\n\n# Confusion matrix for naive_model_1\npenguins %>% \n  tabyl(species, class_1) %>% \n  adorn_percentages(\"row\") %>% \n  adorn_pct_formatting(digits = 2) %>%\n  adorn_ns()\n\n   species       Adelie Chinstrap       Gentoo\n    Adelie 95.39% (145) 0.00% (0)  4.61%   (7)\n Chinstrap  5.88%   (4) 8.82% (6) 85.29%  (58)\n    Gentoo  6.45%   (8) 4.84% (6) 88.71% (110)\n\n\n\naccuracy: 76 percent\n85 percent of Chinstap penguins are misclassified as Gentoo!\n\n\n# Confusion matrix for naive_model_2\npenguins %>% \n  tabyl(species, class_2) %>% \n  adorn_percentages(\"row\") %>% \n  adorn_pct_formatting(digits = 2) %>%\n  adorn_ns()\n\n   species       Adelie   Chinstrap       Gentoo\n    Adelie 96.05% (146)  2.63%  (4)  1.32%   (2)\n Chinstrap  7.35%   (5) 86.76% (59)  5.88%   (4)\n    Gentoo  0.81%   (1)  0.81%  (1) 98.39% (122)\n\n\n\naccuracy: 95 percent\n\n\n\n\n\n# 10-fold cross-validation\nset.seed(84735)\ncv_model_2 <- naive_classification_summary_cv(\n  model = naive_model_2, data = penguins, y = \"species\", k = 10)\n\n\ncv_model_2$cv\n\n   species       Adelie   Chinstrap       Gentoo\n    Adelie 96.05% (146)  2.63%  (4)  1.32%   (2)\n Chinstrap  7.35%   (5) 86.76% (59)  5.88%   (4)\n    Gentoo  0.81%   (1)  0.81%  (1) 98.39% (122)\n\n\n\n\n\n\nNaive Bayes\n\\[f(y | x_{1}, x_{2}, ..., x_{p}) = \\frac{f(y) \\cdot L(y | x_{1}, x_{2}, ..., x_{p})}{\\sum_{y'} f(y') \\cdot L(y' | x_{1}, x_{2}, ..., x_{p})}\\]\n\nconditionally independent \\(\\rightarrow\\) computationally efficient\ngeneralizes to more than two categories\nassumptions violated commonly in practice\n\nLogistic Regression\n\\[\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{p}\\]\n\nbinary classification\ncoefficients \\(\\rightarrow\\) illumination of the relationships among these variables\n\n\n\n\n\n\n\n\n\n\nMeeting chat log\n\n00:44:10    defuneste:  plant@net\n00:44:28    Lisa:   https://identify.plantnet.org/\n00:48:49    Brendan Lam:    Thanks everyone!\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeeting chat log\n\nLOG"
  },
  {
    "objectID": "posts/fonts/fonts.html",
    "href": "posts/fonts/fonts.html",
    "title": "Fonts",
    "section": "",
    "text": "Even though I am not planning on making lecture slides in the upcoming semester, I have been meaning to learn how to find and use fonts in my visuals.\nI have always loved the Cowboy Bebop and felt the zen of the title cards at the end of each episode, such as\n\nI happened upon a Reddit post where someone mentioned that the font is (or is very similar to) Cheltenham Condensed Bold, and I’m sure that the intended font has italics too. Thus, I grabbed the font over at FontGeek, uploaded the .tff file into Canva, and made the following\n\nThis Latin phrase is from the series finale of another late 1990s TV show called Sports Night"
  },
  {
    "objectID": "posts/quarto/quarto_resources.html",
    "href": "posts/quarto/quarto_resources.html",
    "title": "Quarto Resources",
    "section": "",
    "text": "making myself a space to save/archive resources for learning/using Quarto\n\nQuarto documentation\n\n\nYouTube\n\n20230522 Quarto for Academics Mine Cetinkaya-Rundel\n20230512 Getting Started with Quarto Mine Cetinkaya-Rundel\n20230511 Publishing Jupyter Notebooks with Quarto JJ Allaire"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html",
    "href": "posts/exploreUDL/workshop_script.html",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "",
    "text": "If you are planning on attending this workshop and participate live, I advise you to read only the pre-reading in advance.\n\nlibrary(\"tidyverse\")\nlibrary(\"visualize\") #functions to create graphs of probability distributions\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\nIn this workshop, we will not only discuss UDL (universal design for learning) principles, but rather experience classroom exercises where the participants (students) are immersed in various ways to mathematically explore concepts and calculations and are then able express their own preferences in UDL modalities. These exercises are adapted from ideas from Choosing to See: A Framework for Equity in the Math Classroom by Pamela Seda and Kyndall Brown.\n\n\n\nAudience members would perhaps like to read the following passages from Choosing to See by Pamela Seda and Kyndall Brown, and mentally give themselves time to ponder the ideas.\n“The culturally relevant teacher, however, does not view knowledge as static, but rather as continuously re-created and shared by both teacher and student. In this approach, teachers should look for opportunities where they can play the dual roles of student and co-teacher. One way of making a class less teacher-centered is by striving to never say anything kids can say for themselves.” — page 29\n“Because mathematics tends to be perceived as difficult, these negative stereotypes contribute to the social status, expert status, academic status, and peer status attributed to students. Further contributing to unequal status interactions is the idea that mathematics is static, genetic, and/or innate. Members of negatively stereotyped groups will more likely be perceived as incompetent or incapable when they make mistakes in class” — page 56\n“Culturally relevant relationships are, by contrast, fluid, humane, and equitable. In culturally relevant classrooms, teachers and students co-construct academic and behavioral norms and hold each other accountable for maintaining those norms. Culturally relevant teachers encourage connections with all students, emphasize collaboration, and establish communities of learners. Students work on real-world activities and projects that foster interdependence and allow students to use their community-based knowledge to solve problems” — page 71\n\n\n\nThis workshop starts as a look at mental health data. In the first exercise, we use the survey data as the prior probability for calculations with the binomial distribution. In the second exercise, we expand our calculations to approximations with the normal distributions. Each exercise has participants split into groups for the jigsaw approach of active learning. The groups are given various views—verbal, algebraic, symbolic, visual—and present their approach to solving the calculations to their peers. Through the pair of exercises, each participant is immersed in two of the modalities and furthermore hears from their peers about the other approaches. Toward the end of the workshop, each participant ranks their modality preferences, and will possibility feel more optimistic and enthusiastic about solving mathematical tasks in their preferred modality. These exercises are adapted from ideas from Choosing to See: A Framework for Equity in the Math Classroom by Pamela Seda and Kyndall Brown.\n\n\n\n\nPrompt 1 (breakout rooms)\nPrompt 1 (return to main room)\nPrompt 2 (breakout rooms)\nPrompt 2 (return to main room)\ndiscuss examples\ndiscuss culturally relevant teaching\n\n\n\n\n\nEach Jamboard has multiple pages\nSharing screen speeds up discussions\nChoose a spokesperson for your group\nAn answer is nice, but we will like to explore how to find answers"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#workshop-description",
    "href": "posts/exploreUDL/workshop_script.html#workshop-description",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "",
    "text": "In this workshop, we will not only discuss UDL (universal design for learning) principles, but rather experience classroom exercises where the participants (students) are immersed in various ways to mathematically explore concepts and calculations and are then able express their own preferences in UDL modalities. These exercises are adapted from ideas from Choosing to See: A Framework for Equity in the Math Classroom by Pamela Seda and Kyndall Brown."
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#pre-reading",
    "href": "posts/exploreUDL/workshop_script.html#pre-reading",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "",
    "text": "Audience members would perhaps like to read the following passages from Choosing to See by Pamela Seda and Kyndall Brown, and mentally give themselves time to ponder the ideas.\n“The culturally relevant teacher, however, does not view knowledge as static, but rather as continuously re-created and shared by both teacher and student. In this approach, teachers should look for opportunities where they can play the dual roles of student and co-teacher. One way of making a class less teacher-centered is by striving to never say anything kids can say for themselves.” — page 29\n“Because mathematics tends to be perceived as difficult, these negative stereotypes contribute to the social status, expert status, academic status, and peer status attributed to students. Further contributing to unequal status interactions is the idea that mathematics is static, genetic, and/or innate. Members of negatively stereotyped groups will more likely be perceived as incompetent or incapable when they make mistakes in class” — page 56\n“Culturally relevant relationships are, by contrast, fluid, humane, and equitable. In culturally relevant classrooms, teachers and students co-construct academic and behavioral norms and hold each other accountable for maintaining those norms. Culturally relevant teachers encourage connections with all students, emphasize collaboration, and establish communities of learners. Students work on real-world activities and projects that foster interdependence and allow students to use their community-based knowledge to solve problems” — page 71"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#teacher-notes",
    "href": "posts/exploreUDL/workshop_script.html#teacher-notes",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "",
    "text": "This workshop starts as a look at mental health data. In the first exercise, we use the survey data as the prior probability for calculations with the binomial distribution. In the second exercise, we expand our calculations to approximations with the normal distributions. Each exercise has participants split into groups for the jigsaw approach of active learning. The groups are given various views—verbal, algebraic, symbolic, visual—and present their approach to solving the calculations to their peers. Through the pair of exercises, each participant is immersed in two of the modalities and furthermore hears from their peers about the other approaches. Toward the end of the workshop, each participant ranks their modality preferences, and will possibility feel more optimistic and enthusiastic about solving mathematical tasks in their preferred modality. These exercises are adapted from ideas from Choosing to See: A Framework for Equity in the Math Classroom by Pamela Seda and Kyndall Brown."
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#agenda",
    "href": "posts/exploreUDL/workshop_script.html#agenda",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "",
    "text": "Prompt 1 (breakout rooms)\nPrompt 1 (return to main room)\nPrompt 2 (breakout rooms)\nPrompt 2 (return to main room)\ndiscuss examples\ndiscuss culturally relevant teaching"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#advice",
    "href": "posts/exploreUDL/workshop_script.html#advice",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "",
    "text": "Each Jamboard has multiple pages\nSharing screen speeds up discussions\nChoose a spokesperson for your group\nAn answer is nice, but we will like to explore how to find answers"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#prompt-1",
    "href": "posts/exploreUDL/workshop_script.html#prompt-1",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Prompt 1",
    "text": "Prompt 1\n\nWhat is the probability that at most two young adults did not respond as often lonely?\nWhat is the probability that at least one young adult responded as often lonely?"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#group-1",
    "href": "posts/exploreUDL/workshop_script.html#group-1",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Group 1",
    "text": "Group 1\n\nThe probability that exactly zero young adults responded as often lonely is 0.0845\nThe probability that exactly one young adult responded as often lonely is 0.2700\n`The probability that exactly two young adults responded as often lonely is0.3452\nThe probability that exactly three young adults responded as often lonely is 0.2207\nThe probability that exactly four young adults responded as often lonely is 0.0706\nThe probability that exactly five young adults responded as often lonely is 0.0090\nThe probability that exactly zero young adults did not respond as often lonely is 0.0090\nThe probability that exactly one young adult did not respond as often lonely is 0.0706\nThe probability that exactly two young adults did not respond as often lonely is 0.2207\nThe probability that exactly three young adults did not respond as often lonely is 0.3452\nThe probability that exactly four young adults did not respond as often lonely is 0.2700\nThe probability that exactly five young adults did not respond as often lonely is 0.0845"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#group-2",
    "href": "posts/exploreUDL/workshop_script.html#group-2",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Group 2",
    "text": "Group 2\n\n\\(\\binom{5}{0}(0.39)^{0}(0.61)^{5} = 0.0845\\)\n\\(\\binom{5}{1}(0.39)^{1}(0.61)^{4} = 0.2700\\)\n\\(\\binom{5}{2}(0.39)^{2}(0.61)^{3} = 0.3452\\)\n\\(\\binom{5}{3}(0.39)^{3}(0.61)^{2} = 0.2207\\)\n\\(\\binom{5}{4}(0.39)^{4}(0.61)^{1} = 0.0706\\)\n\\(\\binom{5}{5}(0.39)^{5}(0.61)^{0} = 0.0090\\)\n\\(\\binom{5}{0}(0.61)^{0}(0.39)^{5} = 0.0090\\)\n\\(\\binom{5}{1}(0.61)^{1}(0.39)^{4} = 0.0706\\)\n\\(\\binom{5}{2}(0.61)^{2}(0.39)^{3} = 0.2207\\)\n\\(\\binom{5}{3}(0.61)^{3}(0.39)^{2} = 0.3452\\)\n\\(\\binom{5}{4}(0.61)^{4}(0.39)^{1} = 0.2700\\)\n\\(\\binom{5}{5}(0.61)^{5}(0.39)^{0} = 0.0845\\)"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#group-3",
    "href": "posts/exploreUDL/workshop_script.html#group-3",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Group 3",
    "text": "Group 3\n\ndbinom(0,5,0.39) = 0.0845\ndbinom(1,5,0.39) = 0.2700\ndbinom(2,5,0.39) = 0.3452\ndbinom(3,5,0.39) = 0.2207\ndbinom(4,5,0.39) = 0.0706\ndbinom(5,5,0.39) = 0.0090\ndbinom(0,5,0.61) = 0.0090\ndbinom(1,5,0.61) = 0.0706\ndbinom(2,5,0.61) = 0.2207\ndbinom(3,5,0.61) = 0.3452\ndbinom(4,5,0.61) = 0.2700\ndbinom(5,5,0.61) = 0.0845"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#group-4",
    "href": "posts/exploreUDL/workshop_script.html#group-4",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Group 4",
    "text": "Group 4\n\nvisualize.binom(section = \"bounded\", stat = c(0,0), \n                size = 5, prob = 0.39,\n                strict  = c(FALSE, FALSE))\n\n\n\n\n\nvisualize.binom(section = \"bounded\", stat = c(1,1), \n                size = 5, prob = 0.39,\n                strict  = c(FALSE, FALSE))\n\n\n\n\n\nvisualize.binom(section = \"bounded\", stat = c(2,2), \n                size = 5, prob = 0.39,\n                strict  = c(FALSE, FALSE))\n\n\n\n\n\nvisualize.binom(section = \"bounded\", stat = c(3,3), \n                size = 5, prob = 0.39,\n                strict  = c(FALSE, FALSE))\n\n\n\n\n\nvisualize.binom(section = \"bounded\", stat = c(4,4), \n                size = 5, prob = 0.39,\n                strict  = c(FALSE, FALSE))\n\n\n\n\n\nvisualize.binom(section = \"bounded\", stat = c(5,5), \n                size = 5, prob = 0.39,\n                strict  = c(FALSE, FALSE))"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#rule-of-four",
    "href": "posts/exploreUDL/workshop_script.html#rule-of-four",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Rule of Four",
    "text": "Rule of Four\n\n\n\nimage credit: Choosing to See"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#rule-of-four-1",
    "href": "posts/exploreUDL/workshop_script.html#rule-of-four-1",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Rule of Four",
    "text": "Rule of Four\n\n\n\nimage credit: Choosing to See"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#prompt-2",
    "href": "posts/exploreUDL/workshop_script.html#prompt-2",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Prompt 2",
    "text": "Prompt 2\n\n\nSuppose that a patient has a blood test performed, and their results included an MCV reading of 97 fl/cell. Using the information presented to your group as a guide, should the doctor be alarmed?\n\n\n\n\nred blood cells"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#group-4-1",
    "href": "posts/exploreUDL/workshop_script.html#group-4-1",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Group 4",
    "text": "Group 4\nYou can refer to this website: https://emedicine.medscape.com/article/2085770-overview"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#group-3-1",
    "href": "posts/exploreUDL/workshop_script.html#group-3-1",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Group 3",
    "text": "Group 3\n\\[\\displaystyle\\frac{1}{5\\sqrt{2\\pi}} \\int_{80}^{100} \\! e^{-0.5(\\frac{x - 90}{5})^{2}} \\, dx \\approx 0.9545\\]"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#group-2-1",
    "href": "posts/exploreUDL/workshop_script.html#group-2-1",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Group 2",
    "text": "Group 2\n\npnorm(75,90,5) = 0.0013\npnorm(80,90,5) = 0.0228\npnorm(85,90,5) = 0.1587\npnorm(90,90,5) = 0.5000\npnorm(95,90,5) = 0.8413\npnorm(100,90,5) = 0.9772\npnorm(105,90,5) = 0.9987"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#group-1-1",
    "href": "posts/exploreUDL/workshop_script.html#group-1-1",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Group 1",
    "text": "Group 1\n\nvisualize.norm(stat = c(80, 100), mu = 90, sd = 5, section = \"bounded\")"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#prompt-1-1",
    "href": "posts/exploreUDL/workshop_script.html#prompt-1-1",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Prompt 1",
    "text": "Prompt 1\n\nWhat is the probability that at most two young adults did not respond as often lonely?\n\n\npbinom(2, 5, 0.61)\n\n[1] 0.3003084\n\n\n\nWhat is the probability that at least one young adult responded as often lonely?\n\n\n1 - pbinom(0, 5, 0.39)\n\n[1] 0.9155404"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#prompt-2-1",
    "href": "posts/exploreUDL/workshop_script.html#prompt-2-1",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Prompt 2",
    "text": "Prompt 2\nSuppose that a patient has a blood test performed, and their results included an MCV reading of 97 fl/cell. Using the information presented to your group as a guide, should the doctor be alarmed?\nI intended the second set of exercises to have more open-ended discussion (i.e. no “right or wrong” answer)."
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#reference-range",
    "href": "posts/exploreUDL/workshop_script.html#reference-range",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Reference Range",
    "text": "Reference Range\n\nMedScape says that the reference range for MCV is from 80 to 96 fL/cell. Find the probability with visualize.norm that a randomly selected blood test will fall within the reference range. This is also known as normocytic size for MCV.\n\n\nvisualize.norm(stat = c(80,96), mu = 90, sd = 5, section = \"bounded\")"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#macrocytic-anemia",
    "href": "posts/exploreUDL/workshop_script.html#macrocytic-anemia",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Macrocytic Anemia",
    "text": "Macrocytic Anemia\n\nMacrocytic or pernicious anemia describes high levels of MCV, and that may be caused by a nutrient deficiency (for instance, deficiency of vitamin B12). Find the probability with visualize.norm that a randomly selected blood test will report an MCV value above 96 fL/cell.\n\n\nvisualize.norm(stat = 96, mu = 90, sd = 5, section = \"upper\")"
  },
  {
    "objectID": "posts/exploreUDL/workshop_script.html#microcytic-anemia",
    "href": "posts/exploreUDL/workshop_script.html#microcytic-anemia",
    "title": "Hands-On Exploration of UDL Modalities",
    "section": "Microcytic Anemia",
    "text": "Microcytic Anemia\n\nMicrocytic anemia describes low levels of MCV and could be caused by diseases such as thalassemia. If microcytic anemia is diagnosed at MCV levels below 80 fL/cell, find the probability with visualize.norm that a randomly selected blood test will suggest microcytic anemia.\n\n\nvisualize.norm(stat = 80, mu = 90, sd = 5, section = \"lower\")"
  },
  {
    "objectID": "folder_view.html",
    "href": "folder_view.html",
    "title": "Folder Tree",
    "section": "",
    "text": "I saw some tweet where R programmers were asking about how to visualize folders and subfolders among their work files, so I thought I would try it out too. One of the answers was these Stack Overflow threads:\n\nhttps://stackoverflow.com/questions/67357402/how-to-i-see-visual-of-my-folder-structure-in-rstudio\nhttps://stackoverflow.com/questions/36094183/how-to-build-a-dendrogram-from-a-directory-tree\n\n\nlibrary(\"data.tree\")\nlibrary(\"dplyr\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nFirst, I set the working directory to where I have most of my GitHub-linked files. Next, we collect the paths (along with subdirectories)\n\npaths = unique(c(list.dirs(full.names = T),list.files(full.names = T)))\n\nIt appears that the information is collected into a data frame.\n\nx &lt;- lapply(strsplit(paths, \"/\"), function(z) as.data.frame(t(z)))\nx &lt;- plyr::rbind.fill(x) #plyr??\n\nTo restore individual file names, we collapse the information with the slashes.\n\nx$pathString &lt;- apply(x, 1, function(x) paste(trimws(na.omit(x)), collapse=\"/\"))\n\nNow, we can build the tree.\n\nmytree &lt;- data.tree::as.Node(x)\n\nDo we dare print the tree?\n\nprint(mytree)\n\n                                        levelName\n1   .                                            \n2    ¦--.git                                     \n3    ¦   ¦--hooks                                \n4    ¦   ¦--info                                 \n5    ¦   ¦--logs                                 \n6    ¦   ¦   °--refs                             \n7    ¦   ¦       ¦--heads                        \n8    ¦   ¦       °--remotes                      \n9    ¦   ¦           °--origin                   \n10   ¦   ¦--objects                              \n11   ¦   ¦   ¦--info                             \n12   ¦   ¦   °--pack                             \n13   ¦   °--refs                                 \n14   ¦       ¦--heads                            \n15   ¦       ¦--remotes                          \n16   ¦       ¦   °--origin                       \n17   ¦       °--tags                             \n18   ¦--.quarto                                  \n19   ¦   ¦--_freeze                              \n20   ¦   ¦   ¦--posts                            \n21   ¦   ¦   ¦   °--exploreUDL                   \n22   ¦   ¦   ¦       °--workshop_script          \n23   ¦   ¦   ¦           ¦--execute-results      \n24   ¦   ¦   ¦           °--figure-html          \n25   ¦   ¦   °--site_libs                        \n26   ¦   ¦       ¦--clipboard                    \n27   ¦   ¦       °--quarto-listing               \n28   ¦   ¦--idx                                  \n29   ¦   ¦   °--posts                            \n30   ¦   ¦       ¦--bump                         \n31   ¦   ¦       ¦--California_weather           \n32   ¦   ¦       ¦--Canvas_Roster                \n33   ¦   ¦       ¦--Christmas_plots              \n34   ¦   ¦       ¦--cofficient_of_variation      \n35   ¦   ¦       ¦--curly_operator               \n36   ¦   ¦       ¦--Dartmouth_Atlas_data         \n37   ¦   ¦       ¦--exploreUDL                   \n38   ¦   ¦       ¦--fonts                        \n39   ¦   ¦       ¦--graphviz                     \n40   ¦   ¦       ¦--greenhouse                   \n41   ¦   ¦       ¦--inequalities                 \n42   ¦   ¦       ¦--intentional_walks            \n43   ¦   ¦       ¦--JupyterHub_showcase          \n44   ¦   ¦       ¦--oblicubes                    \n45   ¦   ¦       ¦--overviewR                    \n46   ¦   ¦       ¦--patchwork                    \n47   ¦   ¦       ¦--quarto                       \n48   ¦   ¦       ¦--Settlement_Survival          \n49   ¦   ¦       ¦--sex_bio_context              \n50   ¦   ¦       ¦--TidyModels_trees             \n51   ¦   ¦       ¦--typewriter                   \n52   ¦   ¦       ¦--welcome                      \n53   ¦   ¦       ¦--wonderwall                   \n54   ¦   ¦       °--xDBER                        \n55   ¦   ¦--listing                              \n56   ¦   ¦--preview                              \n57   ¦   °--xref                                 \n58   ¦--_freeze                                  \n59   ¦   ¦--posts                                \n60   ¦   ¦   ¦--bayesrules                       \n61   ¦   ¦   ¦   °--14_Naive_Bayes_Classification\n62   ¦   ¦   ¦       ¦--execute-results          \n63   ¦   ¦   ¦       °--figure-html              \n64   ¦   ¦   ¦--bump                             \n65   ¦   ¦   ¦   °--bump                         \n66   ¦   ¦   ¦       ¦--execute-results          \n67   ¦   ¦   ¦       °--figure-html              \n68   ¦   ¦   ¦--California_weather               \n69   ¦   ¦   ¦   °--California_weather           \n70   ¦   ¦   ¦       ¦--execute-results          \n71   ¦   ¦   ¦       °--figure-html              \n72   ¦   ¦   ¦--Canvas_Roster                    \n73   ¦   ¦   ¦   °--Canvas_Roster                \n74   ¦   ¦   ¦       °--execute-results          \n75   ¦   ¦   ¦--Christmas_plots                  \n76   ¦   ¦   ¦   °--Christmas_plots              \n77   ¦   ¦   ¦       ¦--execute-results          \n78   ¦   ¦   ¦       °--figure-html              \n79   ¦   ¦   ¦--coefficient_of_variation         \n80   ¦   ¦   ¦   °--execute-results              \n81   ¦   ¦   ¦--cofficient_of_variation          \n82   ¦   ¦   ¦   °--coefficient_of_variation     \n83   ¦   ¦   ¦       ¦--execute-results          \n84   ¦   ¦   ¦       °--figure-html              \n85   ¦   ¦   ¦--curly_operator                   \n86   ¦   ¦   ¦   °--curly_operator               \n87   ¦   ¦   ¦       °--execute-results          \n88   ¦   ¦   ¦--Dartmouth_Atlas_data             \n89   ¦   ¦   ¦   °--Dartmouth_Atlas_data         \n90   ¦   ¦   ¦       ¦--execute-results          \n91   ¦   ¦   ¦       °--figure-html              \n92   ¦   ¦   ¦--exploreUDL                       \n93   ¦   ¦   ¦   °--workshop_script              \n94   ¦   ¦   ¦       ¦--execute-results          \n95   ¦   ¦   ¦       °--figure-html              \n96   ¦   ¦   ¦--greenhouse                       \n97   ¦   ¦   ¦   °--greenhouse                   \n98   ¦   ¦   ¦       ¦--execute-results          \n99   ¦   ¦   ¦       °--figure-html              \n100  ¦   ¦   °--... 15 nodes w/ 65 sub           \n101  ¦   °--... 1 nodes w/ 107 sub               \n102  °--... 12 nodes w/ 268 sub"
  },
  {
    "objectID": "posts/sex_bio_context/sex_in_a_biological_context.html#biographi-case-study",
    "href": "posts/sex_bio_context/sex_in_a_biological_context.html#biographi-case-study",
    "title": "Sex in a Biological Context",
    "section": "",
    "text": "Goal\n\nEncourage students to use established research materials to explore sex in a biological context\n\nObjectives\n\nComment on an interview with Dr Joan Roughgarden\nSummarize a passage from a book by Dr Joan Roughgarden\nInterpret a couple pieces of data visualizaton from sex studies\n\nContext\n\ndata science course\nmostly biology majors, mostly sophomores\nhomework assignment"
  },
  {
    "objectID": "posts/sex_bio_context/sex_in_a_biological_context.html#interview",
    "href": "posts/sex_bio_context/sex_in_a_biological_context.html#interview",
    "title": "Sex in a Biological Context",
    "section": "",
    "text": "TaskWord CloudSample Responses\n\n\n\n\nDr Joan Roughgarden was one of the most important researchers in the field of sexual selection theory, and she has written several works on the matters including the book Evolution’s Rainbow. Dr Roughgarden spoke at a WiSE conference (Women in Science and Engineering) a couple of years ago. For this resource , you may focus on the middle of the video from the “Evolution’s Rainbow” segment (at the 9:25 mark) to the “Career track” segment (at the 15:50 mark).\n\nhttps://www.youtube.com/watch?v=hDbsQu0XhPo\nsource: https://www.wisecology.net/speakers/joan-roughgarden/\n\n\n\n\n\nDr Joan Roughgarden\n\n\n\n\n\n\n\n\n\nresponses to the video interview\n\n\n\n\n\n“There is a lot of diversity in sex and gender in not only humans, but animals as well. With a variety of sex and gender in living beings that is identified by Dr. Roughgarden, it may mean that the sexual selection theory is outdated as we expand our knowledge on this topic. The theory needs to be rewritten for this modern era.”\n\n“This excerpt from Dr. Joan Roughgarden discusses her work on her book Evolution’s Rainbow and her personal experience as a (trans) woman in science. She discusses her work on gender and sexuality across all animal kingdoms, argues that sexual selection should be scrapped, and thinks it will collapse altogether. As a woman in science, she has seen that men’s careers in science are linear and are privileged with a sense of authority.”\n“The career track segment of Dr. Joan Roughgarden’s speech at the Women in Science and Engineering conference focused on her experiences as a transgender woman in the field of biology. She shared the challenges she faced while navigating her gender identity in a male-dominated field and the discrimination and bias she encountered. However, she also emphasized the importance of being true to oneself and finding a supportive community. Roughgarden’s message was one of resilience and perseverance in the face of adversity.”"
  },
  {
    "objectID": "posts/sex_bio_context/sex_in_a_biological_context.html#book",
    "href": "posts/sex_bio_context/sex_in_a_biological_context.html#book",
    "title": "Sex in a Biological Context",
    "section": "",
    "text": "TaskWord CloudSample Responses\n\n\n\n\nSkim through chapter 2 “Sex versus Gender” of Evolution’s Rainbow. What is your impression of the writing? Or, what information did you get from this chapter?\n\n\n\n\nEvolution’s Rainbow\n\n\n\n\n\n\n\n\n\nresponses to the book excerpt\n\n\n\n\n\n“While reading the small passage, from Evolution’s Rainbow Chapter 2, it states that male and female are just biological categories. It starts by saying that people use the words, gender and sex wrong stating that the word’s mean male and female. In which the word’s male and female does not relate with have two different meanings biological criteria and in the social criteria.”\n“Looking through the text and readings we can see that Dr. Roughgarden discusses in terms about”male” and “female” instead of “man” and “woman”. As she done so she also discusses about when it comes to humans male and female don’t coincide 100 percent. She also discusses about the social categories rests in society and not in science.”\n\n“This excerpt approaches males and females from the sex, gender, and social perspective. It states that male and female as a \"Sex\" does not make sense since sex refers to the mixing of genes or reproduction, but some reproduction is asexual reproduction without two partners. Gender’s definition is embracing the biological definition but the other attributes of gender –masculinity and femininity– are not defined biologically. The author is writing up and tearing down these words and stripping them of meaning to reveal their inconsistencies.”"
  },
  {
    "objectID": "posts/sex_bio_context/sex_in_a_biological_context.html#and-4-data-visualizations",
    "href": "posts/sex_bio_context/sex_in_a_biological_context.html#and-4-data-visualizations",
    "title": "Sex in a Biological Context",
    "section": "",
    "text": "Task (3)Word Cloud (3)Sample Responses (3)Task (4)Word Cloud (4)Sample Responses (4)\n\n\n\n\nWhat is your impression of the data visualization below? What information can you gather from the figure(s)?\n\nManey DL, Merritt JR, Prichard MR, Horton BM, Yi SV. Inside the supergene of the bird with four sexes. Horm Behav. 2020 Nov;126:104850. doi: 10.1016/j.yhbeh.2020.104850. Epub 2020 Sep 19. PMID: 32937166; PMCID: PMC7725849.\n\n\n\n\nData Viz 1\n\n\n\n\n\n\n\n\n\nresponses to the first graph\n\n\n\n\n\n“One morph performs a few of the aforementioned actions more frequently than the other. For instance, both sexes of WS birds sing more frequently in response to simulated territorial invasions than TS birds do. TS females seldom ever sing, despite the fact that TS males frequently sing loudly. While TS males are more likely to stay in their own territories, WS males are more prone to participate in territorial invasions. TS birds provide nestlings more frequently than their WS counterparts, and males reproduce this tendency more frequently than females. In general, WS birds appear to devote more time to mate-hunting and intrasexual rivalry, whereas TS birds adopt a more parental life-history approach.”\n\n“that data infomration that has been provided above has allowed me to follow what is being studied fairly easy since they would use the the color of the birds that they have to be distinguished and is being used in the boxplots in which helps us know which one is for which. with that in mind the boxplot allows us to understand the ranges that they have within the species while also allowing us to see how long the songs that each of the birds would sing within the ranges of ten minutes which shows us how different they are. while within the other boxplot then they would show the trips per hour in which helps us see how different they are compared the sexes from both of the species since the females TS have the highest trips per hour compared to the others. another thing they would use is the means, medians, and the quartiles so that they could be compared between the species and their sexes to know the differences.”\n“I can see that the tan-striped bird makes more trips but they have shorter songs sung. Compared to the white bird he made fewer trips but sang for longer times than the tan-striped bird. The used a box graph to show the difference.”\n\n\n\n\n\nWhat is your impression of the data visualization below? What information can you gather from the figure(s)?\n\nManey DL, Merritt JR, Prichard MR, Horton BM, Yi SV. Inside the supergene of the bird with four sexes. Horm Behav. 2020 Nov;126:104850. doi: 10.1016/j.yhbeh.2020.104850. Epub 2020 Sep 19. PMID: 32937166; PMCID: PMC7725849.\n\n\n\n\nData Viz 2\n\n\n\n\n\n\n\n\n\nresponses to the second graph\n\n\n\n\n\n“Prolactin has been associated with provisioning behaviors in both male and female songbirds. Females had a higher normalized VIP expression. Song rate is much higher in males.”\n\n“My impression of the data visualization is that is clear to follow, the colors are coordinated and it has just the necessary information. From the figures it can be gathered that WS have a higher VIP expression compared to that of the TS, also that TS females have a shorter range than all the other birds. Then from figures C and D we can observe whether there is correlation between the song rate and the VIP expression. For males there is no correlation but for females there is a slight positive correlation.”\n“The males of different markings have more differences to each other than the male and female of similar markings when looking at songs produced. Adding hormones into the mix helped condense the songs between birds of similar markings, making the plot a lot less scattered.”"
  },
  {
    "objectID": "posts/sex_bio_context/sex_in_a_biological_context.html#beyond",
    "href": "posts/sex_bio_context/sex_in_a_biological_context.html#beyond",
    "title": "Sex in a Biological Context",
    "section": "",
    "text": "TaskSample Responses\n\n\n\n\nIf you wanted to discuss the topic of sex (and maybe gender) in a biological context, what other information would you seek out?\n\n\n\n\nresponses to the last prompt\n\n\n\n\n\n\n\n“If I would want to broaden my understanding on this topic, which is something that I will definitely be looking for, is probably toward podcasts and other articles can help me better understand the difference between sex and gender from there experiences.”\n“What are the biological factors of sex? Does mental health come into play?”\n“I would seek out more information on the genetic compositions of organisms and specifically compare genetic maps of female and male organisms. I would also look at asexual organisms and study how reproduction happens with an organism that is neither female nor male or an organism that is both. We could also study the chemicals released on organisms based on gender like how testosterone is released in male organisms and estrogen is released in female organisms.”"
  },
  {
    "objectID": "posts/nbgitpuller/nbgitpuller.html",
    "href": "posts/nbgitpuller/nbgitpuller.html",
    "title": "Using nbgitpuller",
    "section": "",
    "text": "Installation\nEach admin user (“instructor”) can verify the presence of nbgitpuller on their hub presence\n\nopen a terminal session\nrun pip install nbgitpuller\n\n\n\nFile Setup\nThe purpose of nbgitpuller is to ease access to files in a remote GitHub repository.\n\ncreate a GitHub repository\nplace a .ipynb notebook file\nTO DO: test this on a private repository\n\n\n\nCreate Link\nGo to this website (or later use a browser extension)\n\nURL for hub (e.g. https://ucmerced.2i2c.cloud)\nURL for repo (e.g. https://github.com/dsollberger/shared_stuff)\nindicate file name (i.e. the assignment’s notebook)\ncopy-and-paste generated link to assignment (i.e. back in Canvas)\n\n\n\nFuture Work\n\nTO DO: see if this access works from a student perspective\nTO DO: apply GitHub Actions (e.g. release file to students after a certain date/time)"
  },
  {
    "objectID": "posts/AI_resources/ai_resources.html",
    "href": "posts/AI_resources/ai_resources.html",
    "title": "Resources about Artificial Intelligence Tools",
    "section": "",
    "text": "(I wanted web space where I can organize various links I have encountered about artificial intelligence tools.\n\nGeneral\n\n(2023-06-14) Are Your Students Ready for AI? by Oguz A. Acar\n\n\n\nImage Generation\n\n(2023-08-18) AI-Created Art Isn’t Copyrightable by Winston Cho\nComputerphile\n\n(2022-10-20) Stable Diffusion in Code (AI Image Generation)\n(2022-10-04) How AI Image Generators Work (Stable Diffusion / Dall-E)\n\n\n\n\nNews Stories\n\n(2023-08-29) How to Use AI to Talk to Whales—and Save Life on Earth by Camille Bromley\n(2023-08-23) How Artificial Intelligence Gave a Paralyzed Woman Her Voice Back by Robin Marks and Laura Kurtzman\n(2023-08-13) These Women Tried to Warn Us About AI by Lorena O’Neil\n(2023-08-10) Supermarket AI meal planner app suggests recipe that would create chlorine gas by Tess McClure\n(2023-08-02) AI improves breast cancer detection rate by 20 percent by Ashleigh Furlong\n(2023-07-25) Adobe Staff Worried Their AI Tech Could Kill The Jobs Of Their Own Customers by Rounak Jain\n(2023-07-21) An MIT student asked AI to make her headshot more ‘professional.’ about Rona Wang’s explorations\n(2023-03-07) Many bioinformatics programming tasks can be automated with ChatGPT\n(2022-12-15) Artificial intelligence is transforming our world by Max Roser\n\n\n\nStudying\n\n(2023-03-02) How to learn to code FAST using ChatGPT by Tina Huang"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html",
    "href": "posts/lin_reg_demo/lin_reg_demo.html",
    "title": "Linear Regression Demonstration",
    "section": "",
    "text": "Predict how much moxillation takes place at 70 traxolline."
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#tidyverse",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#tidyverse",
    "title": "Linear Regression Demonstration",
    "section": "Tidyverse",
    "text": "Tidyverse\n\nlibrary(\"moderndive\") #for get_regression_table()\nlibrary(\"tidyverse\")\n\ndf_raw &lt;- readr::read_csv(\"air_quality_demo_data.csv\")\n\n# brand colors\n# orange on white: #e77500\n# orange on black: #f58025"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#data",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#data",
    "title": "Linear Regression Demonstration",
    "section": "Data",
    "text": "Data\nThis data set can be seen at aqicn.org and was accessed through the PurpleAir API\n\nhead(df_raw)\n\n# A tibble: 6 × 6\n  time_stamp          humidity temperature pressure pm1.0_atm pm2.5_atm\n  &lt;dttm&gt;                 &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 2023-06-10 06:00:00     34.3        81.3    1004.     0.355      0.64\n2 2023-06-10 12:00:00     30.8        85.0    1003.     8.78      12.6 \n3 2023-06-10 18:00:00     26.9        88.6    1003.    18.2       26.7 \n4 2023-06-11 00:00:00     37.0        82.0    1005.    19.2       28.9 \n5 2023-06-11 06:00:00     49.7        73.6    1006.    27.9       43.0 \n6 2023-06-11 12:00:00     33.6        89.5    1006.    27.7       41.7 \n\n\n\nabout 10 weeks of weather data, at 6-hour intervals (\\(n = 291\\) observations)\nquery: Why do we record different types of particulate matter measurements?"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#wrangling",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#wrangling",
    "title": "Linear Regression Demonstration",
    "section": "Wrangling",
    "text": "Wrangling\n\ndf &lt;- df_raw |&gt;\n  separate(time_stamp, into = c(\"date\", \"time\"), \n           remove = FALSE, sep = \" \") |&gt;\n  mutate(time = ifelse(is.na(time), \"00:00:00\", time),\n         day_part = case_when(\n           time == \"06:00:00\" ~ \"morning\",\n           time == \"12:00:00\" ~ \"noon\",\n           time == \"18:00:00\" ~ \"evening\",\n           .default = \"midnight\"\n         )) |&gt;\n  rename(pm1_0 = pm1.0_atm, pm2_5 = pm2.5_atm) |&gt;\n  select(time, day_part, humidity, temperature, pm2_5)\n\n\nseparate the time stamp into date and time columns\ndescribed parts of the day: morning, noon, evening, midnight\nrenamed particulate matter columns for ease\n\n\nhead(df)\n\n# A tibble: 6 × 5\n  time     day_part humidity temperature pm2_5\n  &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 06:00:00 morning      34.3        81.3  0.64\n2 12:00:00 noon         30.8        85.0 12.6 \n3 18:00:00 evening      26.9        88.6 26.7 \n4 00:00:00 midnight     37.0        82.0 28.9 \n5 06:00:00 morning      49.7        73.6 43.0 \n6 12:00:00 noon         33.6        89.5 41.7"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#on-predictions",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#on-predictions",
    "title": "Linear Regression Demonstration",
    "section": "",
    "text": "Predict how much moxillation takes place at 70 traxolline."
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#where-fit",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#where-fit",
    "title": "Linear Regression Demonstration",
    "section": "Where Fit?",
    "text": "Where Fit?\n\n\n\nWhere do we draw the line?"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#residuals",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#residuals",
    "title": "Linear Regression Demonstration",
    "section": "Residuals",
    "text": "Residuals\n\n\nGoal: Given a bivariate data set \\(\\{x_{i}, y_{i}\\}_{i=1}^{n}\\), form a linear regression model\n\\[\\hat{y} = a + bx\\]\nthat ``best fits’’ the data. Note that such a line will not go through all of the data (except in linear, deterministic situations), so\n\ndenote \\(y_{i}\\) for true outcomes\ndenote \\(\\hat{y}_{i}\\) for estimates (or predictions)\nthen \\(y_{i} - \\hat{y}_{i}\\) is the \\(i^{\\text{th}}\\) residual\n\n\n\n\n\nimage credit: www.jmp.com"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#method-of-least-squares",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#method-of-least-squares",
    "title": "Linear Regression Demonstration",
    "section": "Method of Least Squares",
    "text": "Method of Least Squares\nLike our derivation of formulas for variance and standard deviation, scientists decided to square the residuals (focus on size of residuals, avoid positive versus negative signs). Let the total error be\n\\[E(a,b) = \\displaystyle\\sum_{i = 1}^{n} (y_{i} - \\hat{y}_{i})^{2} = \\displaystyle\\sum_{i = 1}^{n} (y_{i} - a - bx_{i})^{2} \\]\n\nThe ``best-fit line’’ minimizes the error.\nTo minimize the error, start by setting the partial derivatives equal to zero:\n\n\\[\\displaystyle\\frac{\\partial E}{\\partial a} = 0, \\quad \\displaystyle\\frac{\\partial E}{\\partial b} = 0\\] Thankfully, the function \\(E(a,b)\\) is an elliptical paraboloid, so there is a global minimum at the critical point, and that minimum is found where\n\\[a = \\displaystyle\\frac{ (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }, \\quad b = \\displaystyle\\frac{ n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }\\]\n\n\n\n\n\n\nLinear Regression Model (Another View)\n\n\n\n\n\nIf sample means \\(\\bar{x}\\) and \\(\\bar{y}\\), sample standard deviations \\(s_{x}\\) and \\(s_{y}\\), and correlation coefficient \\(r\\) were previously computed, then the best-fit linear regression line \\(\\hat{y} = mx + b\\) is computed with\n\\[m = \\displaystyle\\frac{ rs_{y} }{ s_{x} }, \\quad b = \\bar{y} - m\\bar{x}\\]\n\nIf correlation \\(r &gt; 0\\), then the slope of the regression line is also positive\nIf correlation \\(r &lt; 0\\), then the slope of the regression line is also negative\n\n\n\n\n\n\n\n\n\n\nOutliers\n\n\n\n\n\nIn a scatterplot, an outlier is a point lying far away from the other data points. Paired sample data may include one or more influential points, which are points that strongly affect the graph of the regression line."
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#ggplot",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#ggplot",
    "title": "Linear Regression Demonstration",
    "section": "ggplot",
    "text": "ggplot\n\ngeom_smoothGraphCode\n\n\nThe geom_smooth layer is a quick way to draw the linear regression graph in ggplot.\n\n\n\n\n\n\n\n\n\n\ndf |&gt;\n  ggplot(aes(x = temperature, y = pm2_5)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#e77500\", \n              linewidth = 3, se = FALSE) +\n  labs(title = \"Princeton Air Quality\",\n       subtitle = \"Summer 2023\",\n       caption = \"Source: PurpleAir\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#model",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#model",
    "title": "Linear Regression Demonstration",
    "section": "Model",
    "text": "Model\n\nmodel1 &lt;- lm(pm2_5 ~ temperature, data = df)\n\n\nmodel1\n\n\nCall:\nlm(formula = pm2_5 ~ temperature, data = df)\n\nCoefficients:\n(Intercept)  temperature  \n   -12.9066       0.3805  \n\n\n\nInterpretation: for every one-degree increase in temperature, the PM2.5 level increases by 0.3805"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#prediction",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#prediction",
    "title": "Linear Regression Demonstration",
    "section": "Prediction",
    "text": "Prediction\n\nExamplePredictBookActivity\n\n\nPredict the PM2.5 level for a 78-degree day.\n\n\n\npredict(model1,\n        newdata = data.frame(temperature = 78))\n\n       1 \n16.77266 \n\n\n\n\n\n\n\nSeven Pillars of Statistical Wisdom\n\n\n\n\nThink about what is meant by linear regression. Draw a large area for a graph (\\(x\\)- and \\(y\\)-axes, first quadrant only, you do not have to label the axes). Come up with a scatterplot situation where performing linear regression and a subsequent prediction would be a bad idea."
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#setup-2",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#setup-2",
    "title": "Linear Regression Demonstration",
    "section": "Setup",
    "text": "Setup\n\nresponse variable (\\(Y\\)): particulate matter (2.5 microns)\npredictor variables\n\n\\(X_{1}\\): temperature\n\\(X_{2}\\): humidity\n\n\n\\[Y = a + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3} + b_{4}X_{4} + b_{5}X_{5}\\]\n\\[X_{3} = \\begin{cases} 1 & \\text{morning} \\\\ 0 & \\text{otherwise}\n\\end{cases}\\]"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#activity-2",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#activity-2",
    "title": "Linear Regression Demonstration",
    "section": "Activity",
    "text": "Activity\n\nPredict the PM2.5 level for a 60-degree morning.\nPredict the PM2.5 level for a 75-degree evening.\n\n\\[Y = -42.844 + 0.680X_{1} + 5.056X_{3} + 8.477X_{4} + 4.186X_{5}\\]\n\\[X_{3} = \\begin{cases} 1 & \\text{morning} \\\\ 0 & \\text{otherwise}\n\\end{cases}\\]\n\\[X_{4} = \\begin{cases} 1 & \\text{afternoon} \\\\ 0 & \\text{otherwise}\n\\end{cases}\\]\n\\[X_{5} = \\begin{cases} 1 & \\text{evening} \\\\ 0 & \\text{otherwise}\n\\end{cases}\\]"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#to-consider",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#to-consider",
    "title": "Linear Regression Demonstration",
    "section": "To Consider",
    "text": "To Consider\n\nHow do we know that the predictions are reliable?\nHow do we know which model to choose?\nHow do we know which variables to use?"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#book-2",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#book-2",
    "title": "Linear Regression Demonstration",
    "section": "Book",
    "text": "Book\n\n\n\nBayes Rules!"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#thanks",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#thanks",
    "title": "Linear Regression Demonstration",
    "section": "Thanks!",
    "text": "Thanks!\n\nDerek Sollberger\nLecturer of Data Science"
  },
  {
    "objectID": "posts/lin_reg_demo/lin_reg_demo.html#more-metrics",
    "href": "posts/lin_reg_demo/lin_reg_demo.html#more-metrics",
    "title": "Linear Regression Demonstration",
    "section": "More Metrics",
    "text": "More Metrics\n\nget_regression_table(model3)\n\n# A tibble: 5 × 7\n  term               estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept            -42.8     16.8       -2.54   0.012  -76.0      -9.68\n2 temperature            0.68     0.184      3.69   0        0.317     1.04\n3 day_part: midnight     5.06     2.89       1.75   0.081   -0.628    10.7 \n4 day_part: morning      8.48     3.29       2.57   0.011    2.00     15.0 \n5 day_part: noon         4.19     2.68       1.56   0.12    -1.10      9.47\n\n\n\nget_regression_summaries(model3)\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared   mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.046         0.033  235.  15.3  15.5      3.47   0.009     4   291"
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html",
    "href": "posts/AI_working_group/ai_working_group.html",
    "title": "AI Tools Fall Working Group",
    "section": "",
    "text": "This group of biology instructors will explore the new artificial intelligence technologies and ponder the ramifications on pedagogical environments. We will foster ideas that can benefit instructors and students in learning and production. We will build community guidelines, codes of conduct, and other stores of wisdom to aid our colleagues for the upcoming shifts in education.\n\n\n\n\n7, one-hour meetings, every other Tuesday\nSept 5 to Dec 6\nparticipants share facilitating meetings\nabout 25 minutes of homework before meetings\n\n\n\n\nStemming from literature, discussions, and in-class activities, we will build a combined survey article of lived experiences using artificial intelligence for classroom lessons and other academic endeavors along with providing shareable infographics to encourage conversations about responsible use of AI tools at other institutions.\n\n\n\nWith the relatively new crop of AI tools released in the past year, we understand that few people feel like experts in this area. This group looks to build together with optimism and curiosity about how our workflows and the structures that we have for our students can be altered in amazing ways.\n\n\n\n\nexamples of lesson plans that use AI tools\nexamples of our own use of AI tools\nbi-weekly exploration of AI tools in our group\nSuggestion: start thinking about a project for you own personal self that fits a chatGPT prompt like “Help me make a study plan for …”"
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#mission",
    "href": "posts/AI_working_group/ai_working_group.html#mission",
    "title": "AI Tools Fall Working Group",
    "section": "",
    "text": "This group of biology instructors will explore the new artificial intelligence technologies and ponder the ramifications on pedagogical environments. We will foster ideas that can benefit instructors and students in learning and production. We will build community guidelines, codes of conduct, and other stores of wisdom to aid our colleagues for the upcoming shifts in education."
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#schedule",
    "href": "posts/AI_working_group/ai_working_group.html#schedule",
    "title": "AI Tools Fall Working Group",
    "section": "",
    "text": "7, one-hour meetings, every other Tuesday\nSept 5 to Dec 6\nparticipants share facilitating meetings\nabout 25 minutes of homework before meetings"
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#goals",
    "href": "posts/AI_working_group/ai_working_group.html#goals",
    "title": "AI Tools Fall Working Group",
    "section": "",
    "text": "Stemming from literature, discussions, and in-class activities, we will build a combined survey article of lived experiences using artificial intelligence for classroom lessons and other academic endeavors along with providing shareable infographics to encourage conversations about responsible use of AI tools at other institutions."
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#expertise",
    "href": "posts/AI_working_group/ai_working_group.html#expertise",
    "title": "AI Tools Fall Working Group",
    "section": "",
    "text": "With the relatively new crop of AI tools released in the past year, we understand that few people feel like experts in this area. This group looks to build together with optimism and curiosity about how our workflows and the structures that we have for our students can be altered in amazing ways."
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#deliverables",
    "href": "posts/AI_working_group/ai_working_group.html#deliverables",
    "title": "AI Tools Fall Working Group",
    "section": "",
    "text": "examples of lesson plans that use AI tools\nexamples of our own use of AI tools\nbi-weekly exploration of AI tools in our group\nSuggestion: start thinking about a project for you own personal self that fits a chatGPT prompt like “Help me make a study plan for …”"
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#calculator",
    "href": "posts/AI_working_group/ai_working_group.html#calculator",
    "title": "AI Tools Fall Working Group",
    "section": "Calculator",
    "text": "Calculator\n\n\n“You are not always going to have a calculator in your pocket!”\n\n\n\n\nCalvin and Hobbes, November 25, 1992"
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#wikipedia",
    "href": "posts/AI_working_group/ai_working_group.html#wikipedia",
    "title": "AI Tools Fall Working Group",
    "section": "Wikipedia",
    "text": "Wikipedia\n\n\n\n“But they all just copy Wikipedia!”\nThe Grad Student Rap music video from January 9, 2011\n\n\n\n\n\nMake It Good Chuck"
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#cursive",
    "href": "posts/AI_working_group/ai_working_group.html#cursive",
    "title": "AI Tools Fall Working Group",
    "section": "Cursive",
    "text": "Cursive\n“You will only be allowed to write in cursive when you get to college!”  One Big Happy, March 4, 2020"
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#machine-learning",
    "href": "posts/AI_working_group/ai_working_group.html#machine-learning",
    "title": "AI Tools Fall Working Group",
    "section": "Machine Learning",
    "text": "Machine Learning\n\n\n\n“… machine learning can be considered a variant to traditional programming, in which it is predisposed in a machine the ability to learn from data (experience) independently, without being explicitly programmed to do it, to then be able to reuse what has been learned about a certain task.”\nquote and image source\n\n\n\n\n\nmachine learning"
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#artificial-intelligence",
    "href": "posts/AI_working_group/ai_working_group.html#artificial-intelligence",
    "title": "AI Tools Fall Working Group",
    "section": "Artificial Intelligence",
    "text": "Artificial Intelligence\n\n\n\n“It is the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but [artificial intelligence] does not have to confine itself to methods that are biologically observable.” — John McCarthy, Stanford CS professor, 2004 source\nimage source\n\n\n\n\n\nartificial intelligence"
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#large-language-models",
    "href": "posts/AI_working_group/ai_working_group.html#large-language-models",
    "title": "AI Tools Fall Working Group",
    "section": "Large Language Models",
    "text": "Large Language Models\n\n“Using self-supervised learning or semi-supervised learning, large language models (LLMs), which are language models made up of neural networks with billions of parameters, are trained on massive amounts of unlabeled text. LLMs are general-purpose models that excel at a variety of tasks as opposed to being trained for a single job.”\nquote and image source\n\n\n\n\nLLMs"
  },
  {
    "objectID": "posts/AI_working_group/ai_working_group.html#generative-ai",
    "href": "posts/AI_working_group/ai_working_group.html#generative-ai",
    "title": "AI Tools Fall Working Group",
    "section": "Generative AI",
    "text": "Generative AI\n\nDefinitionDall-EStable Diffusion\n\n\n\n\n\n“Generative AI is a form of artificial intelligence that is designed to generate content, including text, images, video and music. It uses large language models and algorithms to analyze patterns in datasets to mimic the style or structure of specific types of content.”\nquote and image source\n\n\n\n\n\ngenerative AI\n\n\n\n\n\n\n\n\n\nJune 2022\n\n\nimage source\n\n\n\n\n\nAugust 2023\n\n\nimage source"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html",
    "href": "posts/areal_data/14_proximity-and-areal-data.html",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "Learning objectives:\n\nconsider the definition of areal data\ndiscuss matters of graph theory\nexplore various methods of finding neighbors\n\n\nlibrary(\"dbscan\")     #density-based clustering\nlibrary(\"igraph\")     #graph networks\nlibrary(\"Matrix\")     #for sparse representation\nlibrary(\"sf\")         #simple features\nlibrary(\"spatialreg\") #spatial regression analysis\nlibrary(\"spdep\")      #spatial dependence\nlibrary(\"tmap\")       #quick, thematic maps\n\ndata(pol_pres15, package = \"spDataLarge\") #Poland 2015 election data\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] tmap_3.3-3       spdep_1.2-8      spatialreg_1.2-9 spData_2.2.2    \n[5] sf_1.0-14        Matrix_1.5-4     igraph_1.4.3     dbscan_1.1-11   \n\nloaded via a namespace (and not attached):\n [1] raster_3.6-23      xfun_0.40          htmlwidgets_1.6.2  lattice_0.21-8    \n [5] vctrs_0.6.3        tools_4.3.0        crosstalk_1.2.0    LearnBayes_2.15.1 \n [9] generics_0.1.3     parallel_4.3.0     sandwich_3.0-2     tibble_3.2.1      \n[13] proxy_0.4-27       fansi_1.0.4        pkgconfig_2.0.3    KernSmooth_2.23-20\n[17] RColorBrewer_1.1-3 leaflet_2.1.2      lifecycle_1.0.3    compiler_4.3.0    \n[21] deldir_1.0-9       terra_1.7-46       codetools_0.2-19   leafsync_0.1.0    \n[25] stars_0.6-3        htmltools_0.5.6    class_7.3-21       pillar_1.9.0      \n[29] MASS_7.3-58.4      classInt_0.4-10    lwgeom_0.2-13      wk_0.8.0          \n[33] boot_1.3-28.1      abind_1.4-5        multcomp_1.4-25    nlme_3.1-162      \n[37] tidyselect_1.2.0   digest_0.6.33      mvtnorm_1.2-0      dplyr_1.1.3       \n[41] splines_4.3.0      fastmap_1.1.1      grid_4.3.0         expm_0.999-7      \n[45] cli_3.6.1          magrittr_2.0.3     base64enc_0.1-3    dichromat_2.0-0.1 \n[49] XML_3.99-0.14      survival_3.5-5     utf8_1.2.3         leafem_0.2.0      \n[53] TH.data_1.1-2      e1071_1.7-13       sp_2.0-0           spDataLarge_2.0.9 \n[57] rmarkdown_2.24     png_0.1-8          zoo_1.8-12         coda_0.19-4       \n[61] evaluate_0.21      knitr_1.43         tmaptools_3.1-1    viridisLite_0.4.2 \n[65] s2_1.1.4           rlang_1.1.1        Rcpp_1.0.11        glue_1.6.2        \n[69] DBI_1.1.3          rstudioapi_0.15.0  jsonlite_1.8.7     R6_2.5.1          \n[73] units_0.8-3       \n\n\n\n\nAreal units of observation are very often used when simultaneous observations are aggregated within non-overlapping boundaries.\nExample: Lung cancer SIR (standardized incidence rate) in Pennsylvania\n\n\n\nimage credit: Paula Moraga\n\n\n\n\n\nBy proximity, we mean closeness in ways that make sense for the data generation processes thought to be involved. In cross-sectional geostatistical analysis with point support, measured distance makes sense for typical data generation processes.\nExample: Voronoi diagram of Pennsylvania\n\n\n\nimage credit: John Nerbonne\n\n\n\n\n\nBy support of data we mean the physical size (length, area, volume) associated with an individual observational unit\n\nIt is possible to represent the support of areal data by a point, despite the fact that the data have polygonal support\nWhen the intrinsic support of the data is represented as points, but the underlying process is between proximate observations rather than driven chiefly by distance between observations\nrisk of misrepresenting the footprint of the underlying spatial processes\n\n\n\n\nIdeas for spatial autocorrelation\n\n(graph theory) undirected graph, and its neighbors, or\n(geospatial) variogram\n\nBut what about\n\nislands?\ndisconnected subgraphs?\nsparse areas (cutoff by distance threshold)\n\n\n\n\nspdep package: spatial dependence\n\nnb class for neighbor\n\nlist of length 0L for no neighbors\n\nlistw object\n\nnb object\nlist of numerical weights\nhow the weights were calculated\n\nspatialreg package now has the functions for constructing and handling neighbour and spatial weights objects, tests for spatial autocorrelation, and model fitting functions that used to be in spdep\n\n\n\n\n\npol_pres15 |&gt;\n    subset(select = c(TERYT, name, types)) |&gt;\n    head()\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 235157.1 ymin: 366913.3 xmax: 281431.7 ymax: 413016.6\nProjected CRS: ETRS89 / Poland CS92\n   TERYT                name       types                       geometry\n1 020101         BOLESŁAWIEC       Urban MULTIPOLYGON (((261089.5 38...\n2 020102         BOLESŁAWIEC       Rural MULTIPOLYGON (((254150 3837...\n3 020103            GROMADKA       Rural MULTIPOLYGON (((275346 3846...\n4 020104        NOWOGRODZIEC Urban/rural MULTIPOLYGON (((251769.8 37...\n5 020105          OSIECZNICA       Rural MULTIPOLYGON (((263423.9 40...\n6 020106 WARTA BOLESŁAWIECKA       Rural MULTIPOLYGON (((267030.7 38...\n\n\n\n# tmap\ntm_shape(pol_pres15) + tm_fill(\"types\")\n\n\n\n\nFor safety’s sake, we impose topological validity:\n\nif (!all(st_is_valid(pol_pres15)))\n        pol_pres15 &lt;- st_make_valid(pol_pres15)\n\n\n\n\nFor each observation, the poly2nb function checks whether\n\nat least one (queen = TRUE)\nat least two (“rook”, queen = FALSE)\n\npoints are within snap distance of each other.\n\npol_pres15 |&gt; poly2nb(queen = TRUE) -&gt; nb_q\n\n\nprint(nb_q)\n\nNeighbour list object:\nNumber of regions: 2495 \nNumber of nonzero links: 14242 \nPercentage nonzero weights: 0.2287862 \nAverage number of links: 5.708216 \n\n\n\ns2 spherical coordinates are used by default\nsymmetric relationships assumed\nrow.names may be customized\n\n\n\nAre the data connected? (Some model estimation techniques do not support graphs that are not connected.)\n\n(nb_q |&gt; n.comp.nb())$nc\n# result: 1 for TRUE (more than one for FALSE)\n\n\n\nverbose code\n\n\n#library(Matrix, warn.conflicts = FALSE)\n#library(spatialreg, warn.conflicts = FALSE)\nnb_q |&gt; \n    nb2listw(style = \"B\") |&gt; \n    as(\"CsparseMatrix\") -&gt; smat\n#library(igraph, warn.conflicts = FALSE)\nsmat |&gt; graph.adjacency() -&gt; g1\n\ng1 |&gt; count_components()\n\n[1] 1\n\n\n\n\n\n\n\nThe simplest form is by using triangulation, here using the deldir package\n\n# get centroids and save their coordinates\npol_pres15 |&gt; \n    st_geometry() |&gt; \n    st_centroid(of_largest_polygon = TRUE) -&gt; coords \n\n# triangulation\n(coords |&gt; tri2nb() -&gt; nb_tri)\n\nNeighbour list object:\nNumber of regions: 2495 \nNumber of nonzero links: 14930 \nPercentage nonzero weights: 0.2398384 \nAverage number of links: 5.983968 \n\n\n\n\n\n# results in meters\nnb_tri |&gt; \n    nbdists(coords) |&gt; \n    unlist() |&gt; \n    summary()\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   246.6   9847.2  12151.2  13485.2  14993.5 296973.7 \n\n\n\n\n\nThe Sphere of Influence soi.graph function takes triangulated neighbours and prunes off neighbour relationships represented by edges that are unusually long for each point.\n\n(nb_tri |&gt; \n        soi.graph(coords) |&gt; \n        graph2nb() -&gt; nb_soi)\n\nNeighbour list object:\nNumber of regions: 2495 \nNumber of nonzero links: 12792 \nPercentage nonzero weights: 0.2054932 \nAverage number of links: 5.127054 \n\n\n\n# Poland\npol_pres15 |&gt; \n    st_geometry() |&gt; \n    plot(border = \"grey\", lwd = 0.5)\n\n\n\n\n\n# triangulation\nplot(nb_tri, \n     coords = coords, points = FALSE, lwd = 0.5)\n\n\n\n\n\n# sphere of influence\nplot(nb_soi, \n     coords = coords, points = FALSE, lwd = 0.5)\n\n\n\n\n\n\n\n\n\nDistance-based neighbours can be constructed using dnearneigh, with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument\nThe knearneigh function for -nearest neighbours returns a knn object, converted to an nb object using knn2nb\nComputation speed boost through dbscan package\nThe nbdists function returns the length of neighbour relationship edges\n\n\ncoords |&gt; \n    knearneigh(k = 1) |&gt; \n    knn2nb() |&gt; \n    nbdists(coords) |&gt; \n    unlist() |&gt; \n    summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  246.5  6663.4  8538.0  8275.1 10123.9 17978.8 \n\n\nHere the largest first nearest neighbour distance is just under 18 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n# maybe no neighbors\ncoords |&gt; dnearneigh(0, 16000) -&gt; nb_d16\n\n# at least one neighbor\ncoords |&gt; dnearneigh(0, 18000) -&gt; nb_d18\n\n\nAdding 300 m to the threshold gives us a neighbour object with no no-neighbour units, and all units can be reached from all others across the graph.\n\n\n# connected graph\ncoords |&gt; dnearneigh(0, 18300) -&gt; nb_d183\n\nIt is possible to control the numbers of neighbours directly using -nearest neighbours, either accepting asymmetric neighbours\n\ncoords |&gt; knearneigh(k = 6) -&gt; knn_k6\n\n# asymmetrical\nknn_k6 |&gt; knn2nb() -&gt; nb_k6\n\n# symmetrical\nknn_k6 |&gt; knn2nb(sym = TRUE) -&gt; nb_k6s\n\n\n\n\nOnce neighbour objects are available, further choices need to be made in specifying the weights objects.\n\nThe nb2listw function is used to create a listw weights object with an nb object, a matching list of weights vectors, and a style specification\nBecause handling no-neighbour observations now begins to matter, the zero.policy= argument is introduced (default: FALSE)\nn: number of observations\n\\(S_{0}\\): sum of weights\n\nThe “B” binary style gives a weight of unity to each neighbour relationship, and typically up-weights units with no boundaries on the edge of the study area, having a higher count of neighbours.\n\nnb_q |&gt; nb2listw(style = \"B\") -&gt; lw_q_B\n\nlw_q_B |&gt; \n    spweights.constants() |&gt; \n    data.frame() |&gt; \n    subset(select = c(n, S0))\n\n     n    S0\n1 2495 14242\n\n\nThe “W” row-standardised style up-weights units around the edge of the study area that necessarily have fewer neighbours. This style first gives a weight of unity to each neighbour relationship, then it divides these weights by the per unit sums of weights (caution: avoid no-neighbors)\n\nnb_q |&gt; nb2listw(style = \"W\") -&gt; lw_q_W\n\nlw_q_W |&gt; \n    spweights.constants() |&gt; \n    data.frame() |&gt; \n    subset(select = c(n, S0))\n\n     n   S0\n1 2495 2495\n\n\n\n\n\nnb_d183 |&gt; \n    nbdists(coords) |&gt; \n    lapply(function(x) 1/(x/1000)) -&gt; gwts\n\nNo-neighbour handling is by default to prevent the construction of a weights object, making the analyst take a position on how to proceed.\n\n(nb_d183 |&gt; nb2listw(glist=gwts, style=\"B\") -&gt; lw_d183_idw_B) |&gt; \n    spweights.constants() |&gt; \n    data.frame() |&gt; \n    subset(select=c(n, S0))\n\n     n       S0\n1 2495 1841.345\n\n\nUse can be made of the zero.policy= argument to many functions used with nb and listw objects.\n\n(nb_d16 |&gt; \n    nb2listw(style=\"B\", zero.policy=TRUE) |&gt; \n    spweights.constants(zero.policy=TRUE) |&gt; \n    data.frame() |&gt; \n    subset(select=c(n, S0)))\n\n     n    S0\n1 2488 15850\n\n\n\n\n\n\nIf we wish to create an object showing to neighbours, where \\(i\\) is a neighbour of \\(j\\), and \\(j\\) in turn is a neighbour of \\(k\\), so taking two steps on the neighbour graph, we can use nblag (automatically removes \\(i\\) to \\(i\\) self-neighbours)\n\nnb_q |&gt; nblag(2) -&gt; nb_q2\n\nReturning to the graph representation of the same neighbour object, we can ask how many steps might be needed to traverse the graph?\n\nigraph::diameter(g1) #where g1 is a graph object\n\n[1] 52\n\n\nWe step out from each observation across the graph to establish the number of steps needed to reach each other observation by the shortest path (creating an \\(n \\times n\\) matrix sps), once again finding the same maximum count.\n\ng1 |&gt; shortest.paths() -&gt; sps\nsps |&gt; apply(2, max) -&gt; spmax\n\nspmax |&gt; max()\n\n[1] 52\n\n\nThe municipality with the maximum count is called Lutowiska, close to the Ukrainian border in the far south east of the country.\n\nmr &lt;- which.max(spmax)\npol_pres15$name0[mr]\n\n[1] \"Lutowiska\"\n\n\n\npol_pres15$sps1 &lt;- sps[,mr]\ntm_shape(pol_pres15) +\n          tm_fill(\"sps1\", title = \"Shortest path\\ncount\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeeting chat log\n\nLOG"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#areal-data",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#areal-data",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "Areal units of observation are very often used when simultaneous observations are aggregated within non-overlapping boundaries.\nExample: Lung cancer SIR (standardized incidence rate) in Pennsylvania\n\n\n\nimage credit: Paula Moraga"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#proximity-data",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#proximity-data",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "By proximity, we mean closeness in ways that make sense for the data generation processes thought to be involved. In cross-sectional geostatistical analysis with point support, measured distance makes sense for typical data generation processes.\nExample: Voronoi diagram of Pennsylvania\n\n\n\nimage credit: John Nerbonne"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#support",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#support",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "By support of data we mean the physical size (length, area, volume) associated with an individual observational unit\n\nIt is possible to represent the support of areal data by a point, despite the fact that the data have polygonal support\nWhen the intrinsic support of the data is represented as points, but the underlying process is between proximate observations rather than driven chiefly by distance between observations\nrisk of misrepresenting the footprint of the underlying spatial processes"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#representing-proximity",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#representing-proximity",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "Ideas for spatial autocorrelation\n\n(graph theory) undirected graph, and its neighbors, or\n(geospatial) variogram\n\nBut what about\n\nislands?\ndisconnected subgraphs?\nsparse areas (cutoff by distance threshold)"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#spdep-package",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#spdep-package",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "spdep package: spatial dependence\n\nnb class for neighbor\n\nlist of length 0L for no neighbors\n\nlistw object\n\nnb object\nlist of numerical weights\nhow the weights were calculated\n\nspatialreg package now has the functions for constructing and handling neighbour and spatial weights objects, tests for spatial autocorrelation, and model fitting functions that used to be in spdep"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#example-poland-2015-election",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#example-poland-2015-election",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "pol_pres15 |&gt;\n    subset(select = c(TERYT, name, types)) |&gt;\n    head()\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 235157.1 ymin: 366913.3 xmax: 281431.7 ymax: 413016.6\nProjected CRS: ETRS89 / Poland CS92\n   TERYT                name       types                       geometry\n1 020101         BOLESŁAWIEC       Urban MULTIPOLYGON (((261089.5 38...\n2 020102         BOLESŁAWIEC       Rural MULTIPOLYGON (((254150 3837...\n3 020103            GROMADKA       Rural MULTIPOLYGON (((275346 3846...\n4 020104        NOWOGRODZIEC Urban/rural MULTIPOLYGON (((251769.8 37...\n5 020105          OSIECZNICA       Rural MULTIPOLYGON (((263423.9 40...\n6 020106 WARTA BOLESŁAWIECKA       Rural MULTIPOLYGON (((267030.7 38...\n\n\n\n# tmap\ntm_shape(pol_pres15) + tm_fill(\"types\")\n\n\n\n\nFor safety’s sake, we impose topological validity:\n\nif (!all(st_is_valid(pol_pres15)))\n        pol_pres15 &lt;- st_make_valid(pol_pres15)"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#contiguous-neighbors",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#contiguous-neighbors",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "For each observation, the poly2nb function checks whether\n\nat least one (queen = TRUE)\nat least two (“rook”, queen = FALSE)\n\npoints are within snap distance of each other.\n\npol_pres15 |&gt; poly2nb(queen = TRUE) -&gt; nb_q\n\n\nprint(nb_q)\n\nNeighbour list object:\nNumber of regions: 2495 \nNumber of nonzero links: 14242 \nPercentage nonzero weights: 0.2287862 \nAverage number of links: 5.708216 \n\n\n\ns2 spherical coordinates are used by default\nsymmetric relationships assumed\nrow.names may be customized\n\n\n\nAre the data connected? (Some model estimation techniques do not support graphs that are not connected.)\n\n(nb_q |&gt; n.comp.nb())$nc\n# result: 1 for TRUE (more than one for FALSE)\n\n\n\nverbose code\n\n\n#library(Matrix, warn.conflicts = FALSE)\n#library(spatialreg, warn.conflicts = FALSE)\nnb_q |&gt; \n    nb2listw(style = \"B\") |&gt; \n    as(\"CsparseMatrix\") -&gt; smat\n#library(igraph, warn.conflicts = FALSE)\nsmat |&gt; graph.adjacency() -&gt; g1\n\ng1 |&gt; count_components()\n\n[1] 1"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#graph-based-neighbors",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#graph-based-neighbors",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "The simplest form is by using triangulation, here using the deldir package\n\n# get centroids and save their coordinates\npol_pres15 |&gt; \n    st_geometry() |&gt; \n    st_centroid(of_largest_polygon = TRUE) -&gt; coords \n\n# triangulation\n(coords |&gt; tri2nb() -&gt; nb_tri)\n\nNeighbour list object:\nNumber of regions: 2495 \nNumber of nonzero links: 14930 \nPercentage nonzero weights: 0.2398384 \nAverage number of links: 5.983968 \n\n\n\n\n\n# results in meters\nnb_tri |&gt; \n    nbdists(coords) |&gt; \n    unlist() |&gt; \n    summary()\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   246.6   9847.2  12151.2  13485.2  14993.5 296973.7 \n\n\n\n\n\nThe Sphere of Influence soi.graph function takes triangulated neighbours and prunes off neighbour relationships represented by edges that are unusually long for each point.\n\n(nb_tri |&gt; \n        soi.graph(coords) |&gt; \n        graph2nb() -&gt; nb_soi)\n\nNeighbour list object:\nNumber of regions: 2495 \nNumber of nonzero links: 12792 \nPercentage nonzero weights: 0.2054932 \nAverage number of links: 5.127054 \n\n\n\n# Poland\npol_pres15 |&gt; \n    st_geometry() |&gt; \n    plot(border = \"grey\", lwd = 0.5)\n\n\n\n\n\n# triangulation\nplot(nb_tri, \n     coords = coords, points = FALSE, lwd = 0.5)\n\n\n\n\n\n# sphere of influence\nplot(nb_soi, \n     coords = coords, points = FALSE, lwd = 0.5)"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#distance-based-neighbors",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#distance-based-neighbors",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "Distance-based neighbours can be constructed using dnearneigh, with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument\nThe knearneigh function for -nearest neighbours returns a knn object, converted to an nb object using knn2nb\nComputation speed boost through dbscan package\nThe nbdists function returns the length of neighbour relationship edges\n\n\ncoords |&gt; \n    knearneigh(k = 1) |&gt; \n    knn2nb() |&gt; \n    nbdists(coords) |&gt; \n    unlist() |&gt; \n    summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  246.5  6663.4  8538.0  8275.1 10123.9 17978.8 \n\n\nHere the largest first nearest neighbour distance is just under 18 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n# maybe no neighbors\ncoords |&gt; dnearneigh(0, 16000) -&gt; nb_d16\n\n# at least one neighbor\ncoords |&gt; dnearneigh(0, 18000) -&gt; nb_d18\n\n\nAdding 300 m to the threshold gives us a neighbour object with no no-neighbour units, and all units can be reached from all others across the graph.\n\n\n# connected graph\ncoords |&gt; dnearneigh(0, 18300) -&gt; nb_d183\n\nIt is possible to control the numbers of neighbours directly using -nearest neighbours, either accepting asymmetric neighbours\n\ncoords |&gt; knearneigh(k = 6) -&gt; knn_k6\n\n# asymmetrical\nknn_k6 |&gt; knn2nb() -&gt; nb_k6\n\n# symmetrical\nknn_k6 |&gt; knn2nb(sym = TRUE) -&gt; nb_k6s"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#weights-specification",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#weights-specification",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "Once neighbour objects are available, further choices need to be made in specifying the weights objects.\n\nThe nb2listw function is used to create a listw weights object with an nb object, a matching list of weights vectors, and a style specification\nBecause handling no-neighbour observations now begins to matter, the zero.policy= argument is introduced (default: FALSE)\nn: number of observations\n\\(S_{0}\\): sum of weights\n\nThe “B” binary style gives a weight of unity to each neighbour relationship, and typically up-weights units with no boundaries on the edge of the study area, having a higher count of neighbours.\n\nnb_q |&gt; nb2listw(style = \"B\") -&gt; lw_q_B\n\nlw_q_B |&gt; \n    spweights.constants() |&gt; \n    data.frame() |&gt; \n    subset(select = c(n, S0))\n\n     n    S0\n1 2495 14242\n\n\nThe “W” row-standardised style up-weights units around the edge of the study area that necessarily have fewer neighbours. This style first gives a weight of unity to each neighbour relationship, then it divides these weights by the per unit sums of weights (caution: avoid no-neighbors)\n\nnb_q |&gt; nb2listw(style = \"W\") -&gt; lw_q_W\n\nlw_q_W |&gt; \n    spweights.constants() |&gt; \n    data.frame() |&gt; \n    subset(select = c(n, S0))\n\n     n   S0\n1 2495 2495\n\n\n\n\n\nnb_d183 |&gt; \n    nbdists(coords) |&gt; \n    lapply(function(x) 1/(x/1000)) -&gt; gwts\n\nNo-neighbour handling is by default to prevent the construction of a weights object, making the analyst take a position on how to proceed.\n\n(nb_d183 |&gt; nb2listw(glist=gwts, style=\"B\") -&gt; lw_d183_idw_B) |&gt; \n    spweights.constants() |&gt; \n    data.frame() |&gt; \n    subset(select=c(n, S0))\n\n     n       S0\n1 2495 1841.345\n\n\nUse can be made of the zero.policy= argument to many functions used with nb and listw objects.\n\n(nb_d16 |&gt; \n    nb2listw(style=\"B\", zero.policy=TRUE) |&gt; \n    spweights.constants(zero.policy=TRUE) |&gt; \n    data.frame() |&gt; \n    subset(select=c(n, S0)))\n\n     n    S0\n1 2488 15850"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#higher-order-neighbors",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#higher-order-neighbors",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "If we wish to create an object showing to neighbours, where \\(i\\) is a neighbour of \\(j\\), and \\(j\\) in turn is a neighbour of \\(k\\), so taking two steps on the neighbour graph, we can use nblag (automatically removes \\(i\\) to \\(i\\) self-neighbours)\n\nnb_q |&gt; nblag(2) -&gt; nb_q2\n\nReturning to the graph representation of the same neighbour object, we can ask how many steps might be needed to traverse the graph?\n\nigraph::diameter(g1) #where g1 is a graph object\n\n[1] 52\n\n\nWe step out from each observation across the graph to establish the number of steps needed to reach each other observation by the shortest path (creating an \\(n \\times n\\) matrix sps), once again finding the same maximum count.\n\ng1 |&gt; shortest.paths() -&gt; sps\nsps |&gt; apply(2, max) -&gt; spmax\n\nspmax |&gt; max()\n\n[1] 52\n\n\nThe municipality with the maximum count is called Lutowiska, close to the Ukrainian border in the far south east of the country.\n\nmr &lt;- which.max(spmax)\npol_pres15$name0[mr]\n\n[1] \"Lutowiska\"\n\n\n\npol_pres15$sps1 &lt;- sps[,mr]\ntm_shape(pol_pres15) +\n          tm_fill(\"sps1\", title = \"Shortest path\\ncount\")"
  },
  {
    "objectID": "posts/areal_data/14_proximity-and-areal-data.html#meeting-videos",
    "href": "posts/areal_data/14_proximity-and-areal-data.html#meeting-videos",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "Meeting chat log\n\nLOG"
  },
  {
    "objectID": "posts/baseball_gt/baseball_gt.html",
    "href": "posts/baseball_gt/baseball_gt.html",
    "title": "Some Quick Baseball Stats",
    "section": "",
    "text": "library(\"gt\")\nlibrary(\"Lahman\")\nlibrary(\"tidyverse\")\n\n\ndf &lt;- Teams |&gt;\n  filter(yearID &gt;= 2004) #focusing on the past 18 season\n\n\nreadr::write_csv(df, \"baseball_2004_2022.csv\")\n\n\ncolnames(df)\n\n [1] \"yearID\"         \"lgID\"           \"teamID\"         \"franchID\"      \n [5] \"divID\"          \"Rank\"           \"G\"              \"Ghome\"         \n [9] \"W\"              \"L\"              \"DivWin\"         \"WCWin\"         \n[13] \"LgWin\"          \"WSWin\"          \"R\"              \"AB\"            \n[17] \"H\"              \"X2B\"            \"X3B\"            \"HR\"            \n[21] \"BB\"             \"SO\"             \"SB\"             \"CS\"            \n[25] \"HBP\"            \"SF\"             \"RA\"             \"ER\"            \n[29] \"ERA\"            \"CG\"             \"SHO\"            \"SV\"            \n[33] \"IPouts\"         \"HA\"             \"HRA\"            \"BBA\"           \n[37] \"SOA\"            \"E\"              \"DP\"             \"FP\"            \n[41] \"name\"           \"park\"           \"attendance\"     \"BPF\"           \n[45] \"PPF\"            \"teamIDBR\"       \"teamIDlahman45\" \"teamIDretro\"   \n\n\n\ndf |&gt;\n  select(yearID, franchID, HR) |&gt; #select columns to keep\n  slice_max(HR, n = 10) |&gt; #finds 10 highest values by numerical variable\n  gt() #makes nice tables\n\n\n\n\n\n  \n    \n    \n      yearID\n      franchID\n      HR\n    \n  \n  \n    2019\nMIN\n307\n    2019\nNYY\n306\n    2019\nHOU\n288\n    2019\nLAD\n279\n    2018\nNYY\n267\n    2021\nTOR\n262\n    2005\nTEX\n260\n    2010\nTOR\n257\n    2019\nOAK\n257\n    2019\nCHC\n256\n  \n  \n  \n\n\n\n\n\ndf |&gt;\n  select(yearID, franchID, ERA) |&gt; #select columns to keep\n  slice_min(ERA, n = 10) |&gt; #finds 10 loweest values by numerical variable\n  gt() #makes nice tables\n\n\n\n\n\n  \n    \n    \n      yearID\n      franchID\n      ERA\n    \n  \n  \n    2022\nLAD\n2.80\n    2022\nHOU\n2.90\n    2015\nSTL\n2.94\n    2021\nLAD\n3.01\n    2011\nPHI\n3.02\n    2020\nLAD\n3.02\n    2014\nWSN\n3.03\n    2018\nHOU\n3.11\n    2016\nCHC\n3.15\n    2014\nSEA\n3.17"
  },
  {
    "objectID": "posts/baseball_gt/baseball_gt.html#lahman",
    "href": "posts/baseball_gt/baseball_gt.html#lahman",
    "title": "Some Quick Baseball Stats",
    "section": "",
    "text": "library(\"gt\")\nlibrary(\"Lahman\")\nlibrary(\"tidyverse\")\n\n\ndf &lt;- Teams |&gt;\n  filter(yearID &gt;= 2004) #focusing on the past 18 season\n\n\nreadr::write_csv(df, \"baseball_2004_2022.csv\")\n\n\ncolnames(df)\n\n [1] \"yearID\"         \"lgID\"           \"teamID\"         \"franchID\"      \n [5] \"divID\"          \"Rank\"           \"G\"              \"Ghome\"         \n [9] \"W\"              \"L\"              \"DivWin\"         \"WCWin\"         \n[13] \"LgWin\"          \"WSWin\"          \"R\"              \"AB\"            \n[17] \"H\"              \"X2B\"            \"X3B\"            \"HR\"            \n[21] \"BB\"             \"SO\"             \"SB\"             \"CS\"            \n[25] \"HBP\"            \"SF\"             \"RA\"             \"ER\"            \n[29] \"ERA\"            \"CG\"             \"SHO\"            \"SV\"            \n[33] \"IPouts\"         \"HA\"             \"HRA\"            \"BBA\"           \n[37] \"SOA\"            \"E\"              \"DP\"             \"FP\"            \n[41] \"name\"           \"park\"           \"attendance\"     \"BPF\"           \n[45] \"PPF\"            \"teamIDBR\"       \"teamIDlahman45\" \"teamIDretro\"   \n\n\n\ndf |&gt;\n  select(yearID, franchID, HR) |&gt; #select columns to keep\n  slice_max(HR, n = 10) |&gt; #finds 10 highest values by numerical variable\n  gt() #makes nice tables\n\n\n\n\n\n  \n    \n    \n      yearID\n      franchID\n      HR\n    \n  \n  \n    2019\nMIN\n307\n    2019\nNYY\n306\n    2019\nHOU\n288\n    2019\nLAD\n279\n    2018\nNYY\n267\n    2021\nTOR\n262\n    2005\nTEX\n260\n    2010\nTOR\n257\n    2019\nOAK\n257\n    2019\nCHC\n256\n  \n  \n  \n\n\n\n\n\ndf |&gt;\n  select(yearID, franchID, ERA) |&gt; #select columns to keep\n  slice_min(ERA, n = 10) |&gt; #finds 10 loweest values by numerical variable\n  gt() #makes nice tables\n\n\n\n\n\n  \n    \n    \n      yearID\n      franchID\n      ERA\n    \n  \n  \n    2022\nLAD\n2.80\n    2022\nHOU\n2.90\n    2015\nSTL\n2.94\n    2021\nLAD\n3.01\n    2011\nPHI\n3.02\n    2020\nLAD\n3.02\n    2014\nWSN\n3.03\n    2018\nHOU\n3.11\n    2016\nCHC\n3.15\n    2014\nSEA\n3.17"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html",
    "href": "posts/2023_map_challenge/2023_map_challenge.html",
    "title": "30 Day Map Challenge",
    "section": "",
    "text": "Even though I have done the 30 Day Map Challenge each of the past two years, since data visualization is just a hobby for me, I feel like I start from scratch each time. This year, I decided to lean into that feeling and treat the month as production of a long blog post that likewise nearly starts from scratch.\n\nI am interested in learning about the state of New Jersey, and we will perhaps get data from sites such as\n\nNJDEP Bureau of GIS and its ArcGIS repository\nNJGIN Open Data\n\nMost of my work here will be performed with the ggplot2 and sf (“special features”) packages in the R programmer universe.\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"ggtext\")\nlibrary(\"patchwork\")\nlibrary(\"sf\")"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#introduction",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#introduction",
    "title": "30 Day Map Challenge",
    "section": "",
    "text": "Even though I have done the 30 Day Map Challenge each of the past two years, since data visualization is just a hobby for me, I feel like I start from scratch each time. This year, I decided to lean into that feeling and treat the month as production of a long blog post that likewise nearly starts from scratch.\n\nI am interested in learning about the state of New Jersey, and we will perhaps get data from sites such as\n\nNJDEP Bureau of GIS and its ArcGIS repository\nNJGIN Open Data\n\nMost of my work here will be performed with the ggplot2 and sf (“special features”) packages in the R programmer universe.\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"ggtext\")\nlibrary(\"patchwork\")\nlibrary(\"sf\")"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#graph-labels",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#graph-labels",
    "title": "30 Day Map Challenge",
    "section": "Graph Labels",
    "text": "Graph Labels\nFor now, I should get in the habit of labeling my graphs.\n\nnj_colleges |&gt;\n  ggplot() +\n  geom_sf() +\n  labs(title = \"Colleges of New Jersey\",\n       subtitle = \"30 Day Map Challenge, Day 1: Points\",\n       caption = \"Data source: NJGIN\")\n\n\n\n\nOf course, this product lacks meaning without context, but that gives us something to look forward to in future days!"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#aesthetic-customization",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#aesthetic-customization",
    "title": "30 Day Map Challenge",
    "section": "Aesthetic Customization",
    "text": "Aesthetic Customization\nEach day, I may challenge myself to add to the complexity. For now, let us emphasize the “line” in the picture by customizing the color.\n\nnj_state_shp |&gt;\n  ggplot() +\n  geom_sf(color = \"blue\", linewidth = 3) + #updated attributes\n  labs(title = \"The State of New Jersey\",\n       subtitle = \"30 Day Map Challenge, Day 2: Lines\",\n       caption = \"Data source: NJGIN\")"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#aesthetic-mapping",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#aesthetic-mapping",
    "title": "30 Day Map Challenge",
    "section": "Aesthetic Mapping",
    "text": "Aesthetic Mapping\nOne of the joys of programming in R is the ease of color-coding by a categorical label. Here, we can create a rather impressive map by letting the software choose various colors for the counties.\nFirst, we should get a sense of the variable names.\n\ncolnames(nj_counties)\n\n [1] \"OBJECTID\"   \"GLOBALID\"   \"COUNTY\"     \"COUNTY_LAB\" \"CO\"        \n [6] \"GNIS_NAME\"  \"GNIS\"       \"FIPSSTCO\"   \"FIPSCO\"     \"ACRES\"     \n[11] \"SQ_MILES\"   \"POP2020\"    \"POP2010\"    \"POP2000\"    \"POP1990\"   \n[16] \"POP1980\"    \"POPDEN2020\" \"POPDEN2010\" \"POPDEN2000\" \"POPDEN1990\"\n[21] \"POPDEN1980\" \"REGION\"     \"Shape_Leng\" \"Shape_Area\" \"geometry\"  \n\n\n\nnj_counties |&gt;\n  ggplot() +\n  geom_sf(aes(fill = COUNTY)) +\n  labs(title = \"The Counties of New Jersey\",\n       subtitle = \"30 Day Map Challenge, Day 3: Polygons\",\n       caption = \"Data source: NJGIN\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#centroids",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#centroids",
    "title": "30 Day Map Challenge",
    "section": "Centroids",
    "text": "Centroids\nThe way that geom_text or geom_label work inside ggplot is that you need to indicate the aesthetics of locations (i.e. x and y coordinates) and labels. That is, while polygons are many points of data, we need to compute one point per polygon to tell the software where to put the labels. One way to compute those locations is to compute the centroids (and, long story short, there are several ways to compute centroids).\n\n# Calculate the centroid of each hexagon to add the label\n# https://stackoverflow.com/questions/49343958/do-the-values-returned-by-rgeosgcentroid-and-sfst-centroid-differ\ncenters &lt;- data.frame(\n  st_coordinates(st_centroid(nj_counties$geometry)),\n  id=nj_counties$COUNTY)\n\nnj_counties &lt;- nj_counties |&gt;\n  left_join(centers, by = c(\"COUNTY\" = \"id\"))\n\nNow, the data frame has convenient “X” and “Y” coordinates (which happen to be capital letters in these default codes)."
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#aesthetic-labeling",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#aesthetic-labeling",
    "title": "30 Day Map Challenge",
    "section": "Aesthetic Labeling",
    "text": "Aesthetic Labeling\nNow let us see how geom_label works with our map so far.\n\nnj_counties |&gt;\n  ggplot() +\n  geom_sf(aes(fill = COUNTY)) +\n  geom_label(aes(x = X, y = Y, label = COUNTY)) +\n  labs(title = \"The Counties of New Jersey\",\n       subtitle = \"30 Day Map Challenge, Day 4: A Bad Map\",\n       caption = \"Data source: NJGIN\") +\n  theme(legend.position = \"none\")\n\n\n\n\nThere are certainly ways to improve the readability and beauty of this map, and we might explore those ways in future days."
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#coordinate-reference-systems",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#coordinate-reference-systems",
    "title": "30 Day Map Challenge",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\nLong story short, each map of the 3D Earth is projected onto a 2D plane, but there are ways to perform that projection (with goals such as maintaining areas or shapes as well as possible). For our purposes, the work here will be easier if both of the California and New Jersey shapefiles use the same projection.\n\n# check CRS\nst_crs(ca_state_shp)[1]\n\n$input\n[1] \"WGS 84 / Pseudo-Mercator\"\n\nst_crs(nj_state_shp)[1]\n\n$input\n[1] \"NAD83 / New Jersey (ftUS)\"\n\n\nWe can observe that the projection systems are not the same. Hopefully, the simplest fix is to simply re-project the CRS of the California shapefile into the CRS of the New Jersey shapefile.\n\n# set CRS\n# st_crs(ca_state_shp) &lt;- st_crs(nj_state_shp)\nca_state_shp &lt;- st_transform(ca_state_shp, st_crs(nj_state_shp))\n\n# verify\nst_crs(ca_state_shp)[1]\n\n$input\n[1] \"NAD83 / New Jersey (ftUS)\"\n\n\nSo far, here is the juxtaposition.\n\nnj_state_shp |&gt;\n  ggplot() +\n  geom_sf() +\n  geom_sf(data = ca_state_shp)"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#coordinates",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#coordinates",
    "title": "30 Day Map Challenge",
    "section": "Coordinates",
    "text": "Coordinates\nFor some of the math I intend on carrying out, I will need to manipulate the latitude and/or longitude values (i.e. the x and y coordinates). We can extract coordinates using the st_geometry command.\n\nca_sfc &lt;- st_geometry(ca_state_shp) #extracts geom column\nnj_sfc &lt;- st_geometry(nj_state_shp) #extracts geom column\n\nNext, I want the median longitude value for each shapefile. I am going to brute force my way through the list data type.\n\n# ca_long_median_x &lt;- median(ca_sfc[[1]][[7]][[1]][,1])\n# nj_long_median_x &lt;- median(nj_sfc[[1]][[1]][,1])\n# median_difference_x &lt;- nj_long_median_x - ca_long_median_x\n# \n# ca_long_median_y &lt;- median(ca_sfc[[1]][[7]][[1]][,2])\n# nj_long_median_y &lt;- median(nj_sfc[[1]][[1]][,2])\n# median_difference_y &lt;- nj_long_median_y - ca_long_median_y\n\nMaybe centroids will work better?\n\nca_centroid &lt;- st_coordinates(st_centroid(ca_state_shp$geometry))\nnj_centroid &lt;- st_coordinates(st_centroid(nj_state_shp$geometry))\nlong_diff &lt;- nj_centroid[1] - ca_centroid[1]\nlat_diff &lt;- nj_centroid[2] - ca_centroid[2]"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#translation",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#translation",
    "title": "30 Day Map Challenge",
    "section": "Translation",
    "text": "Translation\nNow, I want to translate the Calfornia shape over to the New Jersey shape. Some social media posts were clever by aligning San Francisco with New York City, but my interest came from maintaining latitude values (which might help me understand weather and climate), so that will take place with the zero in c(median_difference, 0).\n\n# ca_shifted_sfc &lt;- ca_sfc + c(median_difference_x, 0)\n# ca_shifted_shp &lt;- st_set_geometry(ca_state_shp, ca_shifted_sfc)\n# st_crs(ca_shifted_shp) &lt;- st_crs(nj_state_shp) #ensure same CRS\n\n\n# using the centroids instead\nca_shifted_sfc &lt;- ca_sfc + c(long_diff, lat_diff)\nca_shifted_shp &lt;- st_set_geometry(ca_state_shp, ca_shifted_sfc)\nst_crs(ca_shifted_shp) &lt;- st_crs(nj_state_shp) #ensure same CRS\n\nSo far, here is the juxtaposition. I decided to place the New Jersey layer on top of the Calfornia layer.\n\nca_shifted_shp |&gt;\n  ggplot() +\n  geom_sf() +\n  geom_sf(data = nj_state_shp)\n\n\n\n\nI ended up maintaining the areas, but alas, not the latitude values."
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#polish",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#polish",
    "title": "30 Day Map Challenge",
    "section": "Polish",
    "text": "Polish\nWith two objects in one map, one neat way of labeling the objects is to change the text colors in the title using the ggtext package.\n\ntitle_string &lt;- \"&lt;span style='color:brown'&gt;&lt;b&gt;California&lt;/b&gt;&lt;/span&gt; and &lt;span style='color:blue'&gt;&lt;b&gt;New Jersey&lt;/b&gt;&lt;/span&gt;\"\n\nca_shifted_shp |&gt;\n  ggplot() +\n  geom_sf(color = \"brown\", fill = \"brown\") +\n  geom_sf(data = nj_state_shp, color = \"blue\", fill = \"blue\") +\n  labs(title = title_string,\n       subtitle = \"30 Day Map Challenge\\nDay 5: Analog Map\",\n       caption = \"Data sources: NJGIN\\nand CA Open Data Portal\") +\n  theme(plot.title = element_markdown()) #need ggtext here"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#markdown-titles",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#markdown-titles",
    "title": "30 Day Map Challenge",
    "section": "Markdown Titles",
    "text": "Markdown Titles\nWith just two objects in one map, one neat way of labeling the objects is to change the text colors in the title using the ggtext package.\n\ntitle_string &lt;- \"&lt;span style='color:brown'&gt;&lt;b&gt;California&lt;/b&gt;&lt;/span&gt; and &lt;span style='color:blue'&gt;&lt;b&gt;New Jersey&lt;/b&gt;&lt;/span&gt;\"\n\nca_shifted_shp |&gt;\n  ggplot() +\n  geom_sf(color = \"brown\", fill = \"brown\") +\n  geom_sf(data = nj_state_shp, color = \"blue\", fill = \"blue\") +\n  labs(title = title_string,\n       subtitle = \"30 Day Map Challenge\\nDay 5: Analog Map\",\n       caption = \"Data sources: NJGIN\\nand CA Open Data Portal\") +\n  theme(plot.title = element_markdown()) #need ggtext here"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#patchwork",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#patchwork",
    "title": "30 Day Map Challenge",
    "section": "Patchwork",
    "text": "Patchwork\nUsing the patchwork package, we can place the maps side-by-side.\n\n# patchwork\nkuwait_map + nj_map +\n  plot_annotation(\n    title = \"Kuwait is close to the size of New Jersey\",\n    subtitle = \"30 Day Map Challenge, Day 6: Asia\",\n    caption = \"Data sources: NJGIN and Princeton Univ\"\n  )"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#square-grid",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#square-grid",
    "title": "30 Day Map Challenge",
    "section": "Square Grid",
    "text": "Square Grid\nThe viewing window seems a bit skewed, and there is a lot of empty space. Next, I will\n\napply a coord_equal layer\nchange the geom_hex binwidth (by guess-and-check)\napply slightly more divergent colors\n\n\nus_cities_df |&gt;\n  filter(state_id == \"NJ\") |&gt;\n  ggplot(aes(x = lng, y = lat)) +\n  coord_equal() +\n  geom_hex(binwidth = c(0.15, 0.15)) +\n  labs(title = \"Density Map of New Jersey Cities\",\n       subtitle = \"30 Day Map Challenge, Day 9: Hexagons\",\n       caption = \"Data Source: SimpleMaps\") +\n  scale_fill_gradient(low = \"green\", high = \"blue\")"
  },
  {
    "objectID": "posts/2023_map_challenge/2023_map_challenge.html#recoding",
    "href": "posts/2023_map_challenge/2023_map_challenge.html#recoding",
    "title": "30 Day Map Challenge",
    "section": "Recoding",
    "text": "Recoding\nLet us try to redo the labels in plain language according to the US Census codes for road types.\n\nnj_road_shp |&gt; \n  mutate(road_type = case_match(RTTYP,\n    \"C\" ~ \"County\",\n    \"I\" ~ \"Interstate\",\n    \"M\" ~ \"common name\",\n    \"S\" ~ \"State recognized\",\n    \"U\" ~ \"U.S.\",\n    .default = \"other\"\n  )) |&gt;\n  ggplot() + \n  geom_sf(aes(color = road_type)) +\n  labs(title = \"The Major Roads of New Jersey\",\n       subtitle = \"30 Day Map Challenge, Day 7: Navigation\",\n       caption = \"Data source: Data.gov\") +\n  scale_color_discrete(name = \"Road Type\")"
  }
]