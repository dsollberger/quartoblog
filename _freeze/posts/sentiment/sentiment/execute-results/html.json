{
  "hash": "9a1f5b800dfcf93270d27a473b3fabd6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sentiment Analysis\"\nauthor: \"Derek Sollberger\"\ndate: \"2024-06-24\"\nformat: html\n---\n\n\nI want to perform some sentiment analysis for a personal project. In the past few years, there have been a few code packages in the R universe that can perhaps perform the sentiment analyses.\n\nFor the workflow, my draft thoughts are\n\n(1) Download a backup file of my journal (on Penzu)\n(2) Convert the PDF file into a text file\n(3) Extract the journal entries---separated by dates\n(4) Use a code package to perform sentiment analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"pdftools\")\nlibrary(\"tidyverse\")\n```\n:::\n\n\n\n## Reprex\n\nThe first two items on the list (along with the forth) can be performed with third-party software.  Currently my bottleneck is with the third step about extracting the journal entries from one large text file.  I want to ask people for help with this, so I will produce a `reprex` (**repr**oducible **ex**ample) for posting on the DLSC Slack workspace (`Data Science Learning Community`). First, I need some fake data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_data <- \"\n    6/22/2024\n    \n    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n    \n    6/23/2024\n    \n    Risus viverra adipiscing at in tellus integer feugiat.\n    \n    6/24/2024\n    \n    Praesent semper feugiat nibh sed pulvinar proin gravida hendrerit lectus.\n\"\n```\n:::\n\n\nNext, I envision the data tibble that I want in the end of this step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njournal_tibble <- tibble::tibble(\n  dates <- c(\"6/22/2024\", \"6/23/2024\", \"6/24/2024\"),\n  texts <- c(\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\", \"Risus viverra adipiscing at in tellus integer feugiat.\", \"Praesent semper feugiat nibh sed pulvinar proin gravida hendrerit lectus.\"))\n\ncolnames(journal_tibble) <- c(\"dates\", \"texts\")\n\n# print\njournal_tibble\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  dates     texts                                                               \n  <chr>     <chr>                                                               \n1 6/22/2024 Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiu…\n2 6/23/2024 Risus viverra adipiscing at in tellus integer feugiat.              \n3 6/24/2024 Praesent semper feugiat nibh sed pulvinar proin gravida hendrerit l…\n```\n\n\n:::\n:::\n\n\n## Loading the Data\n\nI write my journal in Penzu, and a user can get a copy of their archive as a PDF file. We can then load the file and extract the text (following this [blog post](https://finnstats.com/2021/06/15/extract-text-from-pdf-in-r-and-word-detection/))\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_data <- pdftools::pdf_text(\"Penzu_Export_-_20210414.pdf\")\n```\n:::\n\n\n## Regular Expressions\n\nThanks to the help from the (DSLC) Data Science Learning Community, we could then wrangle the `raw_data` into an easy-to-use tibble.\n\nI needed to distinguish between journal entry dates and any other dates found in the text. Each journal entry included \"by Derek Sollberger\" in its start, so I made that a part of the search pattern.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndate_pattern <- \"\\\\d+/\\\\d+/\\\\d+ by Derek Sollberger\"\n\ndates <- str_extract_all(raw_data, date_pattern) %>% unlist()\n\ntexts <- str_split(raw_data, pattern = date_pattern) %>%\n  unlist() %>%\n  str_trim() %>%\n  discard(~ .x == \"\")\n\nif(!length(dates) == length(texts)) stop(\"Dimensions don't match\")\n\n# tibble(date = dates, text = texts)\n```\n:::\n\n\nUser `Jack` at the DSLC wisely put in that dimension check in their response.  At the moment, `texts` has more than twice as many elements than `dates`, and `dates` is working as desired.\n\nUsing commands like `head(texts)`, I realized that the string split was nicely separating the titles and contents of my journal entries, and I probably should keep that structure somehow.  Moreover, I realized that the string split was also taking place at the page breaks in the PDF document.  However, I don't want splits there that happened arbitrarily for longer journal entries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntexts <- raw_data |>\n  paste0(collapse = \"'\") |>\n  str_split(pattern = date_pattern) |>\n  unlist()\n```\n:::\n\n\n## Data Frame\n\nI try to use the tidyverse to solve my data wrangling tasks.  My `raw_data` set here is not too large, so I will turn it into a data frame and attach columns.\n\nUser Anthony Durrant over at the DSLC gave great advice about using `fill` and `group_by` (which will hopefully bring together the multi-page journal entries).\n\nI probably want to also grab the day of the week (Mon., Tue., etc.) in the regular expression to make later steps easier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndate_pattern <- \".{5}\\\\d+/\\\\d+/\\\\d+ by Derek Sollberger\"\n\njournal_df <- data.frame(raw_data) |>\n  \n  # remove empty pages\n  mutate(n_char = nchar(raw_data)) |>\n  filter(n_char > 0) |>\n  \n  # find out when new entries started in file\n  mutate(new_entry = str_detect(raw_data, date_pattern),\n         time_stamp = str_extract(raw_data, date_pattern)) |>\n   \n  # find out when new entries started in calendar\n  mutate(entry_date = str_extract(time_stamp, pattern = \"\\\\d+/\\\\d+/\\\\d+\")) |>\n  fill(entry_date) |>\n  \n  # extract entry title\n  mutate(entry_title = if_else(new_entry,\n                               str_sub(raw_data,\n                                       1, str_locate(raw_data,\n                                                     date_pattern)[,1] - 2),\n                               \"\")) |>\n\n  # extract entry contents\n      # if new time stamp: grab contents after time stamp\n  mutate(page_contents = if_else(new_entry,\n                               str_sub(raw_data,\n                                       str_locate(raw_data,\n                                                  date_pattern)[,2] + 2,\n                                       n_char),\n                               raw_data)) |>\n  \n  # combine multiple-page entries\n  group_by(entry_date) |>\n  mutate(entry_contents = paste0(page_contents, collapse = \" \")) |>\n  ungroup() |>\n  \n  # retain only metadata\n  select(entry_date, entry_contents) |>\n  distinct() |>\n  mutate(word_count = str_count(entry_contents, \" \") + 1) |>\n  select(entry_date, word_count)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}